{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarFunction\n"
     ]
    }
   ],
   "source": [
    "from eaaw_graphlime_utils import *\n",
    "import gc\n",
    "\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "\n",
    "\n",
    "from   pcgrad.pcgrad import PCGrad # from the following: https://github.com/WeiChengTseng/Pytorch-PCGrad. Renamed to 'pcgrad' and moved to site-packages folder.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transorms used when loading computers: ['CreateMaskTransform()']\n",
      "train_mask: 12376\n",
      "test_mask: 689\n",
      "val_mask: 687\n"
     ]
    }
   ],
   "source": [
    "dataset_name='computers'\n",
    "if dataset_attributes[dataset_name]['single_or_multi_graph']=='single':\n",
    "    dataset = prep_data(dataset_name=dataset_name, location='default', batch_size='default', transform_list='default',\n",
    "                        train_val_test_split=[0.9,0.05,0.05])\n",
    "    graph_to_watermark = data = dataset[0]\n",
    "elif dataset_attributes[dataset_name]['single_or_multi_graph']=='multi':\n",
    "    [train_dataset, val_dataset, test_dataset], [train_loader, val_loader, test_loader] = prep_data(dataset_name=dataset_name, location='default', \n",
    "                                                                                                    batch_size='default', transform_list='default',\n",
    "                                                                                                                            train_val_test_split=[0.9,0.05,0.05])\n",
    "    graph_to_watermark = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization_kwargs, node_classifier_kwargs, watermark_kwargs, subgraph_kwargs, augment_kwargs, watermark_loss_kwargs, regression_kwargs = get_presets(dataset, 'default')\n",
    "get_presets(dataset,dataset_name)\n",
    "\n",
    "compare_unimportant_against_random=False\n",
    "\n",
    "config.node_classifier_kwargs['dropout']=0\n",
    "config.node_classifier_kwargs['dropout_subgraphs']=0\n",
    "config.watermark_kwargs['fancy_selection_kwargs']['clf_only_epochs'] = 20\n",
    "config.optimization_kwargs['lr']=0.002\n",
    "config.optimization_kwargs['epochs']=100\n",
    "# config.optimization_kwargs['coefWmk']=300\n",
    "\n",
    "config.optimization_kwargs['coefWmk_kwargs']= {\n",
    "                                                'coefWmk':15,\n",
    "                                                'schedule_coef_wmk': False,\n",
    "                                                'min_coefWmk_scheduled': 100,\n",
    "                                                'reach_max_coef_wmk_by_epoch':70,\n",
    "                                                }\n",
    "\n",
    "config.optimization_kwargs['use_pcgrad']=True\n",
    "config.optimization_kwargs['use_sam']=False\n",
    "config.optimization_kwargs['sam_momentum']=0.5\n",
    "config.optimization_kwargs['sam_rho']=1e-5\n",
    "\n",
    "\n",
    "config.optimization_kwargs['use_gradnorm']=False\n",
    "\n",
    "\n",
    "config.augment_kwargs['nodeDrop']['use']=False\n",
    "config.augment_kwargs['nodeMixUp']['use']=True\n",
    "config.augment_kwargs['nodeFeatMask']['use']=False\n",
    "config.augment_kwargs['edgeDrop']['use']=False\n",
    "config.augment_kwargs['separate_trainset_from_subgraphs'] = True\n",
    "config.augment_kwargs['ignore_subgraphs'] = True\n",
    "config.watermark_kwargs['fancy_selection_kwargs']['percent_of_features_to_watermark']=1\n",
    "config.watermark_kwargs['watermark_type']='fancy'\n",
    "config.subgraph_kwargs['numSubgraphs']=4\n",
    "config.subgraph_kwargs['fraction']=0.01\n",
    "config.subgraph_kwargs['method']='khop'\n",
    "config.optimization_kwargs['sacrifice_kwargs']['method']='subgraph_node_indices'\n",
    "config.optimization_kwargs['sacrifice_kwargs']['percentage']=1\n",
    "config.optimization_kwargs['separate_forward_passes_per_subgraph']=True\n",
    "config.optimization_kwargs['clf_only']=False\n",
    "config.watermark_loss_kwargs['epsilon']=1e-2\n",
    "\n",
    "config.regression_kwargs['lambda']=1e-4\n",
    "\n",
    "validate_kwargs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = merge_kwargs_dicts()#config.node_classifier_kwargs, config.optimization_kwargs, config.watermark_kwargs, config.subgraph_kwargs, config.regression_kwargs, config.watermark_loss_kwargs, config.augment_kwargs)\n",
    "merged_dict_keys = list(merged_dict.keys()) + ['Train Acc','Val Acc','Match Rates','Final Betas','watermark']\n",
    "all_dfs = pd.DataFrame(dict(zip(merged_dict_keys,[]*len(merged_dict_keys))))\n",
    "\n",
    "# all_dfs = pickle.load(open('/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/src/results_df.pkl','rb'))\n",
    "# all_results = {}\n",
    "\n",
    "\n",
    "\n",
    "scale_beta_method=None\n",
    "debug_multiple_subgraphs=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup_subgraph_dict\n",
      "nodeIndices: [122, 998, 1784, 2215, 2253, 3366, 3480, 3875, 4100, 4613, 4739, 5041, 5713, 6026, 7203, 7920, 8210, 9010, 9244, 9674, 9717, 10138, 10408, 10416, 10901, 10941, 11570, 11701, 12729, 13094]\n",
      "nodeIndices: [52, 227, 421, 692, 1168, 1243, 2316, 2611, 3009, 3139, 3924, 5250, 5709, 6013, 6152, 6365, 6561, 6885, 8214, 8376, 8528, 9986, 10307, 10770, 11047, 11118, 11598, 12337, 12888, 13529]\n",
      "nodeIndices: [473, 1322, 1549, 1600, 1629, 2179, 3170, 3659, 3896, 4153, 4531, 4545, 4722, 4862, 5405, 5750, 6314, 6425, 6934, 9446, 9982, 10611, 10682, 10979, 11136, 12189, 12758, 12836, 13155, 13508]\n",
      "nodeIndices: [505, 872, 1425, 1502, 1524, 1891, 3880, 4257, 4528, 5060, 5810, 6120, 6544, 6946, 7168, 7439, 7933, 7984, 8266, 8460, 8841, 9022, 9585, 9697, 11001, 11328, 12592, 12850, 13346, 13646]\n",
      "Sacrificing 100% of subgraph nodes from node classification training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0aee95c2da6471595a5fa440b7ae3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward\n",
      "Epoch:   0, loss_primary = 5.370, loss_watermark = n/a, B*W = n/a, train acc = 0.074, val acc = 0.087\n",
      "backward\n",
      "Epoch:   1, loss_primary = 5.425, loss_watermark = n/a, B*W = n/a, train acc = 0.483, val acc = 0.492\n",
      "backward\n",
      "Epoch:   2, loss_primary = 7.199, loss_watermark = n/a, B*W = n/a, train acc = 0.476, val acc = 0.437\n",
      "backward\n",
      "Epoch:   3, loss_primary = 5.052, loss_watermark = n/a, B*W = n/a, train acc = 0.533, val acc = 0.537\n",
      "backward\n",
      "Epoch:   4, loss_primary = 2.807, loss_watermark = n/a, B*W = n/a, train acc = 0.581, val acc = 0.592\n",
      "backward\n",
      "Epoch:   5, loss_primary = 2.376, loss_watermark = n/a, B*W = n/a, train acc = 0.626, val acc = 0.613\n",
      "backward\n",
      "Epoch:   6, loss_primary = 1.953, loss_watermark = n/a, B*W = n/a, train acc = 0.663, val acc = 0.658\n",
      "backward\n",
      "Epoch:   7, loss_primary = 1.863, loss_watermark = n/a, B*W = n/a, train acc = 0.722, val acc = 0.687\n",
      "backward\n",
      "Epoch:   8, loss_primary = 1.736, loss_watermark = n/a, B*W = n/a, train acc = 0.742, val acc = 0.731\n",
      "backward\n",
      "Epoch:   9, loss_primary = 1.307, loss_watermark = n/a, B*W = n/a, train acc = 0.778, val acc = 0.773\n",
      "backward\n",
      "Epoch:  10, loss_primary = 1.147, loss_watermark = n/a, B*W = n/a, train acc = 0.779, val acc = 0.789\n",
      "backward\n",
      "Epoch:  11, loss_primary = 1.148, loss_watermark = n/a, B*W = n/a, train acc = 0.764, val acc = 0.761\n",
      "backward\n",
      "Epoch:  12, loss_primary = 1.199, loss_watermark = n/a, B*W = n/a, train acc = 0.763, val acc = 0.742\n",
      "backward\n",
      "Epoch:  13, loss_primary = 1.131, loss_watermark = n/a, B*W = n/a, train acc = 0.772, val acc = 0.779\n",
      "backward\n",
      "Epoch:  14, loss_primary = 1.009, loss_watermark = n/a, B*W = n/a, train acc = 0.790, val acc = 0.776\n",
      "backward\n",
      "Epoch:  15, loss_primary = 0.969, loss_watermark = n/a, B*W = n/a, train acc = 0.809, val acc = 0.814\n",
      "backward\n",
      "Epoch:  16, loss_primary = 0.812, loss_watermark = n/a, B*W = n/a, train acc = 0.830, val acc = 0.859\n",
      "backward\n",
      "Epoch:  17, loss_primary = 0.791, loss_watermark = n/a, B*W = n/a, train acc = 0.830, val acc = 0.831\n",
      "backward\n",
      "Epoch:  18, loss_primary = 0.818, loss_watermark = n/a, B*W = n/a, train acc = 0.828, val acc = 0.837\n",
      "backward\n",
      "Epoch:  19, loss_primary = 0.738, loss_watermark = n/a, B*W = n/a, train acc = 0.839, val acc = 0.822\n",
      "apply_fancy_watermark\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0003)\n",
      "importance: tensor(0.0004)\n",
      "importance: tensor(0.0004)\n",
      "Using averaged betas from subgraphs to identify bottom 1% of important feature indices for watermarking, uniformly across subgraphs\n",
      "beta[not_omit_indices]: tensor([ 0.0089, -0.0272, -0.1119, -0.0247, -0.0237,  0.0320,  0.0250],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.021445244550704956, Beta Similarity: 0.005061013158410788\n",
      "beta[not_omit_indices]: tensor([-0.1909, -0.1431, -0.1091,  0.0698, -0.4430,  0.0770, -0.0175],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.06300153583288193, Beta Similarity: 0.032626014202833176\n",
      "beta[not_omit_indices]: tensor([ 0.0680,  0.3433, -0.1121, -0.2638,  0.0614, -0.3645,  0.1335],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.06981240212917328, Beta Similarity: 0.06131090596318245\n",
      "beta[not_omit_indices]: tensor([ 0.0533, -0.0615, -0.0515, -0.0142,  0.0926, -0.1006, -0.0128],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.028325391933321953, Beta Similarity: 0.0071160453371703625\n",
      "backward\n",
      "Epoch:  20, loss_primary = 0.773, loss_watermark = 0.685, B*W = 0.02653, train acc = 0.845, val acc = 0.852\n",
      "beta[not_omit_indices]: tensor([-0.0149, -0.0395, -0.2288,  0.0139, -0.0417, -0.0117,  0.0647],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.021310163661837578, Beta Similarity: 0.025269916281104088\n",
      "beta[not_omit_indices]: tensor([-0.1847, -0.1218, -0.1057,  0.0477, -0.3612,  0.0609, -0.0273],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.05676273629069328, Beta Similarity: 0.02493354305624962\n",
      "beta[not_omit_indices]: tensor([ 0.4536,  0.1105, -0.0511, -0.2503,  0.0559, -0.7372,  0.1779],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0734543427824974, Beta Similarity: 0.12402398139238358\n",
      "beta[not_omit_indices]: tensor([ 0.0452, -0.0300, -0.0502, -0.0139,  0.0526, -0.0522, -0.0044],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.018865181133151054, Beta Similarity: 0.007936750538647175\n",
      "backward\n",
      "Epoch:  21, loss_primary = 0.860, loss_watermark = 0.639, B*W = 0.04554, train acc = 0.824, val acc = 0.802\n",
      "beta[not_omit_indices]: tensor([-0.0177,  0.0299, -0.1699, -0.0401, -0.0308, -0.0132,  0.1180],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.029399501159787178, Beta Similarity: 0.009708268567919731\n",
      "beta[not_omit_indices]: tensor([-0.1975, -0.1479, -0.1116,  0.0626, -0.4332,  0.0690, -0.0207],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0634860023856163, Beta Similarity: 0.030535833910107613\n",
      "beta[not_omit_indices]: tensor([ 0.0864,  0.1911,  0.2092, -0.1137, -0.0337, -0.5160, -0.0774],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04898664355278015, Beta Similarity: 0.0830884650349617\n",
      "beta[not_omit_indices]: tensor([-0.0907,  0.0173, -0.0910,  0.0125,  0.2082, -0.0874,  0.0951],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.06057940796017647, Beta Similarity: -0.026546981185674667\n",
      "backward\n",
      "Epoch:  22, loss_primary = 0.826, loss_watermark = 0.759, B*W = 0.02420, train acc = 0.841, val acc = 0.825\n",
      "beta[not_omit_indices]: tensor([ 0.0463, -0.0705, -0.1530,  0.0150,  0.0568, -0.0203,  0.0061],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02335163950920105, Beta Similarity: 0.014453275129199028\n",
      "beta[not_omit_indices]: tensor([-0.1934, -0.1499, -0.1143,  0.0663, -0.4371,  0.0612, -0.0148],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.062070753425359726, Beta Similarity: 0.03257342800498009\n",
      "beta[not_omit_indices]: tensor([-2.3460e-04,  1.0696e-02,  2.1466e-01, -6.5765e-02, -3.3676e-02,\n",
      "        -4.0800e-01,  2.2797e-02], grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04906532168388367, Beta Similarity: 0.021273205056786537\n",
      "beta[not_omit_indices]: tensor([-0.0056,  0.0412, -0.0658, -0.0195,  0.0772, -0.0830,  0.1165],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.036975182592868805, Beta Similarity: -0.0041253226809203625\n",
      "backward\n",
      "Epoch:  23, loss_primary = 0.753, loss_watermark = 0.643, B*W = 0.01604, train acc = 0.841, val acc = 0.830\n",
      "beta[not_omit_indices]: tensor([ 0.0323, -0.0316, -0.1132,  0.0106, -0.0031,  0.0403, -0.0151],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.014118367806077003, Beta Similarity: 0.014620100148022175\n",
      "beta[not_omit_indices]: tensor([-0.1914, -0.1433, -0.1036,  0.0647, -0.4366,  0.0582, -0.0205],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0604168102145195, Beta Similarity: 0.03321756795048714\n",
      "beta[not_omit_indices]: tensor([ 0.0558,  0.1007,  0.2139, -0.0930, -0.0419, -0.5601,  0.0330],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.05284354090690613, Beta Similarity: 0.059815678745508194\n",
      "beta[not_omit_indices]: tensor([ 0.0095, -0.0860, -0.0209,  0.0236,  0.0474, -0.0318,  0.0393],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.029019905254244804, Beta Similarity: -0.012407575733959675\n",
      "backward\n",
      "Epoch:  24, loss_primary = 0.752, loss_watermark = 0.586, B*W = 0.02381, train acc = 0.840, val acc = 0.849\n",
      "beta[not_omit_indices]: tensor([ 0.0174,  0.0017, -0.0975,  0.0972, -0.1648,  0.1244, -0.0121],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02037968300282955, Beta Similarity: 0.03806550055742264\n",
      "beta[not_omit_indices]: tensor([-2.4229e-01, -1.4774e-01, -2.3460e-04, -1.7277e-02, -2.0247e-01,\n",
      "        -5.1947e-01, -2.1729e-02], grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.06386852264404297, Beta Similarity: 0.04808398708701134\n",
      "beta[not_omit_indices]: tensor([-0.2358,  0.2607,  0.0028,  0.0065,  0.0363, -0.4461,  0.1940],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.07318943738937378, Beta Similarity: 0.03491960093379021\n",
      "beta[not_omit_indices]: tensor([ 0.0036, -0.0387,  0.0081, -0.0188, -0.0220, -0.0384,  0.0438],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022259412333369255, Beta Similarity: -0.0064879828132689\n",
      "backward\n",
      "Epoch:  25, loss_primary = 0.769, loss_watermark = 0.674, B*W = 0.02865, train acc = 0.827, val acc = 0.853\n",
      "beta[not_omit_indices]: tensor([ 0.0791,  0.0454, -0.1477,  0.0204, -0.0929,  0.0018, -0.1397],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.001681431313045323, Beta Similarity: 0.07477065175771713\n",
      "beta[not_omit_indices]: tensor([-0.2414, -0.1421,  0.0106, -0.0171, -0.1949, -0.5361, -0.0127],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.06443732231855392, Beta Similarity: 0.04752281680703163\n",
      "beta[not_omit_indices]: tensor([ 0.1139, -0.2330, -0.1567, -0.2631, -0.2094, -0.3820, -0.0967],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.07372549921274185, Beta Similarity: 0.06607001274824142\n",
      "beta[not_omit_indices]: tensor([ 0.0501, -0.0617, -0.0130,  0.0304,  0.0037, -0.0446,  0.0533],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02124122716486454, Beta Similarity: 0.002771922620013356\n",
      "backward\n",
      "Epoch:  26, loss_primary = 0.782, loss_watermark = 0.604, B*W = 0.04778, train acc = 0.838, val acc = 0.844\n",
      "beta[not_omit_indices]: tensor([ 0.1074,  0.0428, -0.1925,  0.1069, -0.1897,  0.0853, -0.0771],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.013607264496386051, Beta Similarity: 0.09017256647348404\n",
      "beta[not_omit_indices]: tensor([-0.1954, -0.1304, -0.1147,  0.0611, -0.3969,  0.0842, -0.0311],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0628647580742836, Beta Similarity: 0.02768591418862343\n",
      "beta[not_omit_indices]: tensor([-0.1224,  0.1274,  0.0924,  0.0028,  0.0015, -0.4886, -0.0660],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03621605783700943, Beta Similarity: 0.06692450493574142\n",
      "beta[not_omit_indices]: tensor([ 0.1170, -0.0433, -0.0494,  0.0310, -0.0402, -0.0456,  0.0314],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.013521946035325527, Beta Similarity: 0.0297829769551754\n",
      "backward\n",
      "Epoch:  27, loss_primary = 0.761, loss_watermark = 0.473, B*W = 0.05364, train acc = 0.848, val acc = 0.849\n",
      "beta[not_omit_indices]: tensor([ 0.0606, -0.0022, -0.1248,  0.0578, -0.1415,  0.0628, -0.0192],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.012139935977756977, Beta Similarity: 0.04839788004755974\n",
      "beta[not_omit_indices]: tensor([-0.1891, -0.1585, -0.1025,  0.0586, -0.4217,  0.0389, -0.0289],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.059494469314813614, Beta Similarity: 0.03217833489179611\n",
      "beta[not_omit_indices]: tensor([-0.1500, -0.1490, -0.2330, -0.0435, -0.0357, -0.2538, -0.0045],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0540039949119091, Beta Similarity: 0.02635451778769493\n",
      "beta[not_omit_indices]: tensor([ 0.1956, -0.1037, -0.0378, -0.0126,  0.0168, -0.0986, -0.0579],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.023308519273996353, Beta Similarity: 0.036678314208984375\n",
      "backward\n",
      "Epoch:  28, loss_primary = 0.745, loss_watermark = 0.559, B*W = 0.03590, train acc = 0.852, val acc = 0.833\n",
      "beta[not_omit_indices]: tensor([-0.1017,  0.0161, -0.0976,  0.0865, -0.0645,  0.0035,  0.0577],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02755611762404442, Beta Similarity: 0.014548404142260551\n",
      "beta[not_omit_indices]: tensor([-0.1899, -0.1498, -0.0164,  0.0019, -0.2495, -0.3026, -0.0419],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.052544496953487396, Beta Similarity: 0.038959912955760956\n",
      "beta[not_omit_indices]: tensor([ 0.2345,  0.1866, -0.4465, -0.4873, -0.1006, -0.1875, -0.1089],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.07104243338108063, Beta Similarity: 0.11105237901210785\n",
      "beta[not_omit_indices]: tensor([ 0.0764, -0.0208,  0.0237, -0.0244, -0.0132, -0.0794,  0.0023],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.015895167365670204, Beta Similarity: 0.0139677869156003\n",
      "backward\n",
      "Epoch:  29, loss_primary = 0.886, loss_watermark = 0.626, B*W = 0.04463, train acc = 0.838, val acc = 0.821\n",
      "beta[not_omit_indices]: tensor([-0.0363,  0.0709, -0.1018,  0.0508, -0.0948,  0.0161, -0.0563],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.010343213565647602, Beta Similarity: 0.046038489788770676\n",
      "beta[not_omit_indices]: tensor([-0.0658, -0.0286, -0.0262,  0.0212, -0.0113,  0.3564,  0.0194],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0728791132569313, Beta Similarity: -0.0587877556681633\n",
      "beta[not_omit_indices]: tensor([ 0.0783, -0.0338, -0.0421, -0.1300, -0.0790, -0.1845, -0.0272],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.026268349960446358, Beta Similarity: 0.03530413657426834\n",
      "beta[not_omit_indices]: tensor([ 0.1196, -0.0935, -0.0373, -0.0089,  0.0286, -0.0521, -0.0163],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02300719916820526, Beta Similarity: 0.013468020595610142\n",
      "backward\n",
      "Epoch:  30, loss_primary = 1.048, loss_watermark = 0.497, B*W = 0.00901, train acc = 0.817, val acc = 0.796\n",
      "beta[not_omit_indices]: tensor([ 0.0035, -0.0089, -0.0811,  0.0503, -0.0068, -0.0311,  0.0003],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.005560607649385929, Beta Similarity: 0.0233644749969244\n",
      "beta[not_omit_indices]: tensor([-0.0764,  0.1207, -0.1666, -0.0334,  0.0133,  0.0002,  0.1600],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04761706665158272, Beta Similarity: 0.0005719321197830141\n",
      "beta[not_omit_indices]: tensor([ 0.1827, -0.0702, -0.1443, -0.1987, -0.1109, -0.1834, -0.0620],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.041281770914793015, Beta Similarity: 0.059203214943408966\n",
      "beta[not_omit_indices]: tensor([ 0.1129, -0.0675, -0.0699,  0.0049,  0.0051, -0.0119,  0.0474],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022153044119477272, Beta Similarity: 0.011362756602466106\n",
      "backward\n",
      "Epoch:  31, loss_primary = 1.151, loss_watermark = 0.437, B*W = 0.02363, train acc = 0.791, val acc = 0.780\n",
      "beta[not_omit_indices]: tensor([ 0.0025, -0.0293, -0.1095,  0.0182,  0.0578, -0.0531, -0.0238],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.016374079510569572, Beta Similarity: 0.017137663438916206\n",
      "beta[not_omit_indices]: tensor([ 0.0109, -0.0227,  0.0200, -0.0905, -0.0599,  0.0585,  0.0519],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0419379398226738, Beta Similarity: -0.02468068338930607\n",
      "beta[not_omit_indices]: tensor([-0.0243,  0.0011,  0.0299, -0.0067,  0.0054, -0.0861,  0.0004],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.017946181818842888, Beta Similarity: 0.0029186520259827375\n",
      "beta[not_omit_indices]: tensor([ 0.0295, -0.0768, -0.1105, -0.0223,  0.0347, -0.0155, -0.0712],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.023398667573928833, Beta Similarity: 0.01325965765863657\n",
      "backward\n",
      "Epoch:  32, loss_primary = 1.096, loss_watermark = 0.374, B*W = 0.00216, train acc = 0.797, val acc = 0.802\n",
      "beta[not_omit_indices]: tensor([ 0.0350,  0.1864, -0.0960,  0.2292, -0.0582,  0.0681, -0.0037],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.012064731679856777, Beta Similarity: 0.07718467712402344\n",
      "beta[not_omit_indices]: tensor([-0.0143, -0.1497,  0.0311,  0.1194,  0.0339,  0.1513,  0.1461],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.08376564830541611, Beta Similarity: -0.05814007297158241\n",
      "beta[not_omit_indices]: tensor([-0.0101,  0.0122, -0.0103, -0.0357,  0.0324, -0.0944, -0.0092],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.015571461990475655, Beta Similarity: 0.006831577979028225\n",
      "beta[not_omit_indices]: tensor([-0.0337,  0.0841, -0.0749, -0.0934,  0.0065,  0.0250,  0.0710],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.039920367300510406, Beta Similarity: -0.010059901513159275\n",
      "backward\n",
      "Epoch:  33, loss_primary = 0.972, loss_watermark = 0.567, B*W = 0.00395, train acc = 0.803, val acc = 0.825\n",
      "beta[not_omit_indices]: tensor([ 0.1620, -0.3289, -0.2628,  0.0262, -0.0320,  0.0795, -0.1002],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.06119966506958008, Beta Similarity: 0.024958202615380287\n",
      "beta[not_omit_indices]: tensor([ 0.0087, -0.0977, -0.0723,  0.0931, -0.0754, -0.0178,  0.1261],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.035008739680051804, Beta Similarity: 0.00619561318308115\n",
      "beta[not_omit_indices]: tensor([-0.1378, -0.0232,  0.0391, -0.0541, -0.0278, -0.2591, -0.0944],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.042031291872262955, Beta Similarity: 0.018158776685595512\n",
      "beta[not_omit_indices]: tensor([ 0.0543, -0.0516, -0.0925, -0.1193, -0.0172, -0.0484,  0.0038],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.029249979183077812, Beta Similarity: 0.005382197443395853\n",
      "backward\n",
      "Epoch:  34, loss_primary = 0.961, loss_watermark = 0.628, B*W = 0.01367, train acc = 0.814, val acc = 0.806\n",
      "beta[not_omit_indices]: tensor([ 0.1734,  0.0202, -0.1934,  0.0198, -0.0895, -0.0011, -0.1367],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0012738036457449198, Beta Similarity: 0.09059088677167892\n",
      "beta[not_omit_indices]: tensor([-0.0081,  0.0878, -0.0088, -0.0124, -0.0111,  0.1027,  0.0535],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03113093599677086, Beta Similarity: -0.009858982637524605\n",
      "beta[not_omit_indices]: tensor([-0.1029,  0.0465,  0.0038, -0.0090,  0.0388, -0.0265, -0.0617],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02778540924191475, Beta Similarity: -0.0028348308987915516\n",
      "beta[not_omit_indices]: tensor([ 0.1777, -0.0625, -0.0556, -0.0083,  0.0564, -0.0174, -0.0570],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02246369980275631, Beta Similarity: 0.02576950564980507\n",
      "backward\n",
      "Epoch:  35, loss_primary = 1.009, loss_watermark = 0.310, B*W = 0.02592, train acc = 0.812, val acc = 0.814\n",
      "beta[not_omit_indices]: tensor([ 0.0964,  0.0896, -0.0710,  0.2097, -0.0470,  0.0791, -0.1022],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.012722254730761051, Beta Similarity: 0.07671219855546951\n",
      "beta[not_omit_indices]: tensor([ 0.0216, -0.1842, -0.0771,  0.0281, -0.0764,  0.0574, -0.0602],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03736530616879463, Beta Similarity: 0.0031132015865296125\n",
      "beta[not_omit_indices]: tensor([ 0.2059,  0.1695, -0.4348, -0.2340,  0.0864, -0.2610, -0.1872],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04863371700048447, Beta Similarity: 0.1340073198080063\n",
      "beta[not_omit_indices]: tensor([-0.0579, -0.0347, -0.1455, -0.0775, -0.2105,  0.1657, -0.0481],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.053693365305662155, Beta Similarity: 0.00974873173981905\n",
      "backward\n",
      "Epoch:  36, loss_primary = 1.566, loss_watermark = 0.572, B*W = 0.05590, train acc = 0.757, val acc = 0.780\n",
      "beta[not_omit_indices]: tensor([ 0.2448,  0.1121, -0.1426,  0.2233, -0.0404, -0.0067, -0.1570],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.00046508785453625023, Beta Similarity: 0.1324051469564438\n",
      "beta[not_omit_indices]: tensor([-0.0204, -0.0938, -0.0578, -0.0469, -0.0609,  0.0929,  0.0305],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04779267683625221, Beta Similarity: -0.023690223693847656\n",
      "beta[not_omit_indices]: tensor([ 0.0445,  0.2229, -0.3704, -0.0881, -0.0364, -0.3436, -0.2274],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.014017072506248951, Beta Similarity: 0.16528306901454926\n",
      "beta[not_omit_indices]: tensor([ 0.0832, -0.0134, -0.0102, -0.2124, -0.0816,  0.1349,  0.0928],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.07049248367547989, Beta Similarity: -0.039776258170604706\n",
      "backward\n",
      "Epoch:  37, loss_primary = 1.625, loss_watermark = 0.498, B*W = 0.05856, train acc = 0.753, val acc = 0.760\n",
      "beta[not_omit_indices]: tensor([ 0.1178,  0.1080, -0.1866,  0.3518, -0.1962, -0.0215,  0.0039],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0019800676964223385, Beta Similarity: 0.13969393074512482\n",
      "beta[not_omit_indices]: tensor([ 0.0118, -0.0666, -0.0829, -0.0761, -0.1143,  0.1082, -0.0925],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04012316092848778, Beta Similarity: 0.0072340285405516624\n",
      "beta[not_omit_indices]: tensor([-0.1897,  0.1591,  0.0869, -0.0387, -0.0161, -0.3357, -0.0651],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04933292791247368, Beta Similarity: 0.037237439304590225\n",
      "beta[not_omit_indices]: tensor([-0.1748,  0.0413, -0.1549, -0.1063, -0.0456,  0.0285, -0.1646],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04850626364350319, Beta Similarity: 0.01383205782622099\n",
      "backward\n",
      "Epoch:  38, loss_primary = 1.195, loss_watermark = 0.525, B*W = 0.04950, train acc = 0.760, val acc = 0.764\n",
      "beta[not_omit_indices]: tensor([ 0.1917,  0.0974, -0.0218,  0.0334, -0.1003, -0.0183, -0.2476],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.10151413828134537\n",
      "beta[not_omit_indices]: tensor([ 0.0487,  0.0895,  0.0064, -0.0353, -0.0310,  0.0410,  0.0531],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.025124553591012955, Beta Similarity: 0.0047454833984375\n",
      "beta[not_omit_indices]: tensor([ 0.0117, -0.1001,  0.1026,  0.0105, -0.0343, -0.2004, -0.0821],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0318041555583477, Beta Similarity: 0.019479751586914062\n",
      "beta[not_omit_indices]: tensor([ 0.0960, -0.0386, -0.0675, -0.0030,  0.0604, -0.0775, -0.0210],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.018853498622775078, Beta Similarity: 0.0228554867208004\n",
      "backward\n",
      "Epoch:  39, loss_primary = 1.094, loss_watermark = 0.284, B*W = 0.03715, train acc = 0.779, val acc = 0.783\n",
      "beta[not_omit_indices]: tensor([ 0.1213, -0.0057,  0.0183,  0.0272, -0.0026, -0.0444, -0.1251],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.007337744813412428, Beta Similarity: 0.04238755255937576\n",
      "beta[not_omit_indices]: tensor([ 0.0097,  0.0083, -0.0469, -0.0895, -0.0636, -0.0840,  0.1134],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03212888166308403, Beta Similarity: 0.001354217529296875\n",
      "beta[not_omit_indices]: tensor([ 0.1516,  0.3192, -0.4550, -0.0714,  0.0187, -0.3585,  0.0933],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03048546425998211, Beta Similarity: 0.15725170075893402\n",
      "beta[not_omit_indices]: tensor([ 0.3120, -0.0405,  0.0230, -0.0083, -0.0525, -0.0305, -0.0516],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.014551335945725441, Beta Similarity: 0.05353791266679764\n",
      "backward\n",
      "Epoch:  40, loss_primary = 1.160, loss_watermark = 0.317, B*W = 0.06363, train acc = 0.795, val acc = 0.798\n",
      "beta[not_omit_indices]: tensor([ 0.1023,  0.0602, -0.0198,  0.0351, -0.0379, -0.0255, -0.1005],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.05448997765779495\n",
      "beta[not_omit_indices]: tensor([-0.0365,  0.0374,  0.0838, -0.1000, -0.0534, -0.0891, -0.0532],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0357542410492897, Beta Similarity: 0.0018354143248870969\n",
      "beta[not_omit_indices]: tensor([ 0.2041,  0.3574, -0.5293, -0.0914, -0.0021, -0.4699,  0.0696],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.026992427185177803, Beta Similarity: 0.2002371996641159\n",
      "beta[not_omit_indices]: tensor([ 0.1346, -0.0373, -0.0459, -0.0033, -0.0019, -0.0676, -0.0320],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.009813209995627403, Beta Similarity: 0.034500665962696075\n",
      "backward\n",
      "Epoch:  41, loss_primary = 1.176, loss_watermark = 0.272, B*W = 0.07277, train acc = 0.804, val acc = 0.818\n",
      "beta[not_omit_indices]: tensor([ 0.0061, -0.0149,  0.0262,  0.1211, -0.0636, -0.0574, -0.2045],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.009292504750192165, Beta Similarity: 0.058795247226953506\n",
      "beta[not_omit_indices]: tensor([-0.0215, -0.0200,  0.0160, -0.0297, -0.0259, -0.2678, -0.0164],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.018177445977926254, Beta Similarity: 0.031844548881053925\n",
      "beta[not_omit_indices]: tensor([-0.1244,  0.3539,  0.0283, -0.0653,  0.0213, -0.4251, -0.1415],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03991549834609032, Beta Similarity: 0.09730788320302963\n",
      "beta[not_omit_indices]: tensor([ 0.1968, -0.0461,  0.0105,  0.0153, -0.0250, -0.0336,  0.0066],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.013310196809470654, Beta Similarity: 0.029649462550878525\n",
      "backward\n",
      "Epoch:  42, loss_primary = 1.119, loss_watermark = 0.303, B*W = 0.05440, train acc = 0.795, val acc = 0.821\n",
      "beta[not_omit_indices]: tensor([ 0.1025,  0.0118, -0.0034,  0.0914, -0.0551,  0.0363,  0.0026],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.009354901500046253, Beta Similarity: 0.03218763321638107\n",
      "beta[not_omit_indices]: tensor([ 0.0227,  0.1505,  0.0262, -0.1052, -0.0225, -0.2020, -0.0872],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.021624431014060974, Beta Similarity: 0.050496168434619904\n",
      "beta[not_omit_indices]: tensor([-0.0941,  0.3217, -0.0283, -0.1810,  0.0089, -0.3610, -0.0801],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04484902694821358, Beta Similarity: 0.07245581597089767\n",
      "beta[not_omit_indices]: tensor([ 0.2391, -0.0343, -0.0121,  0.0163, -0.0507, -0.0375, -0.0140],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.006331001874059439, Beta Similarity: 0.047912053763866425\n",
      "backward\n",
      "Epoch:  43, loss_primary = 1.035, loss_watermark = 0.308, B*W = 0.05076, train acc = 0.785, val acc = 0.787\n",
      "beta[not_omit_indices]: tensor([ 0.2001, -0.0556,  0.1053,  0.0227, -0.0673, -0.0120,  0.0966],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04107983037829399, Beta Similarity: 0.00635828310623765\n",
      "beta[not_omit_indices]: tensor([ 0.0493,  0.1352,  0.0835, -0.0227, -0.0394, -0.1317, -0.0244],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.018033098429441452, Beta Similarity: 0.03910963982343674\n",
      "beta[not_omit_indices]: tensor([-0.1867,  0.3830,  0.0840, -0.1843,  0.0449, -0.1200, -0.0270],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0771232396364212, Beta Similarity: 0.00431878212839365\n",
      "beta[not_omit_indices]: tensor([ 0.1840, -0.0033, -0.0187,  0.0729, -0.0872, -0.0427, -0.0601],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0018994139973074198, Beta Similarity: 0.06602369248867035\n",
      "backward\n",
      "Epoch:  44, loss_primary = 1.113, loss_watermark = 0.518, B*W = 0.02895, train acc = 0.764, val acc = 0.761\n",
      "beta[not_omit_indices]: tensor([ 0.2749, -0.0504, -0.0409, -0.0315, -0.0827,  0.0989,  0.0149],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03367363288998604, Beta Similarity: 0.028982093557715416\n",
      "beta[not_omit_indices]: tensor([ 0.0049,  0.0695,  0.1553, -0.0142,  0.0151, -0.0885, -0.0101],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03138638287782669, Beta Similarity: -0.0016482217470183969\n",
      "beta[not_omit_indices]: tensor([-0.1638,  0.1676, -0.1256, -0.0463,  0.0394, -0.5122,  0.0025],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.041707593947649, Beta Similarity: 0.07905496656894684\n",
      "beta[not_omit_indices]: tensor([ 0.2019, -0.0305,  0.0125,  0.0005, -0.0583, -0.0482, -0.0198],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.010360890999436378, Beta Similarity: 0.0408085398375988\n",
      "backward\n",
      "Epoch:  45, loss_primary = 1.027, loss_watermark = 0.439, B*W = 0.03680, train acc = 0.770, val acc = 0.761\n",
      "beta[not_omit_indices]: tensor([ 0.1683, -0.0217, -0.0735, -0.0330, -0.0353,  0.0382, -0.1732],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01757584698498249, Beta Similarity: 0.05104419216513634\n",
      "beta[not_omit_indices]: tensor([ 0.0052,  0.0261, -0.0769, -0.0181, -0.0699,  0.0382,  0.0851],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.025173602625727654, Beta Similarity: 0.005245753563940525\n",
      "beta[not_omit_indices]: tensor([-0.2802,  0.0733,  0.3069, -0.0633,  0.0586, -0.3374, -0.0222],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.10699231177568436, Beta Similarity: -0.03943634033203125\n",
      "beta[not_omit_indices]: tensor([ 0.1305,  0.0074, -0.0328, -0.0270, -0.0573, -0.0122, -0.0259],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.00566257955506444, Beta Similarity: 0.0341404490172863\n",
      "backward\n",
      "Epoch:  46, loss_primary = 1.054, loss_watermark = 0.583, B*W = 0.01275, train acc = 0.778, val acc = 0.779\n",
      "beta[not_omit_indices]: tensor([ 0.0816,  0.0106, -0.0796,  0.0066, -0.0468,  0.0832, -0.1389],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.013802598230540752, Beta Similarity: 0.0401175357401371\n",
      "beta[not_omit_indices]: tensor([ 0.0269,  0.0552, -0.0215, -0.0173, -0.1161, -0.0149, -0.0309],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.003901312593370676, Beta Similarity: 0.03547787666320801\n",
      "beta[not_omit_indices]: tensor([-0.1118,  0.3890,  0.2838, -0.0195,  0.1090, -0.1076,  0.1002],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.09632706642150879, Beta Similarity: -0.018235206604003906\n",
      "beta[not_omit_indices]: tensor([-0.0075,  0.0394, -0.1244, -0.0713, -0.0539, -0.0738, -0.1454],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.014111588709056377, Beta Similarity: 0.051170893013477325\n",
      "backward\n",
      "Epoch:  47, loss_primary = 1.258, loss_watermark = 0.481, B*W = 0.02713, train acc = 0.781, val acc = 0.786\n",
      "beta[not_omit_indices]: tensor([ 0.1857,  0.0219, -0.0278, -0.0436, -0.0150,  0.0099, -0.1134],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.010502885095775127, Beta Similarity: 0.044334955513477325\n",
      "beta[not_omit_indices]: tensor([ 0.0436,  0.0455,  0.0170, -0.0570, -0.0918,  0.0212,  0.0356],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.024409979581832886, Beta Similarity: 0.00714868726208806\n",
      "beta[not_omit_indices]: tensor([-0.1777,  0.1346,  0.1359,  0.0375,  0.0293, -0.4013, -0.0500],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.053278420120477676, Beta Similarity: 0.0400826595723629\n",
      "beta[not_omit_indices]: tensor([-0.0500,  0.0454, -0.0699, -0.0901, -0.0711, -0.1502, -0.0559],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022860324010252953, Beta Similarity: 0.036059241741895676\n",
      "backward\n",
      "Epoch:  48, loss_primary = 1.132, loss_watermark = 0.416, B*W = 0.03191, train acc = 0.784, val acc = 0.776\n",
      "beta[not_omit_indices]: tensor([ 0.2527,  0.0275, -0.0721,  0.0058, -0.0719,  0.0240, -0.0544],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.005460945423692465, Beta Similarity: 0.06576456129550934\n",
      "beta[not_omit_indices]: tensor([ 0.0263,  0.0693, -0.0513, -0.0567, -0.1065,  0.0575,  0.0100],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022039314731955528, Beta Similarity: 0.018442153930664062\n",
      "beta[not_omit_indices]: tensor([-0.0909,  0.0093,  0.0795, -0.1347, -0.0214, -0.2658, -0.0978],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0479702353477478, Beta Similarity: 0.01273454912006855\n",
      "beta[not_omit_indices]: tensor([-0.0342, -0.1611, -0.1258, -0.0477,  0.0305, -0.0070,  0.0372],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.05195407196879387, Beta Similarity: -0.025413241237401962\n",
      "backward\n",
      "Epoch:  49, loss_primary = 1.074, loss_watermark = 0.478, B*W = 0.01788, train acc = 0.790, val acc = 0.770\n",
      "beta[not_omit_indices]: tensor([ 0.2483, -0.0193, -0.0452, -0.0451, -0.0575,  0.0088, -0.1257],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.014753788709640503, Beta Similarity: 0.057628631591796875\n",
      "beta[not_omit_indices]: tensor([-0.0147, -0.0030, -0.0266,  0.0261, -0.0593,  0.0209, -0.0023],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.010900094173848629, Beta Similarity: 0.010805538855493069\n",
      "beta[not_omit_indices]: tensor([-0.0779,  0.1273, -0.0707, -0.0398, -0.0347, -0.5006,  0.0368],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.026355167850852013, Beta Similarity: 0.0826856717467308\n",
      "beta[not_omit_indices]: tensor([ 0.0212,  0.0516, -0.0335,  0.0730,  0.0204, -0.0867,  0.0240],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.009211338125169277, Beta Similarity: 0.0316576287150383\n",
      "backward\n",
      "Epoch:  50, loss_primary = 1.220, loss_watermark = 0.230, B*W = 0.04569, train acc = 0.794, val acc = 0.806\n",
      "beta[not_omit_indices]: tensor([ 0.1951,  0.0171, -0.0889,  0.0303, -0.0226,  0.0223, -0.2412],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.004614388104528189, Beta Similarity: 0.0818479061126709\n",
      "beta[not_omit_indices]: tensor([ 0.0736, -0.0373,  0.0211,  0.0281, -0.0840,  0.0037, -0.0460],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.013146982528269291, Beta Similarity: 0.024237768724560738\n",
      "beta[not_omit_indices]: tensor([-0.0817,  0.2645,  0.1955,  0.0305,  0.0602, -0.0696,  0.0252],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.05752537027001381, Beta Similarity: 0.0002753394073806703\n",
      "beta[not_omit_indices]: tensor([-0.0369,  0.0743,  0.0111,  0.0626, -0.1006, -0.0011,  0.0518],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01980087161064148, Beta Similarity: 0.01984405517578125\n",
      "backward\n",
      "Epoch:  51, loss_primary = 1.431, loss_watermark = 0.357, B*W = 0.03155, train acc = 0.807, val acc = 0.798\n",
      "beta[not_omit_indices]: tensor([ 0.2057,  0.0459, -0.1458,  0.0124, -0.0524, -0.0027, -0.2112],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0010427420493215322, Beta Similarity: 0.09657461196184158\n",
      "beta[not_omit_indices]: tensor([-0.0383,  0.1248, -0.0714,  0.0802, -0.0363, -0.0070, -0.0266],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.007338866591453552, Beta Similarity: 0.043988704681396484\n",
      "beta[not_omit_indices]: tensor([-0.0942,  0.0308,  0.0590, -0.0066,  0.0241, -0.0434, -0.0572],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.031989309936761856, Beta Similarity: -0.0075019425712525845\n",
      "beta[not_omit_indices]: tensor([-0.0237,  0.0064,  0.0211,  0.0408, -0.0655,  0.0019, -0.0511],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.011464668437838554, Beta Similarity: 0.016744885593652725\n",
      "backward\n",
      "Epoch:  52, loss_primary = 1.494, loss_watermark = 0.194, B*W = 0.03745, train acc = 0.801, val acc = 0.789\n",
      "beta[not_omit_indices]: tensor([ 3.3947e-01,  9.5615e-02, -1.8830e-01,  3.8147e-06, -5.1945e-02,\n",
      "        -5.0930e-02, -1.3320e-01], grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0014280264731496572, Beta Similarity: 0.12277979403734207\n",
      "beta[not_omit_indices]: tensor([ 0.0085,  0.0281,  0.0127,  0.0358, -0.0089,  0.0099,  0.0121],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.00960824079811573, Beta Similarity: 0.0066743576899170876\n",
      "beta[not_omit_indices]: tensor([ 0.1395,  0.1168, -0.3183, -0.3235, -0.0124, -0.1095, -0.1191],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04763586074113846, Beta Similarity: 0.07029615342617035\n",
      "beta[not_omit_indices]: tensor([ 0.1263, -0.0465, -0.0058, -0.0038, -0.0214,  0.0015, -0.0399],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01228758692741394, Beta Similarity: 0.020227670669555664\n",
      "backward\n",
      "Epoch:  53, loss_primary = 1.314, loss_watermark = 0.266, B*W = 0.05499, train acc = 0.812, val acc = 0.809\n",
      "beta[not_omit_indices]: tensor([ 0.3571,  0.1470, -0.2599,  0.0224, -0.1589,  0.0084, -0.1759],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0026351057458668947, Beta Similarity: 0.1589847356081009\n",
      "beta[not_omit_indices]: tensor([ 0.0181,  0.1272, -0.0378, -0.0240, -0.1006, -0.0175,  0.0197],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.009090902283787727, Beta Similarity: 0.03679370880126953\n",
      "beta[not_omit_indices]: tensor([ 0.0238,  0.0197, -0.0648, -0.1021,  0.0046, -0.0992, -0.0596],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01810053549706936, Beta Similarity: 0.022925376892089844\n",
      "beta[not_omit_indices]: tensor([-0.0895, -0.1363, -0.0989,  0.1418, -0.0211,  0.0154,  0.0755],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.05095114931464195, Beta Similarity: -0.00783429853618145\n",
      "backward\n",
      "Epoch:  54, loss_primary = 1.163, loss_watermark = 0.303, B*W = 0.05272, train acc = 0.810, val acc = 0.814\n",
      "beta[not_omit_indices]: tensor([ 0.2344,  0.0550, -0.1775,  0.0015, -0.0070, -0.0529, -0.1296],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0016539463540539145, Beta Similarity: 0.09397159516811371\n",
      "beta[not_omit_indices]: tensor([ 0.0772,  0.1087, -0.0336,  0.0550, -0.1559, -0.1100, -0.0521],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.08463450521230698\n",
      "beta[not_omit_indices]: tensor([-0.0788, -0.0298, -0.0069, -0.0011, -0.0028, -0.0855,  0.0546],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.030654575675725937, Beta Similarity: -0.009868417866528034\n",
      "beta[not_omit_indices]: tensor([-0.0644, -0.0224, -0.0875,  0.0280, -0.0998, -0.0095,  0.0275],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.020702775567770004, Beta Similarity: 0.01577104814350605\n",
      "backward\n",
      "Epoch:  55, loss_primary = 1.164, loss_watermark = 0.199, B*W = 0.04613, train acc = 0.794, val acc = 0.799\n",
      "beta[not_omit_indices]: tensor([ 0.0812, -0.0203, -0.0893,  0.0018, -0.0478,  0.0116, -0.0319],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.008588199503719807, Beta Similarity: 0.03144967555999756\n",
      "beta[not_omit_indices]: tensor([ 0.0226, -0.0258, -0.1124, -0.0485, -0.1169,  0.1005, -0.1532],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.029262717813253403, Beta Similarity: 0.03288480266928673\n",
      "beta[not_omit_indices]: tensor([-0.0664, -0.0409, -0.0439, -0.0070, -0.0183, -0.1212,  0.0527],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.029570722952485085, Beta Similarity: 0.002327987225726247\n",
      "beta[not_omit_indices]: tensor([-0.0532, -0.1042, -0.1054,  0.0046, -0.1113, -0.0273,  0.0683],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03730515390634537, Beta Similarity: 0.003256389172747731\n",
      "backward\n",
      "Epoch:  56, loss_primary = 1.268, loss_watermark = 0.393, B*W = 0.01748, train acc = 0.788, val acc = 0.783\n",
      "beta[not_omit_indices]: tensor([ 0.1506,  0.0012, -0.1113, -0.0200, -0.0041, -0.0385, -0.0978],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.006397489923983812, Beta Similarity: 0.054763179272413254\n",
      "beta[not_omit_indices]: tensor([ 0.0030, -0.0666, -0.0637, -0.0918, -0.1072,  0.0780, -0.0428],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03905773535370827, Beta Similarity: -0.002807889599353075\n",
      "beta[not_omit_indices]: tensor([-0.0426,  0.0218,  0.0536, -0.0151,  0.0008, -0.1019, -0.0069],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02216513454914093, Beta Similarity: 0.002644947497174144\n",
      "beta[not_omit_indices]: tensor([-0.0109,  0.0250, -0.1031, -0.1204, -0.1414, -0.0562, -0.0230],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.021609650924801826, Beta Similarity: 0.03107561357319355\n",
      "backward\n",
      "Epoch:  57, loss_primary = 1.317, loss_watermark = 0.335, B*W = 0.02142, train acc = 0.781, val acc = 0.774\n",
      "beta[not_omit_indices]: tensor([ 0.0972, -0.0030, -0.0730, -0.0211,  0.0203,  0.0236, -0.0865],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01543685793876648, Beta Similarity: 0.026951517909765244\n",
      "beta[not_omit_indices]: tensor([ 0.0144, -0.0497, -0.0964, -0.0757, -0.1172,  0.1228, -0.0989],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03974877670407295, Beta Similarity: 0.01122392900288105\n",
      "beta[not_omit_indices]: tensor([-0.0427,  0.0141,  0.0529, -0.0048,  0.0024, -0.0875,  0.0026],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022198105230927467, Beta Similarity: -0.00054168701171875\n",
      "beta[not_omit_indices]: tensor([-0.0253, -0.0270, -0.0368, -0.1293, -0.1298, -0.0741, -0.0581],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.030220206826925278, Beta Similarity: 0.01674652099609375\n",
      "backward\n",
      "Epoch:  58, loss_primary = 1.201, loss_watermark = 0.404, B*W = 0.01360, train acc = 0.782, val acc = 0.754\n",
      "beta[not_omit_indices]: tensor([ 0.0305,  0.0391, -0.0779,  0.0254,  0.0119, -0.0556, -0.1163],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0031315612141042948, Beta Similarity: 0.047543082386255264\n",
      "beta[not_omit_indices]: tensor([-0.0300, -0.0795, -0.0628, -0.0904, -0.0643,  0.1164, -0.1621],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.050898827612400055, Beta Similarity: -0.0038790020626038313\n",
      "beta[not_omit_indices]: tensor([-0.0515,  0.0170,  0.0614, -0.0099,  0.0024, -0.0713, -0.0162],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02360473945736885, Beta Similarity: -0.0029755320865660906\n",
      "beta[not_omit_indices]: tensor([-0.0878,  0.0185, -0.0008, -0.0222, -0.0453, -0.0319, -0.0662],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01989082433283329, Beta Similarity: 0.007523059844970703\n",
      "backward\n",
      "Epoch:  59, loss_primary = 1.092, loss_watermark = 0.366, B*W = 0.01205, train acc = 0.790, val acc = 0.782\n",
      "beta[not_omit_indices]: tensor([ 0.0293,  0.0090, -0.0386, -0.0132, -0.0254, -0.0838, -0.0251],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0034518949687480927, Beta Similarity: 0.028310570865869522\n",
      "beta[not_omit_indices]: tensor([-0.0397,  0.0180, -0.1014, -0.0678, -0.0814,  0.0875, -0.1110],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.032140497118234634, Beta Similarity: 0.016685621812939644\n",
      "beta[not_omit_indices]: tensor([-0.0631,  0.0323,  0.0678, -0.0172,  0.0050, -0.0665, -0.0435],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02758687548339367, Beta Similarity: -0.0015493461396545172\n",
      "beta[not_omit_indices]: tensor([-0.0139,  0.1016, -0.0670,  0.0075, -0.0733,  0.0151, -0.0352],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.00736254034563899, Beta Similarity: 0.0365055613219738\n",
      "backward\n",
      "Epoch:  60, loss_primary = 0.975, loss_watermark = 0.265, B*W = 0.01999, train acc = 0.805, val acc = 0.786\n",
      "beta[not_omit_indices]: tensor([ 0.1327,  0.1044, -0.1041, -0.0252,  0.0330, -0.1097, -0.1669],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.011171607300639153, Beta Similarity: 0.0799512267112732\n",
      "beta[not_omit_indices]: tensor([-0.0640,  0.2016, -0.0409,  0.0296,  0.0241,  0.0680,  0.0076],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02910001575946808, Beta Similarity: 0.015492848120629787\n",
      "beta[not_omit_indices]: tensor([-0.1841,  0.0592,  0.0494,  0.0657,  0.1075,  0.1225, -0.1264],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.07191699743270874, Beta Similarity: -0.030298778787255287\n",
      "beta[not_omit_indices]: tensor([-0.0415,  0.1062, -0.0051,  0.0508, -0.0648,  0.0348, -0.0410],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.014460056088864803, Beta Similarity: 0.0273709986358881\n",
      "backward\n",
      "Epoch:  61, loss_primary = 0.916, loss_watermark = 0.475, B*W = 0.02313, train acc = 0.822, val acc = 0.787\n",
      "beta[not_omit_indices]: tensor([ 0.1891,  0.1583, -0.1385,  0.0028, -0.0567, -0.0139, -0.2162],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0010305485920980573, Beta Similarity: 0.11077553778886795\n",
      "beta[not_omit_indices]: tensor([-0.0438,  0.1394,  0.0975, -0.1330, -0.0290, -0.1610, -0.0452],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04347791522741318, Beta Similarity: 0.014324256218969822\n",
      "beta[not_omit_indices]: tensor([-0.1543, -0.1827, -0.2094,  0.0567, -0.0185, -0.0615,  0.1595],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.07521293312311172, Beta Similarity: -0.021492958068847656\n",
      "beta[not_omit_indices]: tensor([-0.0202,  0.0614,  0.0223,  0.0355, -0.0687,  0.0421, -0.0304],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01637503318488598, Beta Similarity: 0.015904562547802925\n",
      "backward\n",
      "Epoch:  62, loss_primary = 0.795, loss_watermark = 0.510, B*W = 0.02988, train acc = 0.840, val acc = 0.833\n",
      "beta[not_omit_indices]: tensor([ 0.2853,  0.1417, -0.1431, -0.0294, -0.0215, -0.0511, -0.3055],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.005624738056212664, Beta Similarity: 0.13129138946533203\n",
      "beta[not_omit_indices]: tensor([ 0.0159,  0.0152, -0.0267,  0.0042,  0.1108,  0.0763,  0.0260],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03555312007665634, Beta Similarity: -0.02157466672360897\n",
      "beta[not_omit_indices]: tensor([-0.1093, -0.0473, -0.1222,  0.1862,  0.0406, -0.4486,  0.0949],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.047450341284275055, Beta Similarity: 0.06640952080488205\n",
      "beta[not_omit_indices]: tensor([ 0.0219,  0.0607, -0.0624,  0.0037, -0.0175,  0.0328, -0.0341],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.00701352721080184, Beta Similarity: 0.023937225341796875\n",
      "backward\n",
      "Epoch:  63, loss_primary = 0.760, loss_watermark = 0.359, B*W = 0.05002, train acc = 0.847, val acc = 0.841\n",
      "beta[not_omit_indices]: tensor([ 0.3153,  0.0970, -0.2509, -0.0071, -0.1090, -0.0211, -0.0095],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0025064630899578333, Beta Similarity: 0.11367600411176682\n",
      "beta[not_omit_indices]: tensor([ 0.0572, -0.0006, -0.1392, -0.0506, -0.0965, -0.0751,  0.0396],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.017250439152121544, Beta Similarity: 0.03960704803466797\n",
      "beta[not_omit_indices]: tensor([ 0.2798, -0.0787,  0.0260, -0.1425, -0.0201, -0.4837,  0.0225],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0442470908164978, Beta Similarity: 0.07341194152832031\n",
      "beta[not_omit_indices]: tensor([ 0.0543,  0.0734, -0.0469,  0.0376, -0.0596, -0.0558, -0.0781],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.057950157672166824\n",
      "backward\n",
      "Epoch:  64, loss_primary = 0.826, loss_watermark = 0.240, B*W = 0.07116, train acc = 0.820, val acc = 0.824\n",
      "beta[not_omit_indices]: tensor([ 0.3087,  0.1145, -0.2775, -0.0222, -0.1341,  0.0030,  0.0064],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.008793596178293228, Beta Similarity: 0.11474186927080154\n",
      "beta[not_omit_indices]: tensor([ 0.0606,  0.0550, -0.1023, -0.0063, -0.0316, -0.0575,  0.0348],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.008730004541575909, Beta Similarity: 0.037975721061229706\n",
      "beta[not_omit_indices]: tensor([ 0.2089,  0.1765, -0.0063, -0.0688, -0.0058, -0.4522, -0.0405],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.012388131581246853, Beta Similarity: 0.1173357293009758\n",
      "beta[not_omit_indices]: tensor([-0.0147, -0.0169, -0.1015,  0.0304, -0.0307, -0.0123,  0.0414],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.014711281284689903, Beta Similarity: 0.014568873681128025\n",
      "backward\n",
      "Epoch:  65, loss_primary = 0.776, loss_watermark = 0.167, B*W = 0.07116, train acc = 0.826, val acc = 0.836\n",
      "beta[not_omit_indices]: tensor([ 0.2844,  0.1371, -0.2844,  0.0172, -0.2114,  0.0308, -0.0718],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.005825282074511051, Beta Similarity: 0.1393413543701172\n",
      "beta[not_omit_indices]: tensor([ 0.0732,  0.0249, -0.0734,  0.0249, -0.0319, -0.0308,  0.0618],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.010252511128783226, Beta Similarity: 0.02818911336362362\n",
      "beta[not_omit_indices]: tensor([ 0.0613,  0.0827, -0.2025, -0.2238, -0.0108, -0.1434, -0.1047],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.033393554389476776, Beta Similarity: 0.054525990039110184\n",
      "beta[not_omit_indices]: tensor([-0.0491,  0.0632, -0.0481,  0.0266, -0.0410,  0.0194, -0.0247],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.012652195058763027, Beta Similarity: 0.01929691806435585\n",
      "backward\n",
      "Epoch:  66, loss_primary = 0.850, loss_watermark = 0.233, B*W = 0.06034, train acc = 0.813, val acc = 0.820\n",
      "beta[not_omit_indices]: tensor([ 0.3242,  0.0488, -0.2675, -0.0120,  0.0104, -0.0657, -0.0755],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0060524968430399895, Beta Similarity: 0.10848535597324371\n",
      "beta[not_omit_indices]: tensor([ 0.0488,  0.0737, -0.1007,  0.0167, -0.0245, -0.0548,  0.0536],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.009084123186767101, Beta Similarity: 0.037951476871967316\n",
      "beta[not_omit_indices]: tensor([-0.0409,  0.0111,  0.0479, -0.0086,  0.0026, -0.0893,  0.0026],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0217788964509964, Beta Similarity: -0.0002941403945442289\n",
      "beta[not_omit_indices]: tensor([ 0.0433,  0.0689, -0.0409,  0.0203, -0.0495, -0.0094, -0.0391],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 8.470804459648207e-05, Beta Similarity: 0.038788385689258575\n",
      "backward\n",
      "Epoch:  67, loss_primary = 0.889, loss_watermark = 0.139, B*W = 0.04623, train acc = 0.810, val acc = 0.809\n",
      "beta[not_omit_indices]: tensor([ 0.3675,  0.1458, -0.3051,  0.0119, -0.2058,  0.0455, -0.0833],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.007933720014989376, Beta Similarity: 0.15342304110527039\n",
      "beta[not_omit_indices]: tensor([ 0.1095,  0.1196, -0.0275, -0.0372, -0.0326, -0.0687, -0.0464],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.006744079291820526, Beta Similarity: 0.0524400994181633\n",
      "beta[not_omit_indices]: tensor([-0.0412,  0.0086,  0.0507, -0.0041,  0.0023, -0.0883,  0.0065],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022311557084321976, Beta Similarity: -0.0011276517761871219\n",
      "beta[not_omit_indices]: tensor([ 0.0155,  0.0263,  0.0329,  0.0344, -0.0401, -0.0094, -0.0760],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.006212986074388027, Beta Similarity: 0.024110522121191025\n",
      "backward\n",
      "Epoch:  68, loss_primary = 0.930, loss_watermark = 0.162, B*W = 0.05721, train acc = 0.818, val acc = 0.814\n",
      "beta[not_omit_indices]: tensor([ 0.2391,  0.1030, -0.2848,  0.0197, -0.1848,  0.0783, -0.0867],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.012607813812792301, Beta Similarity: 0.11997781693935394\n",
      "beta[not_omit_indices]: tensor([ 0.0518,  0.0506, -0.0979,  0.0208, -0.0394,  0.0160, -0.0074],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.004077845718711615, Beta Similarity: 0.03597341105341911\n",
      "beta[not_omit_indices]: tensor([-0.0423,  0.0094,  0.0519, -0.0038,  0.0020, -0.0883,  0.0076],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022582264617085457, Beta Similarity: -0.0013931819703429937\n",
      "beta[not_omit_indices]: tensor([ 0.0480,  0.0749, -0.0090,  0.0249, -0.0543,  0.0400, -0.0714],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.00729418033733964, Beta Similarity: 0.034619469195604324\n",
      "backward\n",
      "Epoch:  69, loss_primary = 0.923, loss_watermark = 0.175, B*W = 0.04729, train acc = 0.833, val acc = 0.827\n",
      "beta[not_omit_indices]: tensor([ 0.1806,  0.0938, -0.2625,  0.0655, -0.2351,  0.1115, -0.0667],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.017359836027026176, Beta Similarity: 0.11321401596069336\n",
      "beta[not_omit_indices]: tensor([ 0.0791,  0.0681,  0.0227,  0.0233, -0.0204, -0.0606, -0.1292],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.004675423260778189, Beta Similarity: 0.0511213019490242\n",
      "beta[not_omit_indices]: tensor([-0.0408,  0.0107,  0.0501, -0.0028,  0.0025, -0.0877,  0.0073],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.021936388686299324, Beta Similarity: -0.0007390975952148438\n",
      "beta[not_omit_indices]: tensor([ 0.0364,  0.0776, -0.0245,  0.0288, -0.0474, -0.0014, -0.0904],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.001231297035701573, Beta Similarity: 0.0437709279358387\n",
      "backward\n",
      "Epoch:  70, loss_primary = 0.881, loss_watermark = 0.170, B*W = 0.05184, train acc = 0.837, val acc = 0.827\n",
      "beta[not_omit_indices]: tensor([ 0.2392,  0.0485, -0.3225,  0.0454, -0.1043, -0.0553, -0.0005],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0013588169822469354, Beta Similarity: 0.11651897430419922\n",
      "beta[not_omit_indices]: tensor([ 0.0760,  0.0762,  0.0549,  0.0235, -0.0377,  0.0110, -0.1186],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01227372232824564, Beta Similarity: 0.037995610386133194\n",
      "beta[not_omit_indices]: tensor([-0.0425,  0.0098,  0.0519, -0.0036,  0.0022, -0.0884,  0.0076],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02256455458700657, Beta Similarity: -0.0013675689697265625\n",
      "beta[not_omit_indices]: tensor([ 0.0345,  0.0252, -0.0317,  0.0314, -0.0113, -0.0760, -0.0578],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.038267407566308975\n",
      "backward\n",
      "Epoch:  71, loss_primary = 0.893, loss_watermark = 0.136, B*W = 0.04785, train acc = 0.836, val acc = 0.827\n",
      "beta[not_omit_indices]: tensor([ 0.2511,  0.1068, -0.2649,  0.0424, -0.1836,  0.0451, -0.1223],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.007867780514061451, Beta Similarity: 0.13229329884052277\n",
      "beta[not_omit_indices]: tensor([-0.0175, -0.0464,  0.0007,  0.0783, -0.0063, -0.0631, -0.0126],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.014046674594283104, Beta Similarity: 0.01368093490600586\n",
      "beta[not_omit_indices]: tensor([-4.4724e-02, -5.6210e-03,  3.5217e-02,  3.0518e-05, -1.3161e-03,\n",
      "        -9.1678e-02,  2.2263e-02], grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.023782532662153244, Beta Similarity: -0.0021142959594726562\n",
      "beta[not_omit_indices]: tensor([ 0.0595,  0.0903, -0.0432,  0.0357, -0.0456, -0.0175, -0.1075],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.0570417121052742\n",
      "backward\n",
      "Epoch:  72, loss_primary = 0.803, loss_watermark = 0.171, B*W = 0.05023, train acc = 0.845, val acc = 0.841\n",
      "beta[not_omit_indices]: tensor([ 0.1895,  0.1001, -0.2601,  0.0929, -0.2646,  0.0525, -0.0650],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.008930445648729801, Beta Similarity: 0.13137146830558777\n",
      "beta[not_omit_indices]: tensor([-0.0508,  0.1672, -0.0886,  0.0411, -0.0565,  0.1003, -0.0256],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.024449419230222702, Beta Similarity: 0.03254488483071327\n",
      "beta[not_omit_indices]: tensor([-0.0542, -0.1913, -0.1621,  0.0797, -0.0262, -0.0628,  0.0902],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.05225798487663269, Beta Similarity: -0.0007145745330490172\n",
      "beta[not_omit_indices]: tensor([ 0.0515,  0.0549, -0.0461,  0.0095, -0.0534,  0.0210, -0.0612],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0044963727705180645, Beta Similarity: 0.036510467529296875\n",
      "backward\n",
      "Epoch:  73, loss_primary = 0.703, loss_watermark = 0.338, B*W = 0.04993, train acc = 0.843, val acc = 0.837\n",
      "beta[not_omit_indices]: tensor([ 0.3160,  0.0699, -0.1957,  0.0075, -0.0570, -0.0594, -0.1367],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.00035173684591427445, Beta Similarity: 0.12033408135175705\n",
      "beta[not_omit_indices]: tensor([ 0.0470,  0.0503, -0.1040, -0.0080, -0.0463, -0.0375,  0.0598],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.012546609155833721, Beta Similarity: 0.031048228964209557\n",
      "beta[not_omit_indices]: tensor([ 0.1619,  0.4033, -0.2066, -0.1375,  0.0818, -0.1323, -0.0464],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0341971293091774, Beta Similarity: 0.10444525629281998\n",
      "beta[not_omit_indices]: tensor([ 0.0176, -0.0028, -0.0675,  0.0918, -0.0789, -0.0573, -0.0323],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.001834019203670323, Beta Similarity: 0.0489436574280262\n",
      "backward\n",
      "Epoch:  74, loss_primary = 0.701, loss_watermark = 0.183, B*W = 0.07619, train acc = 0.824, val acc = 0.841\n",
      "beta[not_omit_indices]: tensor([ 0.2216,  0.0330, -0.1202,  0.0626, -0.0221, -0.0802, -0.2784],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.11688750237226486\n",
      "beta[not_omit_indices]: tensor([ 0.0242,  0.0629,  0.0102, -0.0167, -0.1008, -0.0127,  0.0579],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.016388658434152603, Beta Similarity: 0.01655837520956993\n",
      "beta[not_omit_indices]: tensor([-0.0676,  0.0210, -0.0628, -0.0548,  0.0287, -0.5265,  0.0725],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03765338286757469, Beta Similarity: 0.05524730682373047\n",
      "beta[not_omit_indices]: tensor([-0.0414,  0.0778, -0.0696,  0.0116, -0.0858, -0.0268, -0.0479],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.007341351825743914, Beta Similarity: 0.03973851725459099\n",
      "backward\n",
      "Epoch:  75, loss_primary = 0.928, loss_watermark = 0.230, B*W = 0.05711, train acc = 0.804, val acc = 0.809\n",
      "beta[not_omit_indices]: tensor([ 0.1058,  0.0693, -0.2014,  0.0176,  0.0068, -0.0498, -0.2270],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0023985945153981447, Beta Similarity: 0.09487050026655197\n",
      "beta[not_omit_indices]: tensor([ 0.0242,  0.0587,  0.0184, -0.0237, -0.0998,  0.0010,  0.0629],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02084064856171608, Beta Similarity: 0.0109714949503541\n",
      "beta[not_omit_indices]: tensor([-0.0431, -0.0888, -0.1664, -0.0322, -0.0047, -0.4521,  0.0748],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04060060903429985, Beta Similarity: 0.05490861460566521\n",
      "beta[not_omit_indices]: tensor([-0.0603,  0.0427, -0.0438,  0.0513, -0.0864, -0.1042, -0.0099],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.010054931044578552, Beta Similarity: 0.039709363132715225\n",
      "backward\n",
      "Epoch:  76, loss_primary = 0.995, loss_watermark = 0.277, B*W = 0.05011, train acc = 0.787, val acc = 0.783\n",
      "beta[not_omit_indices]: tensor([ 0.1241,  0.1299, -0.1353, -0.0077, -0.0023, -0.0828, -0.1783],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0036273826844990253, Beta Similarity: 0.09214324504137039\n",
      "beta[not_omit_indices]: tensor([-0.0335, -0.0413, -0.0581, -0.0875, -0.0686,  0.0844, -0.0686],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04095718264579773, Beta Similarity: -0.007342747412621975\n",
      "beta[not_omit_indices]: tensor([ 0.0610, -0.1499, -0.2192, -0.0040,  0.0160, -0.5833,  0.0642],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.039170268923044205, Beta Similarity: 0.08989129960536957\n",
      "beta[not_omit_indices]: tensor([-0.0687,  0.0879, -0.0672, -0.0425, -0.0516, -0.0717, -0.0108],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.018742632120847702, Beta Similarity: 0.025414058938622475\n",
      "backward\n",
      "Epoch:  77, loss_primary = 0.947, loss_watermark = 0.384, B*W = 0.05003, train acc = 0.776, val acc = 0.777\n",
      "beta[not_omit_indices]: tensor([ 0.2105, -0.0026, -0.1774,  0.0601,  0.0288, -0.0484, -0.2428],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.007342885714024305, Beta Similarity: 0.10110848397016525\n",
      "beta[not_omit_indices]: tensor([-0.0007,  0.0412,  0.0104, -0.0306, -0.0720, -0.0001,  0.0465],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01973571814596653, Beta Similarity: 0.003576823743060231\n",
      "beta[not_omit_indices]: tensor([ 0.0569,  0.1402, -0.1331,  0.0092,  0.1057,  0.0453,  0.0796],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.037358153611421585, Beta Similarity: 0.015532493591308594\n",
      "beta[not_omit_indices]: tensor([-0.0144,  0.1064, -0.1090,  0.0173, -0.1125, -0.0269, -0.0620],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0034819685388356447, Beta Similarity: 0.059956688433885574\n",
      "backward\n",
      "Epoch:  78, loss_primary = 0.670, loss_watermark = 0.255, B*W = 0.04504, train acc = 0.831, val acc = 0.833\n",
      "beta[not_omit_indices]: tensor([ 0.0863,  0.0403, -0.1489,  0.0124,  0.0632, -0.0651, -0.1951],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.010463953949511051, Beta Similarity: 0.06926853209733963\n",
      "beta[not_omit_indices]: tensor([-0.0242, -0.0105, -0.0664, -0.0567, -0.0914,  0.0354, -0.1368],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.023843904957175255, Beta Similarity: 0.02396174892783165\n",
      "beta[not_omit_indices]: tensor([-0.0424,  0.0103,  0.0527, -0.0045,  0.0023, -0.0887,  0.0063],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022602053359150887, Beta Similarity: -0.0013227462768554688\n",
      "beta[not_omit_indices]: tensor([ 0.0222,  0.1222, -0.1071,  0.0360, -0.0526, -0.0575, -0.0034],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.000944649800658226, Beta Similarity: 0.0572836734354496\n",
      "backward\n",
      "Epoch:  79, loss_primary = 0.851, loss_watermark = 0.217, B*W = 0.03730, train acc = 0.822, val acc = 0.828\n",
      "beta[not_omit_indices]: tensor([ 0.2262,  0.0054, -0.1656,  0.0274,  0.0141, -0.0952, -0.2602],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.004093377385288477, Beta Similarity: 0.10943412780761719\n",
      "beta[not_omit_indices]: tensor([ 0.0013,  0.0792, -0.1342,  0.0097, -0.0584, -0.0423,  0.0223],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.005902736447751522, Beta Similarity: 0.04326370730996132\n",
      "beta[not_omit_indices]: tensor([-0.0421,  0.0095,  0.0519, -0.0038,  0.0021, -0.0884,  0.0078],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022606516256928444, Beta Similarity: -0.0014051710022613406\n",
      "beta[not_omit_indices]: tensor([ 0.0936,  0.0957, -0.0510,  0.0406, -0.0564, -0.0172, -0.0007],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0013326590415090322, Beta Similarity: 0.05072348564863205\n",
      "backward\n",
      "Epoch:  80, loss_primary = 1.116, loss_watermark = 0.127, B*W = 0.05050, train acc = 0.816, val acc = 0.827\n",
      "beta[not_omit_indices]: tensor([ 0.3123,  0.0631, -0.1932,  0.0430, -0.0944, -0.0170, -0.1385],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.12307196110486984\n",
      "beta[not_omit_indices]: tensor([ 0.0881,  0.1122,  0.1897, -0.0976, -0.0458, -0.2592, -0.0861],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04390580579638481, Beta Similarity: 0.04345232993364334\n",
      "beta[not_omit_indices]: tensor([ 0.0143,  0.0948, -0.0059,  0.0791,  1.1348, -0.0213,  0.0329],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.17025502026081085, Beta Similarity: -0.1360359489917755\n",
      "beta[not_omit_indices]: tensor([ 0.1529,  0.0063, -0.0154, -0.0004, -0.0693, -0.0111, -0.0420],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0020091901533305645, Beta Similarity: 0.0423714779317379\n",
      "backward\n",
      "Epoch:  81, loss_primary = 1.276, loss_watermark = 0.811, B*W = 0.01821, train acc = 0.820, val acc = 0.822\n",
      "beta[not_omit_indices]: tensor([ 3.7424e-01,  7.3749e-02, -2.6478e-01,  4.4128e-02, -8.8589e-02,\n",
      "        -2.7847e-04, -1.7757e-01], grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0013887895038351417, Beta Similarity: 0.14618991315364838\n",
      "beta[not_omit_indices]: tensor([-0.0415, -0.0038,  0.0492, -0.0176,  0.0368, -0.0606, -0.0662],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.028419876471161842, Beta Similarity: -0.003163746325299144\n",
      "beta[not_omit_indices]: tensor([-0.0418,  0.0094,  0.0517, -0.0043,  0.0008, -0.0900,  0.0081],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022487850859761238, Beta Similarity: -0.0010625293944031\n",
      "beta[not_omit_indices]: tensor([-0.0071, -0.0597, -0.0775,  0.0668, -0.0950, -0.0129,  0.0478],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.020644770935177803, Beta Similarity: 0.019671304151415825\n",
      "backward\n",
      "Epoch:  82, loss_primary = 1.240, loss_watermark = 0.274, B*W = 0.04041, train acc = 0.823, val acc = 0.827\n",
      "beta[not_omit_indices]: tensor([ 0.4054,  0.1264, -0.2044, -0.0009, -0.1195,  0.0493, -0.2429],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.010039127431809902, Beta Similarity: 0.1497640609741211\n",
      "beta[not_omit_indices]: tensor([ 0.0647,  0.0135, -0.0091,  0.0683,  0.0230,  0.0921,  0.0361],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02601229026913643, Beta Similarity: 0.0006146942032501101\n",
      "beta[not_omit_indices]: tensor([-0.0443,  0.0074,  0.0459, -0.0033,  0.0035, -0.0874,  0.0079],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.022514553740620613, Beta Similarity: -0.001458985498175025\n",
      "beta[not_omit_indices]: tensor([ 0.1031,  0.0188, -0.0081,  0.0056, -0.0664, -0.0180, -0.0402],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.000912737101316452, Beta Similarity: 0.03714752197265625\n",
      "backward\n",
      "Epoch:  83, loss_primary = 1.034, loss_watermark = 0.223, B*W = 0.04652, train acc = 0.830, val acc = 0.831\n",
      "beta[not_omit_indices]: tensor([ 0.3671,  0.0461, -0.2487,  0.0230, -0.0353, -0.0481, -0.1047],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.12474047392606735\n",
      "beta[not_omit_indices]: tensor([-0.0025,  0.1254, -0.0367, -0.0097,  0.0065,  0.0085,  0.0401],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01674935407936573, Beta Similarity: 0.013553074561059475\n",
      "beta[not_omit_indices]: tensor([-0.1308, -0.2276, -0.2291,  0.0624, -0.0574, -0.1165,  0.2011],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.08422160893678665, Beta Similarity: -0.013454871252179146\n",
      "beta[not_omit_indices]: tensor([ 0.2459,  0.0412,  0.0090, -0.0026, -0.0484,  0.0127, -0.0166],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0077679879032075405, Beta Similarity: 0.04680633544921875\n",
      "backward\n",
      "Epoch:  84, loss_primary = 1.022, loss_watermark = 0.408, B*W = 0.04291, train acc = 0.814, val acc = 0.822\n",
      "beta[not_omit_indices]: tensor([ 0.2009,  0.0247, -0.2638,  0.0741, -0.1096, -0.0279, -0.0484],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.1070544570684433\n",
      "beta[not_omit_indices]: tensor([-0.0376, -0.0524,  0.0778, -0.0588, -0.0699, -0.2569, -0.0483],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03807517886161804, Beta Similarity: 0.021208355203270912\n",
      "beta[not_omit_indices]: tensor([-0.2181, -0.0796,  0.0084,  0.1015, -0.0654, -0.3903,  0.0615],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.05823585391044617, Beta Similarity: 0.027077538892626762\n",
      "beta[not_omit_indices]: tensor([ 0.0676,  0.0419, -0.0590,  0.0467,  0.0019, -0.0238,  0.0560],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.011135035194456577, Beta Similarity: 0.025846276432275772\n",
      "backward\n",
      "Epoch:  85, loss_primary = 0.923, loss_watermark = 0.403, B*W = 0.04530, train acc = 0.827, val acc = 0.833\n",
      "beta[not_omit_indices]: tensor([ 0.1453,  0.0512, -0.2522,  0.0667, -0.1841,  0.0377, -0.0241],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.006814923603087664, Beta Similarity: 0.09798182547092438\n",
      "beta[not_omit_indices]: tensor([-0.0179, -0.0263,  0.0186,  0.0159,  0.0502,  0.0437,  0.0269],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03480618819594383, Beta Similarity: -0.023969922214746475\n",
      "beta[not_omit_indices]: tensor([-0.0684, -0.1163, -0.0826,  0.0833, -0.0448, -0.2021,  0.0762],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04155094549059868, Beta Similarity: 0.021711213514208794\n",
      "beta[not_omit_indices]: tensor([ 0.0744,  0.0059, -0.0289,  0.0659, -0.0982, -0.0147,  0.0134],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.003921988420188427, Beta Similarity: 0.0392477847635746\n",
      "backward\n",
      "Epoch:  86, loss_primary = 0.638, loss_watermark = 0.327, B*W = 0.03374, train acc = 0.857, val acc = 0.847\n",
      "beta[not_omit_indices]: tensor([ 0.1112,  0.0811, -0.2549,  0.0608, -0.0939,  0.0058, -0.0452],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0022612654138356447, Beta Similarity: 0.09159643203020096\n",
      "beta[not_omit_indices]: tensor([-0.0640, -0.0198, -0.0358,  0.0021, -0.0872, -0.0107,  0.0023],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.017711235210299492, Beta Similarity: 0.007094519678503275\n",
      "beta[not_omit_indices]: tensor([-0.1346,  0.0197,  0.0480, -0.0245,  0.0116, -0.0143, -0.0080],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03725042566657066, Beta Similarity: -0.025249004364013672\n",
      "beta[not_omit_indices]: tensor([ 0.1730,  0.0317, -0.0264,  0.0451, -0.0788, -0.0478, -0.0434],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.0637425035238266\n",
      "backward\n",
      "Epoch:  87, loss_primary = 0.584, loss_watermark = 0.215, B*W = 0.03430, train acc = 0.867, val acc = 0.860\n",
      "beta[not_omit_indices]: tensor([ 0.2371,  0.0981, -0.2568,  0.0567, -0.2215,  0.0267, -0.0506],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0052421786822378635, Beta Similarity: 0.1277214139699936\n",
      "beta[not_omit_indices]: tensor([-0.0724, -0.0188,  0.0050, -0.0004, -0.0529,  0.0404, -0.0168],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.02669808641076088, Beta Similarity: -0.0095977783203125\n",
      "beta[not_omit_indices]: tensor([-0.1919,  0.0119,  0.0193,  0.0896,  0.0517, -0.0271, -0.1267],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04185176268219948, Beta Similarity: -0.0010869161924347281\n",
      "beta[not_omit_indices]: tensor([ 0.1268,  0.0075, -0.0198,  0.0152, -0.0811, -0.0094, -0.0236],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.000439714640378952, Beta Similarity: 0.0404902882874012\n",
      "backward\n",
      "Epoch:  88, loss_primary = 0.593, loss_watermark = 0.278, B*W = 0.03938, train acc = 0.857, val acc = 0.843\n",
      "beta[not_omit_indices]: tensor([ 0.1769,  0.0451, -0.2980,  0.0421, -0.1052, -0.0814,  0.0048],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0021130370441824198, Beta Similarity: 0.10629135370254517\n",
      "beta[not_omit_indices]: tensor([-0.0306, -0.0003, -0.0107, -0.0012, -0.0377, -0.0142,  0.0072],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.011324205435812473, Beta Similarity: 0.0033374514896422625\n",
      "beta[not_omit_indices]: tensor([-4.5769e-02, -8.5484e-02, -1.3757e-01,  4.5776e-05, -5.2724e-02,\n",
      "        -1.7332e-01,  9.0622e-02], grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03740406408905983, Beta Similarity: 0.020255710929632187\n",
      "beta[not_omit_indices]: tensor([ 0.1436,  0.0086, -0.0104, -0.0056, -0.0715, -0.0092, -0.0431],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0025483921635895967, Beta Similarity: 0.04010990634560585\n",
      "backward\n",
      "Epoch:  89, loss_primary = 0.686, loss_watermark = 0.200, B*W = 0.04250, train acc = 0.841, val acc = 0.821\n",
      "beta[not_omit_indices]: tensor([ 0.1931,  0.1211, -0.1361,  0.0221, -0.0767, -0.0654, -0.1873],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.11454810202121735\n",
      "beta[not_omit_indices]: tensor([-0.0947, -0.0336, -0.0312, -0.0462, -0.0283, -0.1066,  0.0080],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.031785015016794205, Beta Similarity: -0.0023591178469359875\n",
      "beta[not_omit_indices]: tensor([-0.0911,  0.0003, -0.0248,  0.0614, -0.0421, -0.1603, -0.1192],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.015823299065232277, Beta Similarity: 0.04528876766562462\n",
      "beta[not_omit_indices]: tensor([ 0.1561, -0.0052, -0.0233,  0.0142, -0.0739, -0.0160, -0.0408],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0021697126794606447, Beta Similarity: 0.0455954410135746\n",
      "backward\n",
      "Epoch:  90, loss_primary = 0.798, loss_watermark = 0.187, B*W = 0.05077, train acc = 0.822, val acc = 0.795\n",
      "beta[not_omit_indices]: tensor([ 0.2091,  0.0368, -0.1701,  0.0328, -0.0738, -0.0330, -0.2294],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.11215387284755707\n",
      "beta[not_omit_indices]: tensor([-0.0165, -0.0036, -0.0042, -0.0082, -0.0247, -0.0313,  0.0012],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.010748631320893764, Beta Similarity: 0.004391465801745653\n",
      "beta[not_omit_indices]: tensor([-0.0355,  0.0162,  0.0453,  0.0038,  0.0032, -0.0897, -0.0055],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01780547946691513, Beta Similarity: 0.004471915308386087\n",
      "beta[not_omit_indices]: tensor([ 0.1251,  0.0011, -0.0350,  0.0175, -0.0722, -0.0168, -0.0428],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.001269444008357823, Beta Similarity: 0.0443769171833992\n",
      "backward\n",
      "Epoch:  91, loss_primary = 0.934, loss_watermark = 0.112, B*W = 0.04135, train acc = 0.815, val acc = 0.815\n",
      "beta[not_omit_indices]: tensor([ 0.1339,  0.0383, -0.1248, -0.0008,  0.0612, -0.0035, -0.3404],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.012654068879783154, Beta Similarity: 0.08270563185214996\n",
      "beta[not_omit_indices]: tensor([-0.0614, -0.0101, -0.0305,  0.0044, -0.0732, -0.0321, -0.0012],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.015134270302951336, Beta Similarity: 0.009993280284106731\n",
      "beta[not_omit_indices]: tensor([-0.0340,  0.0134,  0.0414,  0.0040,  0.0042, -0.0896, -0.0036],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.017431775107979774, Beta Similarity: 0.004419735632836819\n",
      "beta[not_omit_indices]: tensor([ 0.1633,  0.0242,  0.0097,  0.0341, -0.0767, -0.0409, -0.0434],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0028193010948598385, Beta Similarity: 0.05326516181230545\n",
      "backward\n",
      "Epoch:  92, loss_primary = 0.764, loss_watermark = 0.180, B*W = 0.03760, train acc = 0.837, val acc = 0.830\n",
      "beta[not_omit_indices]: tensor([ 0.1217, -0.0546, -0.0811,  0.0021, -0.0050, -0.0092, -0.1582],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.011188103817403316, Beta Similarity: 0.04609053581953049\n",
      "beta[not_omit_indices]: tensor([-0.0336, -0.0017, -0.0169, -0.0003, -0.0435, -0.0150,  0.0069],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.011793822050094604, Beta Similarity: 0.004702976904809475\n",
      "beta[not_omit_indices]: tensor([ 0.3693,  0.0486, -0.3438,  0.1204, -0.0126, -0.5512, -0.2082],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0, Beta Similarity: 0.23631538450717926\n",
      "beta[not_omit_indices]: tensor([ 0.1752,  0.0063, -0.0183,  0.0229, -0.0667, -0.0154, -0.0410],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0005304826772771776, Beta Similarity: 0.04939815029501915\n",
      "backward\n",
      "Epoch:  93, loss_primary = 0.667, loss_watermark = 0.088, B*W = 0.08413, train acc = 0.844, val acc = 0.844\n",
      "beta[not_omit_indices]: tensor([ 0.1663,  0.0167, -0.0399, -0.0021, -0.0043, -0.0661, -0.1910],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0025424303021281958, Beta Similarity: 0.06889670342206955\n",
      "beta[not_omit_indices]: tensor([-0.0310, -0.0023, -0.0110, -0.0002, -0.0335, -0.0209,  0.0056],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.011296548880636692, Beta Similarity: 0.003756114514544606\n",
      "beta[not_omit_indices]: tensor([ 0.5011,  0.5167, -0.5994, -0.2136,  0.0335, -0.4310,  0.0877],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.05211629718542099, Beta Similarity: 0.2447708696126938\n",
      "beta[not_omit_indices]: tensor([ 0.1181,  0.0033, -0.0240,  0.0200, -0.0891, -0.0117, -0.0343],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0009620884084142745, Beta Similarity: 0.0429404117166996\n",
      "backward\n",
      "Epoch:  94, loss_primary = 0.582, loss_watermark = 0.251, B*W = 0.09009, train acc = 0.854, val acc = 0.870\n",
      "beta[not_omit_indices]: tensor([ 0.1188,  0.0159, -0.0989, -0.0213, -0.0534, -0.0415, -0.2281],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.004470213316380978, Beta Similarity: 0.07646291702985764\n",
      "beta[not_omit_indices]: tensor([-0.0105,  0.0339,  0.0275, -0.0209,  0.0631,  0.0018, -0.0070],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.025256503373384476, Beta Similarity: -0.011841501109302044\n",
      "beta[not_omit_indices]: tensor([ 0.0305,  0.2877, -0.1165, -0.2485,  0.0425, -0.4323, -0.0545],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.044428348541259766, Beta Similarity: 0.0900626853108406\n",
      "beta[not_omit_indices]: tensor([ 0.1347,  0.0288,  0.0059,  0.0337, -0.0894, -0.0295, -0.0400],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0022765242028981447, Beta Similarity: 0.05003247782588005\n",
      "backward\n",
      "Epoch:  95, loss_primary = 0.635, loss_watermark = 0.287, B*W = 0.05118, train acc = 0.831, val acc = 0.805\n",
      "beta[not_omit_indices]: tensor([ 0.1658, -0.0469, -0.0475, -0.0323, -0.0499, -0.0145, -0.2437],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.014164075255393982, Beta Similarity: 0.06317833811044693\n",
      "beta[not_omit_indices]: tensor([-0.0143,  0.0174, -0.0040, -0.0085,  0.0104, -0.0162,  0.0090],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.012601296417415142, Beta Similarity: -0.0006663628737442195\n",
      "beta[not_omit_indices]: tensor([ 0.0637, -0.0114, -0.0028, -0.0459,  0.0403, -0.4287, -0.0914],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.01926531456410885, Beta Similarity: 0.06984914839267731\n",
      "beta[not_omit_indices]: tensor([ 0.1759,  0.0339,  0.0121,  0.0398, -0.0584, -0.0490, -0.0414],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0031549944542348385, Beta Similarity: 0.0551866814494133\n",
      "backward\n",
      "Epoch:  96, loss_primary = 0.735, loss_watermark = 0.184, B*W = 0.04689, train acc = 0.815, val acc = 0.795\n",
      "beta[not_omit_indices]: tensor([ 0.0857,  0.0254,  0.0018,  0.0294, -0.0655,  0.0692, -0.2148],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.013000966981053352, Beta Similarity: 0.04995795711874962\n",
      "beta[not_omit_indices]: tensor([-0.1210, -0.0301,  0.0024, -0.0504,  0.0065, -0.2610,  0.0085],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0398595817387104, Beta Similarity: 0.005993434228003025\n",
      "beta[not_omit_indices]: tensor([ 0.0769,  0.5172, -0.1909, -0.2052,  0.0844, -0.5043, -0.0697],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.04422890022397041, Beta Similarity: 0.15276268124580383\n",
      "beta[not_omit_indices]: tensor([ 0.0960,  0.0294, -0.0027,  0.0609, -0.0899, -0.0209, -0.0086],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0012353515485301614, Beta Similarity: 0.04405321553349495\n",
      "backward\n",
      "Epoch:  97, loss_primary = 0.643, loss_watermark = 0.369, B*W = 0.06319, train acc = 0.833, val acc = 0.825\n",
      "beta[not_omit_indices]: tensor([ 0.1227,  0.0477, -0.0841, -0.0207, -0.0698, -0.0544, -0.1512],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.004385915584862232, Beta Similarity: 0.07274259626865387\n",
      "beta[not_omit_indices]: tensor([-0.0168,  0.0072, -0.0062, -0.0029, -0.0118, -0.0181,  0.0087],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.00928550772368908, Beta Similarity: 0.0021318537183105946\n",
      "beta[not_omit_indices]: tensor([ 0.0269,  0.5300, -0.3621, -0.0159, -0.0520, -0.3897,  0.0093],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.006462576799094677, Beta Similarity: 0.19078609347343445\n",
      "beta[not_omit_indices]: tensor([ 0.1791,  0.0223,  0.0149,  0.0296, -0.0482, -0.0221, -0.0449],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.0035604422446340322, Beta Similarity: 0.0473284050822258\n",
      "backward\n",
      "Epoch:  98, loss_primary = 0.584, loss_watermark = 0.089, B*W = 0.07825, train acc = 0.849, val acc = 0.856\n",
      "beta[not_omit_indices]: tensor([ 0.1215,  0.0736, -0.1181, -0.0207, -0.0568, -0.0568, -0.2416],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.004382509272545576, Beta Similarity: 0.09251846373081207\n",
      "beta[not_omit_indices]: tensor([-0.0168,  0.0078, -0.0066, -0.0032, -0.0110, -0.0179,  0.0089],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.009224762208759785, Beta Similarity: 0.002053226809948683\n",
      "beta[not_omit_indices]: tensor([ 0.1863,  0.7889, -0.3687, -0.1510,  0.0012, -0.4731,  0.0942],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.03948637843132019, Beta Similarity: 0.2243785858154297\n",
      "beta[not_omit_indices]: tensor([ 0.1748,  0.0305,  0.0182,  0.0461, -0.0552, -0.0453, -0.0319],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Subgraph: Loss Watermark: 0.004031284712255001, Beta Similarity: 0.052225928753614426\n",
      "backward\n",
      "Epoch:  99, loss_primary = 0.571, loss_watermark = 0.214, B*W = 0.09279, train acc = 0.860, val acc = 0.872\n",
      "Node classifier, history, subgraph dict, feature importances, watermark indices, and probas saved in:\n",
      "/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/training_results/computers/archGCN_elu_nLayers3_hDim256_drop0_skipTrue/_1%UnimportantIndices_average_20ClfEpochs_khop1_fraction0.01_numSubgraphs4_maxDegree40_eps0.01_raw_beta_nodeMixUp40_lr0.002_epochs100_coefWmk15_sacrifice1subNodes_regressionLambda0.0001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for mom in [1e-3, 1e-2, 1e-1, 0.5]:\n",
    "#     for rho in [1e-7,1e-5,1e-3]:\n",
    "# config.optimization_kwargs['sam_momentum'] = mom\n",
    "# config.optimization_kwargs['sam_rho'] = rho\n",
    "# for i in range(10):\n",
    "#     if i<5:\n",
    "#         config.optimization_kwargs['use_sam']=True\n",
    "#         title = f'With SAM - momentum={np.round(config.optimization_kwargs['sam_momentum'],3)}, rho={config.optimization_kwargs['sam_rho']}'\n",
    "#     else:\n",
    "# config.optimization_kwargs['use_sam']=False\n",
    "title = f'Without SAM'\n",
    "Trainer_ = Trainer(data, dataset_name)\n",
    "node_classifier, history, subgraph_dict, \\\n",
    "all_feature_importances, all_watermark_indices, probas = Trainer_.train(debug_multiple_subgraphs=True, save=True, print_every=1, perturb=False, perturb_lr=1e-3)\n",
    "\n",
    "primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "percent_matches, percent_match_mean, percent_match_std, \\\n",
    "    primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "\n",
    "final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup_subgraph_dict\n",
      "nodeIndices: [998, 1784, 2215, 2253, 3366, 3875, 4739, 5041, 5713, 6026, 8210, 9010, 10408, 10901, 10941, 12729, 13094]\n",
      "nodeIndices: [227, 421, 1243, 1418, 2611, 3924, 4165, 5250, 6013, 8214, 8417, 8528, 11598, 12337, 12888, 13529, 13708]\n",
      "nodeIndices: [473, 648, 1322, 1549, 3896, 4153, 4862, 6314, 6425, 9446, 9982, 10250, 10611, 11136, 12189, 12758, 12836]\n",
      "nodeIndices: [505, 872, 1524, 2316, 4257, 5060, 5408, 6120, 6544, 7439, 7984, 8266, 8376, 8460, 8841, 12592, 12850]\n",
      "nodeIndices: [400, 830, 1660, 3799, 3892, 4427, 4578, 5510, 6546, 7485, 7748, 9203, 9529, 9574, 9937, 10474, 13304]\n",
      "nodeIndices: [830]\n",
      "nodeIndices: [892, 1093, 1189, 2497, 3915, 4401, 4920, 5927, 6671, 7172, 8280, 8367, 8734, 9717, 10725, 10793, 13573]\n",
      "Sacrificing 100% of subgraph nodes from node classification training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea3690148a24aa7b05b2929252931e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "''' (individualize boolean, multisubgraph method, index selection method) '''\n",
    "selection_kwargss = [#(False, None, 'random'), # random, not individualized\n",
    "                    (False,'average','unimportant'), # average, not individualized\n",
    "                    #(False,'concat','unimportant'), # concat, not inividualized\n",
    "                    # (True,None,'unimportant') # unimportant, individualized\n",
    "                    ]\n",
    "\n",
    "''' (method, regenerate_boolean) '''\n",
    "subgraph_methods = [('random',False)]\n",
    "\n",
    "''' [subset, percentage] '''\n",
    "sacrifice_kwargss = [['train_node_indices',1],['train_node_indices',0.75],['subgraph_node_indices',1],[None,None]]\n",
    "\n",
    "''' (method, L2_lambda (if applicable, else None) '''\n",
    "regularization_kwargs = [(None,None),('L2',0.01),('beta_var',None)]\n",
    "\n",
    "\n",
    "variables = {'augment': [{'separate_trainset_from_subgraphs': True,\n",
    "                          'ignore_subgraphs': True,\n",
    "                          'nodeDrop': False,\n",
    "                          'nodeMixUp': False,\n",
    "                          'nodeFeatMask': False,\n",
    "                          'edgeDrop': False},\n",
    "                          {'separate_trainset_from_subgraphs': True,\n",
    "                          'ignore_subgraphs': True,\n",
    "                          'nodeDrop': True,\n",
    "                          'nodeMixUp': True,\n",
    "                          'nodeFeatMask': True,\n",
    "                          'edgeDrop': True},],\n",
    "             'sacrifice_kwargs':  sacrifice_kwargss,\n",
    "             'beta_selection': selection_kwargss,\n",
    "             'use_PCgrad': [True,False],\n",
    "             'subgraph_method': subgraph_methods,\n",
    "             'reg': regularization_kwargs,\n",
    "             'perc': [3],\n",
    "             'clf_epochs': [10,20],\n",
    "             'coef_wmk': [200,600],\n",
    "             'frac': [0.02],\n",
    "             'num_subgraphs': [2,7],\n",
    "             'balance_beta_weights': [True,False],\n",
    "             'ignore_subgraph_neighbors': [True, False]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# config.optimization_kwargs['clf_only']=False       \n",
    "# watermarking_order = ['augment','use_PCgrad','sacrifice_kwargs','beta_selection','subgraph_method','reg','perc','clf_epochs','frac','coef_wmk','num_subgraphs','balance_beta_weights','ignore_subgraph_neighbors']\n",
    "# print(\"watermarking:\", watermarking_order)\n",
    "# count, [node_classifier, history, subgraph_dict,\\\n",
    "#          all_feature_importances, all_watermark_indices, probas] = dynamic_grid_search(data, dataset_name, debug_multiple_subgraphs, \n",
    "#                                                                                        all_dfs, False, variables, watermarking_order,\n",
    "#                                                                                        count_only=True)\n",
    "# print(count)\n",
    "\n",
    "\n",
    "config.augment_kwargs['separate_trainset_from_subgraphs'] = True\n",
    "config.augment_kwargs['ignore_subgraphs'] = True\n",
    "perturb=False\n",
    "perturb_lr = 1e-3\n",
    "node_classifier, history, subgraph_dict, \\\n",
    "    all_feature_importances, all_watermark_indices, probas = train(data, dataset_name, debug_multiple_subgraphs=False, save=True, print_every=1,perturb=perturb,perturb_lr=perturb_lr)\n",
    "primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "    percent_matches, percent_match_mean, percent_match_std, \\\n",
    "        primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "final_plot(history, '', percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got builtin_function_or_method)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got builtin_function_or_method)"
     ]
    }
   ],
   "source": [
    "data.x[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/training_results/computers/archGCN_elu_nLayers3_hDim256_drop0_skipTrue/_3%UnimportantIndices_average_10ClfEpochs_random_fraction0.01_numSubgraphs10_eps0.1_raw_beta_nodeMixUp40_lr0.002_epochs80_coefWmk200_regressionLambda0.1'\n",
    "probas = pickle.load(open(f'{folder}/probas','rb'))\n",
    "subgraph_dict = pickle.load(open(f'{folder}/subgraph_dict','rb'))\n",
    "all_watermark_indices = pickle.load(open(f'{folder}/all_watermark_indices','rb'))\n",
    "node_classifier = pickle.load(open(f'{folder}/node_classifier','rb'))\n",
    "history = pickle.load(open(f'{folder}/history','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlopt\n",
      "  Downloading nlopt-2.6.2.tar.gz (2.0 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages (from nlopt) (1.26.4)\n",
      "Building wheels for collected packages: nlopt\n",
      "  Building wheel for nlopt (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m>\u001b[0m \u001b[31m[215 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m cmake version 3.26.4\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m CMake suite maintained and supported by Kitware (kitware.com/cmake).\n",
      "  \u001b[31m   \u001b[0m -- The C compiler identification is AppleClang 15.0.0.15000309\n",
      "  \u001b[31m   \u001b[0m -- The CXX compiler identification is AppleClang 15.0.0.15000309\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Found PythonInterp: /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/bin/python (found version \"3.12\")\n",
      "  \u001b[31m   \u001b[0m -- Found Python includes: /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/include/python3.12\n",
      "  \u001b[31m   \u001b[0m -- Found Python libs: /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib\n",
      "  \u001b[31m   \u001b[0m \u001b[0mCMake Deprecation Warning at extern/nlopt/CMakeLists.txt:15 (cmake_minimum_required):\n",
      "  \u001b[31m   \u001b[0m   Compatibility with CMake < 2.8.12 will be removed from a future version of\n",
      "  \u001b[31m   \u001b[0m   CMake.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  \u001b[31m   \u001b[0m   CMake that the project does not need compatibility with older versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m -- NLopt version 2.6.2\n",
      "  \u001b[31m   \u001b[0m -- Looking for dlfcn.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for dlfcn.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for getopt.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for getopt.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for unistd.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for unistd.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for string.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for string.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for strings.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for strings.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for inttypes.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for inttypes.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for memory.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for memory.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for stdlib.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for stdlib.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for stdint.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for stdint.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for time.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for time.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/types.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/types.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/stat.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/stat.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/time.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/time.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for getpid\n",
      "  \u001b[31m   \u001b[0m -- Looking for getpid - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for syscall\n",
      "  \u001b[31m   \u001b[0m -- Looking for syscall - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for isinf\n",
      "  \u001b[31m   \u001b[0m -- Looking for isinf - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for isnan\n",
      "  \u001b[31m   \u001b[0m -- Looking for isnan - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for gettimeofday\n",
      "  \u001b[31m   \u001b[0m -- Looking for gettimeofday - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for qsort_r\n",
      "  \u001b[31m   \u001b[0m -- Looking for qsort_r - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for time\n",
      "  \u001b[31m   \u001b[0m -- Looking for time - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for copysign\n",
      "  \u001b[31m   \u001b[0m -- Looking for copysign - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for stddef.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for stddef.h - found\n",
      "  \u001b[31m   \u001b[0m -- Check size of uint32_t\n",
      "  \u001b[31m   \u001b[0m -- Check size of uint32_t - done\n",
      "  \u001b[31m   \u001b[0m -- Check size of unsigned int\n",
      "  \u001b[31m   \u001b[0m -- Check size of unsigned int - done\n",
      "  \u001b[31m   \u001b[0m -- Check size of unsigned long\n",
      "  \u001b[31m   \u001b[0m -- Check size of unsigned long - done\n",
      "  \u001b[31m   \u001b[0m -- Looking for sqrt in m\n",
      "  \u001b[31m   \u001b[0m -- Looking for sqrt in m - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for fpclassify\n",
      "  \u001b[31m   \u001b[0m -- Looking for fpclassify - TRUE\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAVE_THREAD_LOCAL_STORAGE\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAVE_THREAD_LOCAL_STORAGE - Success\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAVE_THREAD_LOCAL_STORAGE\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAVE_THREAD_LOCAL_STORAGE - Failed\n",
      "  \u001b[31m   \u001b[0m -- Looking for __cplusplus\n",
      "  \u001b[31m   \u001b[0m -- Looking for __cplusplus - found\n",
      "  \u001b[31m   \u001b[0m -- Performing Test SUPPORTS_STDCXX11\n",
      "  \u001b[31m   \u001b[0m -- Performing Test SUPPORTS_STDCXX11 - Success\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAS_FPIC\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAS_FPIC - Success\n",
      "  \u001b[31m   \u001b[0m -- Found PythonLibs: /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib (found suitable exact version \"3.12.0\")\n",
      "  \u001b[31m   \u001b[0m -- Found NumPy: /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/numpy/core/include (found version \"1.26.4\")\n",
      "  \u001b[31m   \u001b[0m error: (\"/opt/homebrew/opt/pkg-config/bin/pkg-config\" \"--variable=prefix\" \"guile-3.0\") exited with non-zero error code 127\n",
      "  \u001b[31m   \u001b[0m error: (\"/opt/homebrew/opt/pkg-config/bin/pkg-config\" \"--variable=sitedir\" \"guile-3.0\") exited with non-zero error code 127\n",
      "  \u001b[31m   \u001b[0m error: (\"/opt/homebrew/opt/pkg-config/bin/pkg-config\" \"--variable=extensiondir\" \"guile-3.0\") exited with non-zero error code 127\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find Guile (missing: GUILE_ROOT_DIR GUILE_INCLUDE_DIRS GUILE_LIBRARIES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find SWIG (missing: SWIG_EXECUTABLE SWIG_DIR)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find Octave (missing: OCTAVE_EXECUTABLE OCTAVE_ROOT_DIR OCTAVE_INCLUDE_DIRS OCTAVE_LIBRARIES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find Matlab (missing: Matlab_INCLUDE_DIRS Matlab_MEX_LIBRARY Matlab_MEX_EXTENSION Matlab_ROOT_DIR Matlab_MX_LIBRARY MX_LIBRARY MAIN_PROGRAM) (found version \"NOTFOUND\")\n",
      "  \u001b[31m   \u001b[0m -- Configuring done (10.2s)\n",
      "  \u001b[31m   \u001b[0m -- Generating done (0.0s)\n",
      "  \u001b[31m   \u001b[0m -- Build files have been written to: /private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/build/temp.macosx-11.0-arm64-cpython-312\n",
      "  \u001b[31m   \u001b[0m [  3%] \u001b[34m\u001b[1mGenerating nlopt.hpp\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [  3%] \u001b[34m\u001b[1mGenerating nlopt.f\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[0mCMake Deprecation Warning at /private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/extern/nlopt/cmake/generate-fortran.cmake:1 (cmake_minimum_required):\n",
      "  \u001b[31m   \u001b[0m   Compatibility with CMake < 2.8.12 will be removed from a future version of\n",
      "  \u001b[31m   \u001b[0m   CMake.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  \u001b[31m   \u001b[0m   CMake that the project does not need compatibility with older versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\u001b[0mCMake Deprecation Warning at /private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/extern/nlopt/cmake/generate-cpp.cmake:1 (cmake_minimum_required):\n",
      "  \u001b[31m   \u001b[0m   Compatibility with CMake < 2.8.12 will be removed from a future version of\n",
      "  \u001b[31m   \u001b[0m   CMake.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  \u001b[31m   \u001b[0m   CMake that the project does not need compatibility with older versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m [  3%] Built target generate-fortran\n",
      "  \u001b[31m   \u001b[0m [  3%] Built target generate-cpp\n",
      "  \u001b[31m   \u001b[0m [  7%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/direct/DIRect.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [  7%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/direct/direct_wrap.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [  9%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/direct/DIRserial.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 11%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/direct/DIRsubrout.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 13%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/cdirect/cdirect.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 15%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/cdirect/hybrid.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 17%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/praxis/praxis.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 19%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/luksan/plis.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 21%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/luksan/plip.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 23%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/luksan/pnet.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 25%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/luksan/mssubs.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 27%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/luksan/pssubs.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 29%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/crs/crs.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 31%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/mlsl/mlsl.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 33%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/mma/mma.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 35%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/mma/ccsa_quadratic.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 37%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/cobyla/cobyla.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 39%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/newuoa/newuoa.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 41%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/neldermead/nldrmd.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 43%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/neldermead/sbplx.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 45%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/auglag/auglag.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 47%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/bobyqa/bobyqa.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 49%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/isres/isres.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 50%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/slsqp/slsqp.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 52%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/esch/esch.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 54%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/api/general.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 56%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/api/options.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 58%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/api/optimize.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 60%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/api/deprecated.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 62%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/api/f77api.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 64%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/mt19937ar.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 66%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/sobolseq.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 68%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/timer.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 70%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/stop.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 72%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/redblack.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 74%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/qsort_r.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 76%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/rescale.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 78%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/stogo/global.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 80%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/stogo/linalg.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 82%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/stogo/local.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 84%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/stogo/stogo.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 86%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/stogo/tools.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 88%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/ags/evolvent.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 90%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/ags/solver.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 92%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/ags/local_optimizer.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 94%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/ags/ags.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 96%] \u001b[32m\u001b[1mLinking CXX static library libnlopt.a\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [100%] Built target nlopt\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/setup.py\", line 85, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/__init__.py\", line 104, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/wheel/bdist_wheel.py\", line 368, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/setup.py\", line 28, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/setup.py\", line 70, in build_extension\n",
      "  \u001b[31m   \u001b[0m     nlopt_py = next(Path(self.build_temp).rglob(\"nlopt.py\"))\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m StopIteration\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for nlopt\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for nlopt\n",
      "Failed to build nlopt\n",
      "\u001b[31mERROR: Could not build wheels for nlopt, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nlopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlopt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define your objective function\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nlopt'"
     ]
    }
   ],
   "source": [
    "import nlopt\n",
    "import numpy as np\n",
    "\n",
    "# Define your objective function\n",
    "def objective(x, grad):\n",
    "    if grad.size > 0:\n",
    "        # Compute gradient if needed\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).reshape(original_shape)\n",
    "        x_tensor.requires_grad = True\n",
    "        log_logits_sub = node_classifier(x_tensor, edge_index)\n",
    "        probas_sub = log_logits_sub.clone().exp()\n",
    "\n",
    "        omit_indices, not_omit_indices = get_omit_indices(x_tensor, this_watermark, ignore_zeros_from_subgraphs=False)\n",
    "        raw_beta = solve_regression(x_tensor, probas_sub, regression_kwargs['lambda'])\n",
    "        beta = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta * this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "        balanced_beta_weights = torch.ones_like(B_x_W)\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        loss = torch.mean(torch.clamp(watermark_loss_kwargs['epsilon'] - B_x_W, min=0) * balanced_beta_weights)\n",
    "        loss.backward()\n",
    "\n",
    "        grad[:] = x_tensor.grad.numpy().flatten()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_minimize_lbfgsb options: 2 2\n",
      "_prepare_scalar_function\n",
      "C\n",
      "D\n",
      "F\n",
      "init\n",
      "update_fun: <function ScalarFunction.__init__.<locals>.update_fun at 0x3170d3880>\n",
      "fun_wrapped\n",
      "args: ()\n",
      "FD_METHODS\n",
      "finite_diff_options: {'method': '2-point', 'rel_step': None, 'abs_step': 1e-08, 'bounds': (array([-inf, -inf, -inf, ..., -inf, -inf, -inf]), array([inf, inf, inf, ..., inf, inf, inf]))}\n",
      "Help on function update_grad in module scipy.optimize._differentiable_functions:\n",
      "\n",
      "update_grad()\n",
      "\n",
      "update_grad: None\n",
      "not self.g_updated\n",
      "approx deriv\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m110\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m}\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Use the L-BFGS-B algorithm for optimization\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_watermark_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_sub_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# The optimal x found by the optimizer\u001b[39;00m\n\u001b[1;32m     45\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:308\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 308\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    314\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_optimize.py:390\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 390\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:188\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate_grad:\u001b[39m\u001b[38;5;124m'\u001b[39m,help(update_grad))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad_impl \u001b[38;5;241m=\u001b[39m update_grad\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Hessian Evaluation\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(hess):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:271\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot self.g_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:183\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapprox deriv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    574\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[1;32m    575\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    578\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 456\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:141\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs:\u001b[39m\u001b[38;5;124m'\u001b[39m,args)\n\u001b[0;32m--> 141\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mcompute_watermark_loss\u001b[0;34m(x_sub_flat)\u001b[0m\n\u001b[1;32m     21\u001b[0m log_logits_sub \u001b[38;5;241m=\u001b[39m node_classifier(x_sub, edge_index)\n\u001b[1;32m     22\u001b[0m probas_sub \u001b[38;5;241m=\u001b[39m log_logits_sub\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m---> 25\u001b[0m omit_indices,not_omit_indices \u001b[38;5;241m=\u001b[39m \u001b[43mget_omit_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_watermark\u001b[49m\u001b[43m,\u001b[49m\u001b[43mignore_zeros_from_subgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#indices where watermark is 0\u001b[39;00m\n\u001b[1;32m     26\u001b[0m raw_beta            \u001b[38;5;241m=\u001b[39m solve_regression(x_sub, probas_sub, regression_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m beta                \u001b[38;5;241m=\u001b[39m process_beta(raw_beta, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m], omit_indices, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_beta_method\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/eaaw_graphlime_utils.py:250\u001b[0m, in \u001b[0;36mget_omit_indices\u001b[0;34m(x_sub, watermark, ignore_zeros_from_subgraphs)\u001b[0m\n\u001b[1;32m    248\u001b[0m zero_indices_within_watermark \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(watermark\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    249\u001b[0m omit_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(zero_features_within_subgraph[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m+\u001b[39m zero_indices_within_watermark[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())))\n\u001b[0;32m--> 250\u001b[0m not_omit_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_sub\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43momit_indices\u001b[49m])\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m omit_indices, not_omit_indices\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/_tensor.py:1116\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, element)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1113\u001b[0m     element, (torch\u001b[38;5;241m.\u001b[39mTensor, Number, torch\u001b[38;5;241m.\u001b[39mSymInt, torch\u001b[38;5;241m.\u001b[39mSymFloat, torch\u001b[38;5;241m.\u001b[39mSymBool)\n\u001b[1;32m   1114\u001b[0m ):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1120\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.optimize import minimize\n",
    "results = []\n",
    "for sig in subgraph_dict.keys():\n",
    "    watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "    regression_kwargs = config.regression_kwargs\n",
    "    this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "    x_sub = data_sub.x\n",
    "    edge_index = data_sub.edge_index\n",
    "    original_shape = x_sub.shape\n",
    "    x_sub_flat = x_sub.flatten()\n",
    "    # global i\n",
    "    # i = 0\n",
    "    def compute_watermark_loss(x_sub_flat):\n",
    "        # global i\n",
    "        # i+=1\n",
    "        # print(/i,end='\\r')\n",
    "        # print('hi')\n",
    "        x_sub = torch.tensor(x_sub_flat, dtype=torch.float32).reshape(original_shape)\n",
    "        balanced_beta_weights = torch.ones(x_sub.shape[1])\n",
    "\n",
    "        log_logits_sub = node_classifier(x_sub, edge_index)\n",
    "        probas_sub = log_logits_sub.clone().exp()\n",
    "\n",
    "\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        raw_beta            = solve_regression(x_sub, probas_sub, regression_kwargs['lambda'])\n",
    "        beta                = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta*this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        this_loss_watermark = torch.mean(torch.clamp(watermark_loss_kwargs['epsilon']-B_x_W, min=0)*balanced_beta_weights)\n",
    "        loss_watermark  = this_loss_watermark\n",
    "        # print('ok')\n",
    "        return loss_watermark.item()\n",
    "\n",
    "    # Ensure maxiter is enforced and debugging output\n",
    "    options = {'maxfun': 2, 'iprint':110, 'maxls':2, 'maxiter':2}\n",
    "\n",
    "    # Use the L-BFGS-B algorithm for optimization\n",
    "    result = minimize(compute_watermark_loss, x_sub_flat, \n",
    "                      method='L-BFGS-B', \n",
    "                      options=options)\n",
    "\n",
    "    # The optimal x found by the optimizer\n",
    "    results.append(result)\n",
    "    # print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scipy.optimize' from '/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/__init__.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(torch.wherae(torch.eq(data_copy.x, data.x)==False)[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[24,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.8543e-05,  0.0000e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.x[[24,25],[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([1,12])\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/20: 19.604000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m\n\u001b[1;32m     62\u001b[0m x_copy \u001b[38;5;241m=\u001b[39m x_copy\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#    edge_index_copy, x_copy, _ = augment_data(data, node_aug, edge_aug, train_nodes_to_consider, all_subgraph_indices)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#    x_copy = x_copy.requires_grad_(True)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m log_logits_copy          \u001b[38;5;241m=\u001b[39m \u001b[43mnode_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m probas_copy \u001b[38;5;241m=\u001b[39m log_logits_copy\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m     68\u001b[0m loss_watermark_scaled_copy \u001b[38;5;241m=\u001b[39m compute_watermark_loss(subgraph_dict, probas_copy, beta_weights)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/models.py:201\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    199\u001b[0m intermediate_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnLayers):\n\u001b[0;32m--> 201\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(x)\n\u001b[1;32m    203\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:263\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:547\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 547\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    549\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_beta_weights(subgraph_dict, num_features):\n",
    "    if config.watermark_loss_kwargs['balance_beta_weights'] == True:\n",
    "        beta_weights = get_balanced_beta_weights([subgraph_dict[sig]['subgraph'] for sig in subgraph_dict.keys()])\n",
    "    elif config.watermark_loss_kwargs['balance_beta_weights'] == False:\n",
    "        beta_weights = torch.ones(len(subgraph_dict),num_features)\n",
    "    return beta_weights\n",
    "\n",
    "def process_beta(beta, alpha, omit_indices, scale_beta_method='clip'):\n",
    "    if scale_beta_method=='tanh':\n",
    "        beta = torch.tanh(alpha*beta)\n",
    "    elif scale_beta_method=='tan':\n",
    "        beta = torch.tan(alpha*beta)\n",
    "    elif scale_beta_method=='clip':\n",
    "        beta = torch.clip(beta,min=-1,max=1)\n",
    "    elif scale_beta_method==None:\n",
    "        pass\n",
    "    beta = beta.clone()  # Avoid in-place operation\n",
    "    if omit_indices is not None and len(omit_indices)>0:\n",
    "        beta[omit_indices] = 0 # zero out non-contributing indices\n",
    "    return beta\n",
    "\n",
    "def compute_watermark_loss(subgraph_dict, probas, beta_weights):\n",
    "    watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "    regression_kwargs = config.regression_kwargs\n",
    "    optimization_kwargs = config.optimization_kwargs\n",
    "\n",
    "    loss_watermark = torch.tensor(0.0)\n",
    "    for s, sig in enumerate(subgraph_dict.keys()):\n",
    "        this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "        x_sub, y_sub = data_sub.x, probas[subgraph_node_indices]\n",
    "        ''' epoch condtion: epoch==epoch-1'''\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        raw_beta            = solve_regression(x_sub, y_sub, regression_kwargs['lambda'])\n",
    "        beta                = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta*this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "\n",
    "        balanced_beta_weights = beta_weights[s]\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        loss_watermark = loss_watermark+torch.mean(torch.clamp(watermark_loss_kwargs['epsilon']-B_x_W, min=0)*balanced_beta_weights)\n",
    "\n",
    "\n",
    "    loss_watermark = loss_watermark/len(subgraph_dict)\n",
    "    loss_watermark_scaled = loss_watermark*optimization_kwargs['coefWmk']\n",
    "    return loss_watermark_scaled\n",
    "\n",
    "\n",
    "beta_weights = get_beta_weights(subgraph_dict, data.x.shape[1])\n",
    "all_subgraph_indices = []\n",
    "for sig in subgraph_dict.keys():\n",
    "    nodeIndices = subgraph_dict[sig]['nodeIndices']\n",
    "    all_subgraph_indices+=nodeIndices.tolist()\n",
    "reg_copy=None\n",
    "\n",
    "\n",
    "lr = 0.002\n",
    "optimizer = optim.Adam(node_classifier.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for j in range(100):\n",
    "    x_copy,edge_index_copy = copy.deepcopy(data).x, copy.deepcopy(data).edge_index\n",
    "    x_copy = x_copy.requires_grad_(True)\n",
    "    #with torch.no_grad():\n",
    "    #    edge_index_copy, x_copy, _ = augment_data(data, node_aug, edge_aug, train_nodes_to_consider, all_subgraph_indices)\n",
    "    #    x_copy = x_copy.requires_grad_(True)\n",
    "    log_logits_copy          = node_classifier(x_copy, edge_index_copy)\n",
    "    probas_copy = log_logits_copy.clone().exp()\n",
    "    loss_watermark_scaled_copy = compute_watermark_loss(subgraph_dict, probas_copy, beta_weights)\n",
    "    wmk_loss = loss_watermark_scaled_copy+reg_copy if reg_copy is not None else loss_watermark_scaled_copy\n",
    "    optimizer.zero_grad()\n",
    "    wmk_loss.backward()\n",
    "    print(f'{j}/20: {wmk_loss:3f}',end='\\r')\n",
    "    this_grad = torch.zeros_like(data.x)\n",
    "    this_grad[all_subgraph_indices] = x_copy.grad.clone()[all_subgraph_indices]\n",
    "    this_grad[all_subgraph_indices] = (this_grad[all_subgraph_indices]-this_grad[all_subgraph_indices].mean())/(this_grad[all_subgraph_indices].max()-this_grad[all_subgraph_indices].min())\n",
    "    x_grad_wmk_loss = torch.zeros_like(data.x)\n",
    "    x_grad_wmk_loss[all_subgraph_indices] = this_grad.clone()[all_subgraph_indices]\n",
    "    # print(x_grad_wmk_loss[all_subgraph_indices])\n",
    "    data.x = data.x - perturb_lr*x_grad_wmk_loss\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  7,  11,  23,  34,  37,  41,  43,  69,  73,  86,  97, 105, 109, 110,\n",
      "        134, 144, 154, 159, 168, 177, 185, 186, 195, 212, 217, 219, 225, 227,\n",
      "        238, 250, 259, 277, 279, 284, 298, 328, 355, 358, 363, 365, 367, 376,\n",
      "        382, 386, 427, 430, 439, 442, 443, 457, 489, 503, 506, 507, 515, 520,\n",
      "        542, 544, 554, 559, 582, 597, 617, 649, 664, 687, 695, 698, 704, 708,\n",
      "        715, 729, 732, 747, 755, 758])\n"
     ]
    }
   ],
   "source": [
    "beta_weights = get_beta_weights(subgraph_dict, data.x.shape[1])\n",
    "for s, sig in enumerate(subgraph_dict.keys()):\n",
    "    this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "    x_sub, y_sub = data_sub.x, probas[subgraph_node_indices]\n",
    "    ''' epoch condtion: epoch==epoch-1'''\n",
    "    omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "    print(not_omit_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['11293_4896_5573_3951_10751_63'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_2_env",
   "language": "python",
   "name": "proj_2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
