{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarFunction\n"
     ]
    }
   ],
   "source": [
    "from eaaw_graphlime_utils import *\n",
    "import gc\n",
    "\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "\n",
    "\n",
    "from   pcgrad.pcgrad import PCGrad # from the following: https://github.com/WeiChengTseng/Pytorch-PCGrad. Renamed to 'pcgrad' and moved to site-packages folder.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transorms used when loading computers: ['CreateMaskTransform()']\n",
      "train_mask: 12376\n",
      "test_mask: 689\n",
      "val_mask: 687\n"
     ]
    }
   ],
   "source": [
    "dataset_name='computers'\n",
    "if dataset_attributes[dataset_name]['single_or_multi_graph']=='single':\n",
    "    dataset = prep_data(dataset_name=dataset_name, location='default', batch_size='default', transform_list='default',\n",
    "                        train_val_test_split=[0.9,0.05,0.05])\n",
    "    graph_to_watermark = data = dataset[0]\n",
    "elif dataset_attributes[dataset_name]['single_or_multi_graph']=='multi':\n",
    "    [train_dataset, val_dataset, test_dataset], [train_loader, val_loader, test_loader] = prep_data(dataset_name=dataset_name, location='default', \n",
    "                                                                                                    batch_size='default', transform_list='default',\n",
    "                                                                                                                            train_val_test_split=[0.9,0.05,0.05])\n",
    "    graph_to_watermark = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization_kwargs, node_classifier_kwargs, watermark_kwargs, subgraph_kwargs, augment_kwargs, watermark_loss_kwargs, regression_kwargs = get_presets(dataset, 'default')\n",
    "get_presets(dataset,dataset_name)\n",
    "\n",
    "compare_unimportant_against_random=False\n",
    "\n",
    "config.node_classifier_kwargs['dropout']=0.1\n",
    "config.node_classifier_kwargs['dropout_subgraphs']=0\n",
    "config.watermark_kwargs['fancy_selection_kwargs']['clf_only_epochs'] = 20\n",
    "config.optimization_kwargs['lr']=0.001\n",
    "config.optimization_kwargs['epochs']=50\n",
    "# config.optimization_kwargs['coefWmk']=300\n",
    "config.optimization_kwargs['perturb_x']=False\n",
    "config.optimization_kwargs['perturb_lr']=1e5\n",
    "\n",
    "config.optimization_kwargs['coefWmk_kwargs']= {\n",
    "                                                'coefWmk':50,\n",
    "                                                'schedule_coef_wmk': False,\n",
    "                                                'min_coefWmk_scheduled': 100,\n",
    "                                                'reach_max_coef_wmk_by_epoch':70,\n",
    "                                                }\n",
    "\n",
    "config.optimization_kwargs['use_pcgrad']=False\n",
    "config.optimization_kwargs['use_sam']=False\n",
    "config.optimization_kwargs['sam_momentum']=0.5\n",
    "config.optimization_kwargs['sam_rho']=1e-5\n",
    "\n",
    "\n",
    "config.optimization_kwargs['use_gradnorm']=False\n",
    "\n",
    "\n",
    "config.augment_kwargs['nodeDrop']['use']=False\n",
    "config.augment_kwargs['nodeMixUp']['use']=True\n",
    "config.augment_kwargs['nodeMixUp']['lambda']=10\n",
    "config.augment_kwargs['nodeFeatMask']['use']=False\n",
    "config.augment_kwargs['edgeDrop']['use']=False\n",
    "config.augment_kwargs['separate_trainset_from_subgraphs'] = True\n",
    "config.augment_kwargs['ignore_subgraphs'] = True\n",
    "config.watermark_kwargs['fancy_selection_kwargs']['percent_of_features_to_watermark']=3\n",
    "config.watermark_kwargs['watermark_type']='fancy'\n",
    "config.subgraph_kwargs['numSubgraphs']=7\n",
    "config.subgraph_kwargs['fraction']=0.03\n",
    "config.subgraph_kwargs['method']='random'\n",
    "config.optimization_kwargs['sacrifice_kwargs']['method']='subgraph_node_indices'\n",
    "config.optimization_kwargs['sacrifice_kwargs']['percentage']=1\n",
    "config.optimization_kwargs['separate_forward_passes_per_subgraph']=False\n",
    "config.optimization_kwargs['clf_only']=False\n",
    "config.watermark_loss_kwargs['epsilon']=1e-1\n",
    "\n",
    "config.regression_kwargs['lambda']=1e-3\n",
    "\n",
    "validate_kwargs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = merge_kwargs_dicts()#config.node_classifier_kwargs, config.optimization_kwargs, config.watermark_kwargs, config.subgraph_kwargs, config.regression_kwargs, config.watermark_loss_kwargs, config.augment_kwargs)\n",
    "merged_dict_keys = list(merged_dict.keys()) + ['Train Acc','Val Acc','Match Rates','Final Betas','watermark']\n",
    "all_dfs = pd.DataFrame(dict(zip(merged_dict_keys,[]*len(merged_dict_keys))))\n",
    "\n",
    "# all_dfs = pickle.load(open('/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/src/results_df.pkl','rb'))\n",
    "# all_results = {}\n",
    "\n",
    "\n",
    "\n",
    "scale_beta_method=None\n",
    "debug_multiple_subgraphs=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer_test = Trainer(data, dataset_name)\n",
    "# Trainer_test.x, Trainer_test.edge_index = data.x, data.edge_index\n",
    "# Trainer_test.test_perturb_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 10\n",
      "setup_subgraph_dict\n",
      "nodeIndices: [3941, 10762, 7971, 10573, 12294, 9009, 2051, 7184, 12412, 3009, 8366, 11870, 8078, 4310, 1436, 5810, 5176, 63, 5480, 3341, 3391, 3471, 10484, 13199, 271, 8673, 4218, 7932, 4076, 77, 3494, 12140, 12368, 8868, 7950, 828, 10143, 3512, 4907, 6076, 11597, 7539, 3852, 10297, 2495, 2228, 1059, 5460, 10440, 6053, 13425, 5291, 5392]\n",
      "nodeIndices: [10420, 6468, 7324, 12547, 2648, 2352, 6451, 8998, 12179, 4946, 10235, 9702, 7397, 7543, 13192, 6969, 13118, 2503, 7228, 4760, 4088, 11662, 2083, 4610, 9324, 10388, 6961, 11461, 2921, 7085, 4745, 12816, 8528, 13447, 9648, 7067, 8270, 10815, 12520, 9413, 4964, 2868, 2400, 11572, 204, 12844, 9258, 4512, 12080, 2056, 404, 10371, 10955]\n",
      "nodeIndices: [3846, 12387, 10313, 6170, 9120, 11052, 11328, 3261, 694, 2515, 8044, 9323, 4843, 3838, 5289, 8344, 7747, 6051, 10242, 197, 8988, 359, 13407, 11363, 7318, 9849, 638, 802, 3091, 9023, 11745, 5595, 9287, 1080, 9358, 2420, 11009, 1284, 3787, 12090, 2164, 3720, 6824, 1604, 6672, 13673, 2697, 2029, 6411, 1872, 8306, 2163, 6644]\n",
      "nodeIndices: [7329, 12775, 129, 11529, 10597, 9307, 11585, 5024, 3866, 10031, 8417, 9357, 5995, 6826, 5696, 6308, 6788, 10612, 1909, 3178, 5794, 13624, 1566, 10750, 363, 6719, 11499, 2188, 9263, 10456, 8321, 502, 2413, 2336, 408, 3221, 5283, 5075, 11442, 5764, 3346, 4061, 2657, 13327, 5033, 11177, 13707, 11600, 11919, 11379, 7858, 11824, 3633]\n",
      "nodeIndices: [1302, 8370, 2546, 3219, 8587, 1722, 1577, 9244, 12516, 6485, 513, 7700, 523, 3135, 4080, 4130, 751, 9969, 1670, 6310, 11549, 5793, 8678, 11151, 13476, 13296, 1464, 13532, 10684, 3635, 1188, 10302, 1401, 1600, 11316, 8039, 13256, 10378, 6994, 1549, 1773, 13477, 96, 4481, 10847, 9214, 2072, 5629, 703, 4178, 11992, 3856, 539]\n",
      "nodeIndices: [13409, 3491, 554, 2719, 7409, 4144, 3938, 7308, 10174, 7065, 7216, 13424, 7117, 4580, 12728, 8032, 6812, 4143, 11927, 1075, 3973, 6027, 7671, 516, 659, 11165, 4408, 56, 6100, 925, 10888, 12458, 6284, 12306, 9500, 6548, 4060, 6266, 11105, 2233, 10565, 1370, 3399, 8369, 10055, 9577, 6619, 12142, 11606, 2623, 11385, 1940, 6088]\n",
      "nodeIndices: [10861, 9478, 8240, 12350, 6959, 6518, 3493, 4806, 3310, 13454, 3611, 12525, 5774, 9963, 9160, 13536, 12609, 43, 12146, 6622, 5110, 4191, 6063, 3036, 1331, 7486, 4265, 7464, 4884, 10449, 11396, 10452, 13600, 2826, 279, 6246, 10179, 2470, 12289, 7056, 5859, 11567, 12846, 10187, 13321, 11331, 3760, 2841, 8074, 8312, 3615, 1434, 10863]\n",
      "Sacrificing 100% of subgraph nodes from node classification training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec9789378540a8962c7213a33494e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0, loss_primary = 2.674, loss_watermark = n/a, B*W = n/a, train acc = 0.066, val acc = 0.052\n",
      "Epoch:   1, loss_primary = 1.982, loss_watermark = n/a, B*W = n/a, train acc = 0.398, val acc = 0.419\n",
      "Epoch:   2, loss_primary = 1.581, loss_watermark = n/a, B*W = n/a, train acc = 0.600, val acc = 0.601\n",
      "Epoch:   3, loss_primary = 1.301, loss_watermark = n/a, B*W = n/a, train acc = 0.673, val acc = 0.677\n",
      "Epoch:   4, loss_primary = 1.117, loss_watermark = n/a, B*W = n/a, train acc = 0.700, val acc = 0.705\n",
      "Epoch:   5, loss_primary = 0.994, loss_watermark = n/a, B*W = n/a, train acc = 0.721, val acc = 0.750\n",
      "Epoch:   6, loss_primary = 0.868, loss_watermark = n/a, B*W = n/a, train acc = 0.749, val acc = 0.755\n",
      "Epoch:   7, loss_primary = 0.787, loss_watermark = n/a, B*W = n/a, train acc = 0.773, val acc = 0.779\n",
      "Epoch:   8, loss_primary = 0.741, loss_watermark = n/a, B*W = n/a, train acc = 0.788, val acc = 0.786\n",
      "Epoch:   9, loss_primary = 0.676, loss_watermark = n/a, B*W = n/a, train acc = 0.803, val acc = 0.799\n",
      "Epoch:  10, loss_primary = 0.648, loss_watermark = n/a, B*W = n/a, train acc = 0.816, val acc = 0.831\n",
      "Epoch:  11, loss_primary = 0.620, loss_watermark = n/a, B*W = n/a, train acc = 0.827, val acc = 0.854\n",
      "Epoch:  12, loss_primary = 0.586, loss_watermark = n/a, B*W = n/a, train acc = 0.836, val acc = 0.860\n",
      "Epoch:  13, loss_primary = 0.567, loss_watermark = n/a, B*W = n/a, train acc = 0.845, val acc = 0.847\n",
      "Epoch:  14, loss_primary = 0.553, loss_watermark = n/a, B*W = n/a, train acc = 0.846, val acc = 0.844\n",
      "Epoch:  15, loss_primary = 0.540, loss_watermark = n/a, B*W = n/a, train acc = 0.850, val acc = 0.859\n",
      "Epoch:  16, loss_primary = 0.528, loss_watermark = n/a, B*W = n/a, train acc = 0.851, val acc = 0.850\n",
      "Epoch:  17, loss_primary = 0.509, loss_watermark = n/a, B*W = n/a, train acc = 0.856, val acc = 0.850\n",
      "Epoch:  18, loss_primary = 0.492, loss_watermark = n/a, B*W = n/a, train acc = 0.864, val acc = 0.873\n",
      "Epoch:  19, loss_primary = 0.499, loss_watermark = n/a, B*W = n/a, train acc = 0.865, val acc = 0.869\n",
      "apply_fancy_watermark\n",
      "importance: tensor(2.6309e-05)\n",
      "importance: tensor(3.7357e-05)\n",
      "importance: tensor(4.8833e-05)\n",
      "importance: tensor(5.0956e-05)\n",
      "importance: tensor(0.0001)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0003)\n",
      "importance: tensor(0.0004)\n",
      "importance: tensor(0.0004)\n",
      "importance: tensor(0.0004)\n",
      "importance: tensor(0.0004)\n",
      "importance: tensor(0.0004)\n",
      "importance: tensor(0.0004)\n",
      "importance: tensor(0.0006)\n",
      "importance: tensor(0.0007)\n",
      "importance: tensor(0.0007)\n",
      "importance: tensor(0.0008)\n",
      "importance: tensor(0.0008)\n",
      "importance: tensor(0.0009)\n",
      "importance: tensor(0.0009)\n",
      "Using averaged betas from subgraphs to identify bottom 3% of important feature indices for watermarking, uniformly across subgraphs\n",
      "Epoch:  20, loss_primary = 0.487, loss_watermark = 5.056, B*W = 0.00022, train acc = 0.865, val acc = 0.866\n",
      "Epoch:  21, loss_primary = 0.498, loss_watermark = 4.457, B*W = 0.01255, train acc = 0.863, val acc = 0.865\n",
      "Epoch:  22, loss_primary = 0.532, loss_watermark = 3.858, B*W = 0.02518, train acc = 0.851, val acc = 0.846\n",
      "Epoch:  23, loss_primary = 0.521, loss_watermark = 3.315, B*W = 0.03833, train acc = 0.851, val acc = 0.844\n",
      "Epoch:  24, loss_primary = 0.477, loss_watermark = 3.030, B*W = 0.04550, train acc = 0.867, val acc = 0.882\n",
      "Epoch:  25, loss_primary = 0.483, loss_watermark = 2.618, B*W = 0.05385, train acc = 0.869, val acc = 0.873\n",
      "Epoch:  26, loss_primary = 0.534, loss_watermark = 2.566, B*W = 0.05414, train acc = 0.852, val acc = 0.844\n",
      "Epoch:  27, loss_primary = 0.490, loss_watermark = 2.328, B*W = 0.05984, train acc = 0.868, val acc = 0.856\n",
      "Epoch:  28, loss_primary = 0.492, loss_watermark = 2.223, B*W = 0.06148, train acc = 0.867, val acc = 0.875\n",
      "Epoch:  29, loss_primary = 0.480, loss_watermark = 1.874, B*W = 0.07037, train acc = 0.871, val acc = 0.876\n",
      "Epoch:  30, loss_primary = 0.503, loss_watermark = 1.873, B*W = 0.07103, train acc = 0.862, val acc = 0.865\n",
      "Epoch:  31, loss_primary = 0.478, loss_watermark = 1.623, B*W = 0.07647, train acc = 0.870, val acc = 0.868\n",
      "Epoch:  32, loss_primary = 0.498, loss_watermark = 1.532, B*W = 0.07789, train acc = 0.865, val acc = 0.868\n",
      "Epoch:  33, loss_primary = 0.498, loss_watermark = 1.413, B*W = 0.08010, train acc = 0.866, val acc = 0.870\n",
      "Epoch:  34, loss_primary = 0.484, loss_watermark = 1.284, B*W = 0.08466, train acc = 0.872, val acc = 0.875\n",
      "Epoch:  35, loss_primary = 0.480, loss_watermark = 1.210, B*W = 0.08544, train acc = 0.869, val acc = 0.865\n",
      "Epoch:  36, loss_primary = 0.470, loss_watermark = 1.066, B*W = 0.08809, train acc = 0.870, val acc = 0.881\n",
      "Epoch:  37, loss_primary = 0.479, loss_watermark = 1.043, B*W = 0.08903, train acc = 0.870, val acc = 0.882\n",
      "Epoch:  38, loss_primary = 0.471, loss_watermark = 0.970, B*W = 0.08995, train acc = 0.870, val acc = 0.870\n",
      "Epoch:  39, loss_primary = 0.481, loss_watermark = 0.894, B*W = 0.09151, train acc = 0.871, val acc = 0.868\n",
      "Epoch:  40, loss_primary = 0.488, loss_watermark = 1.015, B*W = 0.08970, train acc = 0.868, val acc = 0.879\n",
      "Epoch:  41, loss_primary = 0.487, loss_watermark = 0.851, B*W = 0.09223, train acc = 0.870, val acc = 0.872\n",
      "Epoch:  42, loss_primary = 0.472, loss_watermark = 0.816, B*W = 0.09342, train acc = 0.872, val acc = 0.869\n",
      "Epoch:  43, loss_primary = 0.470, loss_watermark = 0.708, B*W = 0.09680, train acc = 0.873, val acc = 0.857\n",
      "Epoch:  44, loss_primary = 0.468, loss_watermark = 0.755, B*W = 0.09574, train acc = 0.870, val acc = 0.870\n",
      "Epoch:  45, loss_primary = 0.463, loss_watermark = 0.632, B*W = 0.09844, train acc = 0.871, val acc = 0.881\n",
      "Epoch:  46, loss_primary = 0.483, loss_watermark = 0.659, B*W = 0.09797, train acc = 0.869, val acc = 0.862\n",
      "Epoch:  47, loss_primary = 0.455, loss_watermark = 0.570, B*W = 0.09966, train acc = 0.873, val acc = 0.882\n",
      "Epoch:  48, loss_primary = 0.465, loss_watermark = 0.567, B*W = 0.09962, train acc = 0.875, val acc = 0.872\n",
      "Epoch:  49, loss_primary = 0.459, loss_watermark = 0.508, B*W = 0.10113, train acc = 0.873, val acc = 0.872\n",
      "Node classifier, history, subgraph dict, feature importances, watermark indices, and probas saved in:\n",
      "/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/training_results/computers/archGCN_elu_nLayers3_hDim256_drop0.1_skipTrue/_3%UnimportantIndices_average_20ClfEpochs_random_fraction0.03_numSubgraphs7_eps0.1_raw_beta_nodeMixUp10_lr0.001_epochs50_coefWmk50_sacrifice1subNodes_regressionLambda0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for _ in range(3):\n",
    "# for mom in [1e-3, 1e-2, 1e-1, 0.5]:\n",
    "#     for rho in [1e-7,1e-5,1e-3]:\n",
    "# config.optimization_kwargs['sam_momentum'] = mom\n",
    "# config.optimization_kwargs['sam_rho'] = rho\n",
    "# for i in range(10):\n",
    "#     if i<5:\n",
    "#         config.optimization_kwargs['use_sam']=True\n",
    "#         title = f'With SAM - momentum={np.round(config.optimization_kwargs['sam_momentum'],3)}, rho={config.optimization_kwargs['sam_rho']}'\n",
    "#     else:\n",
    "# config.optimization_kwargs['use_sam']=False\n",
    "for _ in range(1):\n",
    "    config.optimization_kwargs['perturb_x']=False\n",
    "    config.optimization_kwargs['separate_forward_passes_per_subgraph']=True\n",
    "    title = f'Separate'\n",
    "    Trainer_ = Trainer(data, dataset_name)\n",
    "    node_classifier, history, subgraph_dict, \\\n",
    "    all_feature_importances, all_watermark_indices, probas = Trainer_.train(debug_multiple_subgraphs=False, save=True, print_every=1)\n",
    "\n",
    "    primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "    percent_matches, percent_match_mean, percent_match_std, \\\n",
    "        primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "    final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "# for _ in range(1):\n",
    "#     config.optimization_kwargs['perturb_x']=False\n",
    "#     config.optimization_kwargs['separate_forward_passes_per_subgraph']=False\n",
    "#     title = f'Not Separate'\n",
    "#     Trainer_ = Trainer(data, dataset_name)\n",
    "#     node_classifier, history, subgraph_dict, \\\n",
    "#     all_feature_importances, all_watermark_indices, probas = Trainer_.train(debug_multiple_subgraphs=False, save=True, print_every=1)\n",
    "\n",
    "#     primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "#     percent_matches, percent_match_mean, percent_match_std, \\\n",
    "#         primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "#     final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "# for _ in range(1):\n",
    "#     config.optimization_kwargs['perturb_x']=False\n",
    "#     title = f'Vanilla'\n",
    "#     Trainer_ = Trainer(data, dataset_name)\n",
    "#     node_classifier, history, subgraph_dict, \\\n",
    "#     all_feature_importances, all_watermark_indices, probas = Trainer_.train(debug_multiple_subgraphs=False, save=True, print_every=1)\n",
    "\n",
    "#     primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "#     percent_matches, percent_match_mean, percent_match_std, \\\n",
    "#         primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "#     final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "# for _ in range(1):\n",
    "#     config.optimization_kwargs['separate_forward_passes_per_subgraph']=True\n",
    "#     title = f'Separate Forward Passes for D_trn and D_wmk'\n",
    "#     Trainer_ = Trainer(data, dataset_name)\n",
    "#     node_classifier, history, subgraph_dict, \\\n",
    "#     all_feature_importances, all_watermark_indices, probas = Trainer_.train(debug_multiple_subgraphs=False, save=True, print_every=1)\n",
    "\n",
    "#     primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "#     percent_matches, percent_match_mean, percent_match_std, \\\n",
    "#         primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "#     final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer_.optimizer.zero_grad()\n",
    "log_logits          = Trainer_.forward(data.x, data.edge_index, dropout=config.node_classifier_kwargs['dropout'])\n",
    "\n",
    "\n",
    "acc_trn = accuracy(log_logits[Trainer_.train_mask], data.y[Trainer_.train_mask],verbose=False)\n",
    "acc_val = accuracy(log_logits[Trainer_.val_mask],   data.y[Trainer_.val_mask],verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [10., 10.,  0.,  ...,  0., 10.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [10., -9.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 1., -9., -9.,  ..., -9.,  0., -9.],\n",
       "        [10., 10.,  0.,  ..., 10., 10.,  0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = list(Trainer_.subgraph_dict.keys())[0]\n",
    "node_indices = Trainer_.subgraph_dict[sig]['nodeIndices']\n",
    "Trainer_.x#[node_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5135, -2.0671, -2.5162,  ..., -2.5989, -2.0855, -2.4737],\n",
       "        [-2.9204, -2.0398, -3.1309,  ..., -3.5097, -1.9472, -3.0037],\n",
       "        [-2.6028, -2.2278, -2.8592,  ..., -2.9719, -1.3860, -2.3866],\n",
       "        ...,\n",
       "        [-2.9182, -2.6029, -2.9013,  ..., -3.2258, -1.1315, -2.7037],\n",
       "        [-3.1914, -1.7261, -3.4235,  ..., -3.5965, -1.5664, -3.1513],\n",
       "        [-1.9254, -2.5194, -2.0963,  ..., -2.5219, -1.8731, -2.4154]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_.forward(data.x, data.edge_index, dropout=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_subgraph(self, p_to_swap, subgraph_node_indices):\n",
    "    num_to_swap = int(p_to_swap*len(subgraph_node_indices))\n",
    "    random_indices = torch.randperm(len(subgraph_node_indices))\n",
    "    subgraph_node_indices = subgraph_node_indices[random_indices[:len(subgraph_node_indices)-num_to_swap]]\n",
    "    filtered_tensor = Trainer_.train_nodes_to_consider[~Trainer_.train_nodes_to_consider.unsqueeze(1).eq(subgraph_node_indices).any(dim=1)]\n",
    "    random_index = torch.randint(0, filtered_tensor.size(0), (num_to_swap,))\n",
    "    random_element = filtered_tensor[random_index]\n",
    "    subgraph_node_indices = torch.concatenate([subgraph_node_indices, random_element])\n",
    "\n",
    "    sub_edge_index, _ = subgraph(subgraph_node_indices, self.data.edge_index, relabel_nodes=True, num_nodes=self.data.num_nodes)\n",
    "    shifted_subgraph = Data(\n",
    "        x          = self.data.x[subgraph_node_indices]          if self.data.x is not None else None,\n",
    "        edge_index = sub_edge_index,\n",
    "        y          = self.data.y[subgraph_node_indices]          if self.data.y is not None else None,\n",
    "        train_mask = self.data.train_mask[subgraph_node_indices] if self.data.train_mask is not None else None,\n",
    "        test_mask  = self.data.test_mask[subgraph_node_indices]  if self.data.test_mask is not None else None,\n",
    "        val_mask   = self.data.val_mask[subgraph_node_indices]   if self.data.val_mask is not None else None)\n",
    "    return shifted_subgraph\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100.0, 100.0, 100.0, 95.65217391304348, 100.0, 100.0, 100.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_.percent_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full prediction accuracies:\n",
      "tensor(0.8679, dtype=torch.float64)\n",
      "tensor(0.8679, dtype=torch.float64)\n",
      "tensor(0.8113, dtype=torch.float64)\n",
      "tensor(0.8113, dtype=torch.float64)\n",
      "tensor(0.8113, dtype=torch.float64)\n",
      "tensor(0.9434, dtype=torch.float64)\n",
      "tensor(0.8679, dtype=torch.float64)\n",
      "Match rates when replacing 0% watermark subgraph indices:\n",
      "['100.0%', '100.0%', '100.0%', '95.652%', '100.0%', '100.0%', '100.0%']\n",
      "match counts across subgraphs: 22\n",
      "Match rates when replacing 5% watermark subgraph indices:\n",
      "['100.0%', '95.652%', '95.652%', '91.304%', '95.652%', '86.957%', '100.0%']\n",
      "match counts across subgraphs: 16\n",
      "Match rates when replacing 10% watermark subgraph indices:\n",
      "['91.304%', '78.261%', '82.609%', '86.957%', '100.0%', '86.957%', '86.957%']\n",
      "match counts across subgraphs: 8\n",
      "Match rates when replacing 15% watermark subgraph indices:\n",
      "['82.609%', '82.609%', '91.304%', '86.957%', '95.652%', '86.957%', '78.261%']\n",
      "match counts across subgraphs: 8\n",
      "Match rates when replacing 20% watermark subgraph indices:\n",
      "['82.609%', '73.913%', '95.652%', '69.565%', '82.609%', '86.957%', '82.609%']\n",
      "match counts across subgraphs: 5\n",
      "Match rates when replacing 25% watermark subgraph indices:\n",
      "['82.609%', '73.913%', '78.261%', '91.304%', '86.957%', '86.957%', '69.565%']\n",
      "match counts across subgraphs: 5\n",
      "Match rates when replacing 30% watermark subgraph indices:\n",
      "['82.609%', '60.87%', '65.217%', '56.522%', '91.304%', '69.565%', '78.261%']\n",
      "match counts across subgraphs: 2\n",
      "Match rates when replacing 35% watermark subgraph indices:\n",
      "['65.217%', '56.522%', '60.87%', '69.565%', '65.217%', '65.217%', '60.87%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 40% watermark subgraph indices:\n",
      "['69.565%', '56.522%', '78.261%', '69.565%', '60.87%', '69.565%', '69.565%']\n",
      "match counts across subgraphs: 1\n",
      "Match rates when replacing 45% watermark subgraph indices:\n",
      "['60.87%', '43.478%', '65.217%', '60.87%', '69.565%', '73.913%', '60.87%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 50% watermark subgraph indices:\n",
      "['56.522%', '65.217%', '43.478%', '52.174%', '56.522%', '69.565%', '39.13%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 55% watermark subgraph indices:\n",
      "['47.826%', '56.522%', '39.13%', '47.826%', '52.174%', '34.783%', '52.174%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 60% watermark subgraph indices:\n",
      "['43.478%', '60.87%', '43.478%', '69.565%', '60.87%', '39.13%', '60.87%']\n",
      "match counts across subgraphs: 1\n",
      "Match rates when replacing 65% watermark subgraph indices:\n",
      "['56.522%', '56.522%', '56.522%', '34.783%', '65.217%', '52.174%', '47.826%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 70% watermark subgraph indices:\n",
      "['43.478%', '39.13%', '65.217%', '39.13%', '60.87%', '56.522%', '52.174%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 75% watermark subgraph indices:\n",
      "['56.522%', '60.87%', '47.826%', '39.13%', '56.522%', '60.87%', '43.478%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 80% watermark subgraph indices:\n",
      "['56.522%', '30.435%', '56.522%', '34.783%', '39.13%', '52.174%', '39.13%']\n",
      "match counts across subgraphs: 1\n",
      "Match rates when replacing 85% watermark subgraph indices:\n",
      "['65.217%', '39.13%', '39.13%', '43.478%', '60.87%', '78.261%', '52.174%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 90% watermark subgraph indices:\n",
      "['65.217%', '56.522%', '60.87%', '34.783%', '65.217%', '52.174%', '47.826%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 95% watermark subgraph indices:\n",
      "['60.87%', '34.783%', '60.87%', '56.522%', '65.217%', '43.478%', '69.565%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 100% watermark subgraph indices:\n",
      "['52.174%', '60.87%', '56.522%', '39.13%', '52.174%', '56.522%', '60.87%']\n",
      "match counts across subgraphs: 1\n"
     ]
    }
   ],
   "source": [
    "Trainer_.node_classifier.eval()\n",
    "Trainer_.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "log_logits   = Trainer_.forward(data.x, data.edge_index, dropout=0)\n",
    "print('Full prediction accuracies:')\n",
    "for i, sig in enumerate(Trainer_.subgraph_dict.keys()):\n",
    "    node_indices = Trainer_.subgraph_dict[sig]['nodeIndices']\n",
    "    these_out = log_logits[node_indices]\n",
    "    these_y = data.y[node_indices]\n",
    "    acc = accuracy(these_out, these_y, verbose=False)\n",
    "    print(acc)\n",
    "for p_to_swap in [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]:\n",
    "    matches = []\n",
    "    sign_betas = []\n",
    "    for i, sig in enumerate(Trainer_.subgraph_dict.keys()):\n",
    "        node_indices = Trainer_.subgraph_dict[sig]['nodeIndices']\n",
    "        num_to_swap = int(p_to_swap*len(node_indices))\n",
    "\n",
    "        # print('node indices start:', node_indices)\n",
    "        random_indices = torch.randperm(len(node_indices))\n",
    "        node_indices = node_indices[random_indices[:len(node_indices)-num_to_swap]]\n",
    "        filtered_tensor = Trainer_.train_nodes_to_consider[~Trainer_.train_nodes_to_consider.unsqueeze(1).eq(node_indices).any(dim=1)]\n",
    "        random_index = torch.randint(0, filtered_tensor.size(0), (num_to_swap,))\n",
    "        random_element = filtered_tensor[random_index]\n",
    "        node_indices = torch.concatenate([node_indices, random_element])\n",
    "        # print('node indices end  :', node_indices)\n",
    "\n",
    "        sub_edge_index, _ = subgraph(node_indices, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "        subgraph_ = Data(\n",
    "            x=data.x[node_indices] if data.x is not None else None,\n",
    "            edge_index=sub_edge_index,\n",
    "            y=data.y[node_indices] if data.y is not None else None,\n",
    "            train_mask=data.train_mask[node_indices] if data.train_mask is not None else None,\n",
    "            test_mask=data.test_mask[node_indices] if data.test_mask is not None else None,\n",
    "            val_mask=data.val_mask[node_indices] if data.val_mask is not None else None)\n",
    "    \n",
    "\n",
    "        x_sub, edge_index_sub = subgraph_.x, subgraph_.edge_index\n",
    "        these_log_logits   = Trainer_.forward(x_sub, edge_index_sub, dropout=config.node_classifier_kwargs['dropout'])\n",
    "        these_probas = these_log_logits.clone().exp()\n",
    "        y_sub = these_probas\n",
    "\n",
    "        watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "        regression_kwargs = config.regression_kwargs\n",
    "        this_watermark = subgraph_dict[sig]['watermark']#[subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "\n",
    "\n",
    "        ''' epoch condtion: epoch==epoch-1'''\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        this_raw_beta = solve_regression(x_sub, y_sub, regression_kwargs['lambda'])\n",
    "        beta  = process_beta(this_raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        these_watermark_indices = Trainer_.all_watermark_indices[i]\n",
    "        #print('watermark:',this_watermark[these_watermark_indices])\n",
    "        #print('beta:     ',torch.sign(beta[these_watermark_indices]))\n",
    "        match = torch.sum(this_watermark[these_watermark_indices]==torch.sign(beta[these_watermark_indices]))/len(these_watermark_indices)\n",
    "        sign_betas.append(torch.sign(beta[these_watermark_indices]))\n",
    "        match_str = str(np.round(100*match.item(),3)) + '%'\n",
    "        matches.append(match_str)\n",
    "    print(f'Match rates when replacing {int(100*p_to_swap)}% watermark subgraph indices:')\n",
    "    print(matches)\n",
    "    bs = torch.vstack(sign_betas)\n",
    "    match_counts= count_matches(bs)\n",
    "    print('match counts across subgraphs:',match_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(these_watermark_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = Trainer_.train_nodes_to_consider\n",
    "\n",
    "# Separate list of elements to exclude\n",
    "exclude_tensor = node_indices\n",
    "\n",
    "# Convert exclude_list to a tensor for comparison\n",
    "# exclude_tensor = torch.tensor(exclude_list)\n",
    "\n",
    "# Filter the original tensor to exclude elements in the exclude list\n",
    "filtered_tensor = tensor[~tensor.unsqueeze(1).eq(exclude_tensor).any(dim=1)]\n",
    "random_index = torch.randint(0, filtered_tensor.size(0), (5,))\n",
    "random_element = filtered_tensor[random_index]\n",
    "node_indices = torch.concatenate([node_indices, random_element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
       "        [ 0., 10.,  0.,  ..., 10.,  0., 10.],\n",
       "        ...,\n",
       "        [10.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0., -9.,  0.,  ...,  0., -9.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_.x[torch.tensor(list(set(np.concatenate(Trainer_.all_watermark_indices))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['arch', 'activation', 'nLayers', 'hDim', 'dropout', 'dropout_subgraphs', 'skip_connections', 'heads_1', 'heads_2', 'inDim', 'outDim'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.node_classifier_kwargs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m perturb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     59\u001b[0m perturb_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m     60\u001b[0m node_classifier, history, subgraph_dict, \\\n\u001b[0;32m---> 61\u001b[0m     all_feature_importances, all_watermark_indices, probas \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m(data, dataset_name, debug_multiple_subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,perturb\u001b[38;5;241m=\u001b[39mperturb,perturb_lr\u001b[38;5;241m=\u001b[39mperturb_lr)\n\u001b[1;32m     62\u001b[0m primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n\u001b[1;32m     63\u001b[0m     percent_matches, percent_match_mean, percent_match_std, \\\n\u001b[1;32m     64\u001b[0m         primary_acc_curve, watermark_acc_curve, train_acc, val_acc \u001b[38;5;241m=\u001b[39m get_performance_trends(history, subgraph_dict)\n\u001b[1;32m     65\u001b[0m final_plot(history, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "''' (individualize boolean, multisubgraph method, index selection method) '''\n",
    "selection_kwargss = [#(False, None, 'random'), # random, not individualized\n",
    "                    (False,'average','unimportant'), # average, not individualized\n",
    "                    #(False,'concat','unimportant'), # concat, not inividualized\n",
    "                    # (True,None,'unimportant') # unimportant, individualized\n",
    "                    ]\n",
    "\n",
    "''' (method, regenerate_boolean) '''\n",
    "subgraph_methods = [('random',False)]\n",
    "\n",
    "''' [subset, percentage] '''\n",
    "sacrifice_kwargss = [['train_node_indices',1],['train_node_indices',0.75],['subgraph_node_indices',1],[None,None]]\n",
    "\n",
    "''' (method, L2_lambda (if applicable, else None) '''\n",
    "regularization_kwargs = [(None,None),('L2',0.01),('beta_var',None)]\n",
    "\n",
    "\n",
    "variables = {'augment': [{'separate_trainset_from_subgraphs': True,\n",
    "                          'ignore_subgraphs': True,\n",
    "                          'nodeDrop': False,\n",
    "                          'nodeMixUp': False,\n",
    "                          'nodeFeatMask': False,\n",
    "                          'edgeDrop': False},\n",
    "                          {'separate_trainset_from_subgraphs': True,\n",
    "                          'ignore_subgraphs': True,\n",
    "                          'nodeDrop': True,\n",
    "                          'nodeMixUp': True,\n",
    "                          'nodeFeatMask': True,\n",
    "                          'edgeDrop': True},],\n",
    "             'sacrifice_kwargs':  sacrifice_kwargss,\n",
    "             'beta_selection': selection_kwargss,\n",
    "             'use_PCgrad': [True,False],\n",
    "             'subgraph_method': subgraph_methods,\n",
    "             'reg': regularization_kwargs,\n",
    "             'perc': [3],\n",
    "             'clf_epochs': [10,20],\n",
    "             'coef_wmk': [200,600],\n",
    "             'frac': [0.02],\n",
    "             'num_subgraphs': [2,7],\n",
    "             'balance_beta_weights': [True,False],\n",
    "             'ignore_subgraph_neighbors': [True, False]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# config.optimization_kwargs['clf_only']=False       \n",
    "# watermarking_order = ['augment','use_PCgrad','sacrifice_kwargs','beta_selection','subgraph_method','reg','perc','clf_epochs','frac','coef_wmk','num_subgraphs','balance_beta_weights','ignore_subgraph_neighbors']\n",
    "# print(\"watermarking:\", watermarking_order)\n",
    "# count, [node_classifier, history, subgraph_dict,\\\n",
    "#          all_feature_importances, all_watermark_indices, probas] = dynamic_grid_search(data, dataset_name, debug_multiple_subgraphs, \n",
    "#                                                                                        all_dfs, False, variables, watermarking_order,\n",
    "#                                                                                        count_only=True)\n",
    "# print(count)\n",
    "\n",
    "\n",
    "config.augment_kwargs['separate_trainset_from_subgraphs'] = True\n",
    "config.augment_kwargs['ignore_subgraphs'] = True\n",
    "perturb=False\n",
    "perturb_lr = 1e-3\n",
    "node_classifier, history, subgraph_dict, \\\n",
    "    all_feature_importances, all_watermark_indices, probas = train(data, dataset_name, debug_multiple_subgraphs=False, save=True, print_every=1,perturb=perturb,perturb_lr=perturb_lr)\n",
    "primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "    percent_matches, percent_match_mean, percent_match_std, \\\n",
    "        primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "final_plot(history, '', percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got builtin_function_or_method)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got builtin_function_or_method)"
     ]
    }
   ],
   "source": [
    "data.x[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/training_results/computers/archGCN_elu_nLayers3_hDim256_drop0_skipTrue/_3%UnimportantIndices_average_10ClfEpochs_random_fraction0.01_numSubgraphs10_eps0.1_raw_beta_nodeMixUp40_lr0.002_epochs80_coefWmk200_regressionLambda0.1'\n",
    "probas = pickle.load(open(f'{folder}/probas','rb'))\n",
    "subgraph_dict = pickle.load(open(f'{folder}/subgraph_dict','rb'))\n",
    "all_watermark_indices = pickle.load(open(f'{folder}/all_watermark_indices','rb'))\n",
    "node_classifier = pickle.load(open(f'{folder}/node_classifier','rb'))\n",
    "history = pickle.load(open(f'{folder}/history','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlopt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define your objective function\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nlopt'"
     ]
    }
   ],
   "source": [
    "import nlopt\n",
    "import numpy as np\n",
    "\n",
    "# Define your objective function\n",
    "def objective(x, grad):\n",
    "    if grad.size > 0:\n",
    "        # Compute gradient if needed\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).reshape(original_shape)\n",
    "        x_tensor.requires_grad = True\n",
    "        log_logits_sub = node_classifier(x_tensor, edge_index)\n",
    "        probas_sub = log_logits_sub.clone().exp()\n",
    "\n",
    "        omit_indices, not_omit_indices = get_omit_indices(x_tensor, this_watermark, ignore_zeros_from_subgraphs=False)\n",
    "        raw_beta = solve_regression(x_tensor, probas_sub, regression_kwargs['lambda'])\n",
    "        beta = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta * this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "        balanced_beta_weights = torch.ones_like(B_x_W)\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        loss = torch.mean(torch.clamp(watermark_loss_kwargs['epsilon'] - B_x_W, min=0) * balanced_beta_weights)\n",
    "        loss.backward()\n",
    "\n",
    "        grad[:] = x_tensor.grad.numpy().flatten()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_minimize_lbfgsb options: 2 2\n",
      "_prepare_scalar_function\n",
      "C\n",
      "D\n",
      "F\n",
      "init\n",
      "update_fun: <function ScalarFunction.__init__.<locals>.update_fun at 0x3170d3880>\n",
      "fun_wrapped\n",
      "args: ()\n",
      "FD_METHODS\n",
      "finite_diff_options: {'method': '2-point', 'rel_step': None, 'abs_step': 1e-08, 'bounds': (array([-inf, -inf, -inf, ..., -inf, -inf, -inf]), array([inf, inf, inf, ..., inf, inf, inf]))}\n",
      "Help on function update_grad in module scipy.optimize._differentiable_functions:\n",
      "\n",
      "update_grad()\n",
      "\n",
      "update_grad: None\n",
      "not self.g_updated\n",
      "approx deriv\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m110\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m}\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Use the L-BFGS-B algorithm for optimization\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_watermark_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_sub_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# The optimal x found by the optimizer\u001b[39;00m\n\u001b[1;32m     45\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:308\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 308\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    314\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_optimize.py:390\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 390\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:188\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate_grad:\u001b[39m\u001b[38;5;124m'\u001b[39m,help(update_grad))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad_impl \u001b[38;5;241m=\u001b[39m update_grad\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Hessian Evaluation\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(hess):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:271\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot self.g_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:183\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapprox deriv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    574\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[1;32m    575\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    578\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 456\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:141\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs:\u001b[39m\u001b[38;5;124m'\u001b[39m,args)\n\u001b[0;32m--> 141\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mcompute_watermark_loss\u001b[0;34m(x_sub_flat)\u001b[0m\n\u001b[1;32m     21\u001b[0m log_logits_sub \u001b[38;5;241m=\u001b[39m node_classifier(x_sub, edge_index)\n\u001b[1;32m     22\u001b[0m probas_sub \u001b[38;5;241m=\u001b[39m log_logits_sub\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m---> 25\u001b[0m omit_indices,not_omit_indices \u001b[38;5;241m=\u001b[39m \u001b[43mget_omit_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_watermark\u001b[49m\u001b[43m,\u001b[49m\u001b[43mignore_zeros_from_subgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#indices where watermark is 0\u001b[39;00m\n\u001b[1;32m     26\u001b[0m raw_beta            \u001b[38;5;241m=\u001b[39m solve_regression(x_sub, probas_sub, regression_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m beta                \u001b[38;5;241m=\u001b[39m process_beta(raw_beta, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m], omit_indices, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_beta_method\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/eaaw_graphlime_utils.py:250\u001b[0m, in \u001b[0;36mget_omit_indices\u001b[0;34m(x_sub, watermark, ignore_zeros_from_subgraphs)\u001b[0m\n\u001b[1;32m    248\u001b[0m zero_indices_within_watermark \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(watermark\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    249\u001b[0m omit_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(zero_features_within_subgraph[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m+\u001b[39m zero_indices_within_watermark[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())))\n\u001b[0;32m--> 250\u001b[0m not_omit_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_sub\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43momit_indices\u001b[49m])\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m omit_indices, not_omit_indices\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/_tensor.py:1116\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, element)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1113\u001b[0m     element, (torch\u001b[38;5;241m.\u001b[39mTensor, Number, torch\u001b[38;5;241m.\u001b[39mSymInt, torch\u001b[38;5;241m.\u001b[39mSymFloat, torch\u001b[38;5;241m.\u001b[39mSymBool)\n\u001b[1;32m   1114\u001b[0m ):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1120\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.optimize import minimize\n",
    "results = []\n",
    "for sig in subgraph_dict.keys():\n",
    "    watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "    regression_kwargs = config.regression_kwargs\n",
    "    this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "    x_sub = data_sub.x\n",
    "    edge_index = data_sub.edge_index\n",
    "    original_shape = x_sub.shape\n",
    "    x_sub_flat = x_sub.flatten()\n",
    "    # global i\n",
    "    # i = 0\n",
    "    def compute_watermark_loss(x_sub_flat):\n",
    "        # global i\n",
    "        # i+=1\n",
    "        # print(/i,end='\\r')\n",
    "        # print('hi')\n",
    "        x_sub = torch.tensor(x_sub_flat, dtype=torch.float32).reshape(original_shape)\n",
    "        balanced_beta_weights = torch.ones(x_sub.shape[1])\n",
    "\n",
    "        log_logits_sub = node_classifier(x_sub, edge_index)\n",
    "        probas_sub = log_logits_sub.clone().exp()\n",
    "\n",
    "\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        raw_beta            = solve_regression(x_sub, probas_sub, regression_kwargs['lambda'])\n",
    "        beta                = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta*this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        this_loss_watermark = torch.mean(torch.clamp(watermark_loss_kwargs['epsilon']-B_x_W, min=0)*balanced_beta_weights)\n",
    "        loss_watermark  = this_loss_watermark\n",
    "        # print('ok')\n",
    "        return loss_watermark.item()\n",
    "\n",
    "    # Ensure maxiter is enforced and debugging output\n",
    "    options = {'maxfun': 2, 'iprint':110, 'maxls':2, 'maxiter':2}\n",
    "\n",
    "    # Use the L-BFGS-B algorithm for optimization\n",
    "    result = minimize(compute_watermark_loss, x_sub_flat, \n",
    "                      method='L-BFGS-B', \n",
    "                      options=options)\n",
    "\n",
    "    # The optimal x found by the optimizer\n",
    "    results.append(result)\n",
    "    # print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scipy.optimize' from '/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/__init__.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(torch.wherae(torch.eq(data_copy.x, data.x)==False)[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[24,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.8543e-05,  0.0000e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.x[[24,25],[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([1,12])\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/20: 19.604000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m\n\u001b[1;32m     62\u001b[0m x_copy \u001b[38;5;241m=\u001b[39m x_copy\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#    edge_index_copy, x_copy, _ = augment_data(data, node_aug, edge_aug, train_nodes_to_consider, all_subgraph_indices)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#    x_copy = x_copy.requires_grad_(True)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m log_logits_copy          \u001b[38;5;241m=\u001b[39m \u001b[43mnode_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m probas_copy \u001b[38;5;241m=\u001b[39m log_logits_copy\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m     68\u001b[0m loss_watermark_scaled_copy \u001b[38;5;241m=\u001b[39m compute_watermark_loss(subgraph_dict, probas_copy, beta_weights)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/models.py:201\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    199\u001b[0m intermediate_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnLayers):\n\u001b[0;32m--> 201\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(x)\n\u001b[1;32m    203\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:263\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:547\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 547\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    549\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_beta_weights(subgraph_dict, num_features):\n",
    "    if config.watermark_loss_kwargs['balance_beta_weights'] == True:\n",
    "        beta_weights = get_balanced_beta_weights([subgraph_dict[sig]['subgraph'] for sig in subgraph_dict.keys()])\n",
    "    elif config.watermark_loss_kwargs['balance_beta_weights'] == False:\n",
    "        beta_weights = torch.ones(len(subgraph_dict),num_features)\n",
    "    return beta_weights\n",
    "\n",
    "def process_beta(beta, alpha, omit_indices, scale_beta_method='clip'):\n",
    "    if scale_beta_method=='tanh':\n",
    "        beta = torch.tanh(alpha*beta)\n",
    "    elif scale_beta_method=='tan':\n",
    "        beta = torch.tan(alpha*beta)\n",
    "    elif scale_beta_method=='clip':\n",
    "        beta = torch.clip(beta,min=-1,max=1)\n",
    "    elif scale_beta_method==None:\n",
    "        pass\n",
    "    beta = beta.clone()  # Avoid in-place operation\n",
    "    if omit_indices is not None and len(omit_indices)>0:\n",
    "        beta[omit_indices] = 0 # zero out non-contributing indices\n",
    "    return beta\n",
    "\n",
    "def compute_watermark_loss(subgraph_dict, probas, beta_weights):\n",
    "    watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "    regression_kwargs = config.regression_kwargs\n",
    "    optimization_kwargs = config.optimization_kwargs\n",
    "\n",
    "    loss_watermark = torch.tensor(0.0)\n",
    "    for s, sig in enumerate(subgraph_dict.keys()):\n",
    "        this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "        x_sub, y_sub = data_sub.x, probas[subgraph_node_indices]\n",
    "        ''' epoch condtion: epoch==epoch-1'''\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        raw_beta            = solve_regression(x_sub, y_sub, regression_kwargs['lambda'])\n",
    "        beta                = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta*this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "\n",
    "        balanced_beta_weights = beta_weights[s]\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        loss_watermark = loss_watermark+torch.mean(torch.clamp(watermark_loss_kwargs['epsilon']-B_x_W, min=0)*balanced_beta_weights)\n",
    "\n",
    "\n",
    "    loss_watermark = loss_watermark/len(subgraph_dict)\n",
    "    loss_watermark_scaled = loss_watermark*optimization_kwargs['coefWmk']\n",
    "    return loss_watermark_scaled\n",
    "\n",
    "\n",
    "beta_weights = get_beta_weights(subgraph_dict, data.x.shape[1])\n",
    "all_subgraph_indices = []\n",
    "for sig in subgraph_dict.keys():\n",
    "    nodeIndices = subgraph_dict[sig]['nodeIndices']\n",
    "    all_subgraph_indices+=nodeIndices.tolist()\n",
    "reg_copy=None\n",
    "\n",
    "\n",
    "lr = 0.002\n",
    "optimizer = optim.Adam(node_classifier.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for j in range(100):\n",
    "    x_copy,edge_index_copy = copy.deepcopy(data).x, copy.deepcopy(data).edge_index\n",
    "    x_copy = x_copy.requires_grad_(True)\n",
    "    #with torch.no_grad():\n",
    "    #    edge_index_copy, x_copy, _ = augment_data(data, node_aug, edge_aug, train_nodes_to_consider, all_subgraph_indices)\n",
    "    #    x_copy = x_copy.requires_grad_(True)\n",
    "    log_logits_copy          = node_classifier(x_copy, edge_index_copy)\n",
    "    probas_copy = log_logits_copy.clone().exp()\n",
    "    loss_watermark_scaled_copy = compute_watermark_loss(subgraph_dict, probas_copy, beta_weights)\n",
    "    wmk_loss = loss_watermark_scaled_copy+reg_copy if reg_copy is not None else loss_watermark_scaled_copy\n",
    "    optimizer.zero_grad()\n",
    "    wmk_loss.backward()\n",
    "    print(f'{j}/20: {wmk_loss:3f}',end='\\r')\n",
    "    this_grad = torch.zeros_like(data.x)\n",
    "    this_grad[all_subgraph_indices] = x_copy.grad.clone()[all_subgraph_indices]\n",
    "    this_grad[all_subgraph_indices] = (this_grad[all_subgraph_indices]-this_grad[all_subgraph_indices].mean())/(this_grad[all_subgraph_indices].max()-this_grad[all_subgraph_indices].min())\n",
    "    x_grad_wmk_loss = torch.zeros_like(data.x)\n",
    "    x_grad_wmk_loss[all_subgraph_indices] = this_grad.clone()[all_subgraph_indices]\n",
    "    # print(x_grad_wmk_loss[all_subgraph_indices])\n",
    "    data.x = data.x - perturb_lr*x_grad_wmk_loss\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  7,  11,  23,  34,  37,  41,  43,  69,  73,  86,  97, 105, 109, 110,\n",
      "        134, 144, 154, 159, 168, 177, 185, 186, 195, 212, 217, 219, 225, 227,\n",
      "        238, 250, 259, 277, 279, 284, 298, 328, 355, 358, 363, 365, 367, 376,\n",
      "        382, 386, 427, 430, 439, 442, 443, 457, 489, 503, 506, 507, 515, 520,\n",
      "        542, 544, 554, 559, 582, 597, 617, 649, 664, 687, 695, 698, 704, 708,\n",
      "        715, 729, 732, 747, 755, 758])\n"
     ]
    }
   ],
   "source": [
    "beta_weights = get_beta_weights(subgraph_dict, data.x.shape[1])\n",
    "for s, sig in enumerate(subgraph_dict.keys()):\n",
    "    this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "    x_sub, y_sub = data_sub.x, probas[subgraph_node_indices]\n",
    "    ''' epoch condtion: epoch==epoch-1'''\n",
    "    omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "    print(not_omit_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['11293_4896_5573_3951_10751_63'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_2_env",
   "language": "python",
   "name": "proj_2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
