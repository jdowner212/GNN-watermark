{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarFunction\n"
     ]
    }
   ],
   "source": [
    "from eaaw_graphlime_utils import *\n",
    "import gc\n",
    "\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "\n",
    "\n",
    "from   pcgrad.pcgrad import PCGrad # from the following: https://github.com/WeiChengTseng/Pytorch-PCGrad. Renamed to 'pcgrad' and moved to site-packages folder.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: <class 'torch_geometric.datasets.flickr.Flickr'>\n",
      "Transorms used when loading Flickr: ['NormalizeFeatures()', 'ChooseLargestMaskForTrain()']\n",
      "train_mask: 44625\n",
      "test_mask: 22313\n",
      "val_mask: 22312\n"
     ]
    }
   ],
   "source": [
    "dataset_name='Flickr'\n",
    "if dataset_attributes[dataset_name]['single_or_multi_graph']=='single':\n",
    "    dataset = prep_data(dataset_name=dataset_name, location='default', batch_size='default', transform_list='default',\n",
    "                        train_val_test_split=[0.9,0.05,0.05])\n",
    "    graph_to_watermark = data = dataset[0]\n",
    "elif dataset_attributes[dataset_name]['single_or_multi_graph']=='multi':\n",
    "    [train_dataset, val_dataset, test_dataset], [train_loader, val_loader, test_loader] = prep_data(dataset_name=dataset_name, location='default', \n",
    "                                                                                                    batch_size='default', transform_list='default',\n",
    "                                                                                                                            train_val_test_split=[0.9,0.05,0.05])\n",
    "    graph_to_watermark = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization_kwargs, node_classifier_kwargs, watermark_kwargs, subgraph_kwargs, augment_kwargs, watermark_loss_kwargs, regression_kwargs = get_presets(dataset, 'default')\n",
    "get_presets(dataset,dataset_name)\n",
    "\n",
    "compare_unimportant_against_random=False\n",
    "\n",
    "config.node_classifier_kwargs['dropout']=0.1\n",
    "config.node_classifier_kwargs['dropout_subgraphs']=0\n",
    "config.watermark_kwargs['unimportant_selection_kwargs']['clf_only_epochs'] = 20\n",
    "config.optimization_kwargs['lr']=0.001\n",
    "config.optimization_kwargs['epochs']=70\n",
    "# config.optimization_kwargs['coefWmk']=300\n",
    "config.optimization_kwargs['perturb_x']=False\n",
    "config.optimization_kwargs['perturb_lr']=1e5\n",
    "\n",
    "config.optimization_kwargs['coefWmk_kwargs']= {\n",
    "                                                'coefWmk':50,\n",
    "                                                'schedule_coef_wmk': False,\n",
    "                                                'min_coefWmk_scheduled': 100,\n",
    "                                                'reach_max_coef_wmk_by_epoch':70,\n",
    "                                                }\n",
    "\n",
    "config.optimization_kwargs['use_pcgrad']=True\n",
    "config.optimization_kwargs['use_sam']=False\n",
    "config.optimization_kwargs['sam_momentum']=0.5\n",
    "config.optimization_kwargs['sam_rho']=1e-5\n",
    "\n",
    "config.optimization_kwargs['penalize_similar_subgraphs']=False\n",
    "config.optimization_kwargs['p_swap']=0.5\n",
    "config.optimization_kwargs['shifted_subgraph_loss_coef']=1e-1\n",
    "\n",
    "\n",
    "config.optimization_kwargs['use_gradnorm']=False\n",
    "\n",
    "\n",
    "config.augment_kwargs['p']=0.3\n",
    "config.augment_kwargs['nodeDrop']['use']=True\n",
    "config.augment_kwargs['nodeDrop']['p']=0.1\n",
    "config.augment_kwargs['nodeMixUp']['use']=True\n",
    "config.augment_kwargs['nodeMixUp']['lambda']=5\n",
    "config.augment_kwargs['nodeFeatMask']['use']=False\n",
    "config.augment_kwargs['edgeDrop']['use']=True\n",
    "config.augment_kwargs['edgeDrop']['p']=0.1\n",
    "\n",
    "config.augment_kwargs['separate_trainset_from_subgraphs'] = True\n",
    "config.augment_kwargs['ignore_subgraphs'] = True\n",
    "config.watermark_kwargs['percent_of_features_to_watermark']=3\n",
    "config.watermark_kwargs['watermark_type']='most_represented'\n",
    "config.subgraph_kwargs['numSubgraphs']=7\n",
    "config.subgraph_kwargs['fraction']=0.01\n",
    "config.subgraph_kwargs['method']='random'\n",
    "config.optimization_kwargs['sacrifice_kwargs']['method']='subgraph_node_indices'\n",
    "config.optimization_kwargs['sacrifice_kwargs']['percentage']=1\n",
    "config.optimization_kwargs['separate_forward_passes_per_subgraph']=False\n",
    "config.optimization_kwargs['clf_only']=False\n",
    "config.watermark_loss_kwargs['epsilon']=1e-1\n",
    "\n",
    "config.regression_kwargs['lambda']=1e-3\n",
    "\n",
    "validate_kwargs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = merge_kwargs_dicts()#config.node_classifier_kwargs, config.optimization_kwargs, config.watermark_kwargs, config.subgraph_kwargs, config.regression_kwargs, config.watermark_loss_kwargs, config.augment_kwargs)\n",
    "merged_dict_keys = list(merged_dict.keys()) + ['Train Acc','Val Acc','Match Rates','Final Betas','watermark']\n",
    "all_dfs = pd.DataFrame(dict(zip(merged_dict_keys,[]*len(merged_dict_keys))))\n",
    "scale_beta_method=None\n",
    "debug_multiple_subgraphs=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 5\n",
      "enumerate_over_me: range(0, 2)\n",
      "Before generate_subgraph: numHops = 1\n",
      "selected_nodes: tensor([52043, 80982, 81279, 70818, 18272, 39812, 71085,  8955, 29682, 64421,\n",
      "         9781, 78657, 55580, 24486, 51615, 29299, 36853, 70046, 69811,  2303,\n",
      "        73512, 33737, 51021, 15574, 62714, 66927, 20956, 86705, 73681, 74292,\n",
      "        19697, 54290, 64002, 47058, 28498, 75353, 71068, 31883, 44830, 27529,\n",
      "        16994, 86528, 28127, 74993, 38129,  5175, 79800, 62001, 29788, 61327,\n",
      "        41870, 25925, 52268, 57187, 12641,  9666, 37265, 69493, 72019, 16774,\n",
      "         1944,  7908,  9449,  3685, 13293, 52316, 22058, 32318, 86067, 66819,\n",
      "        63850, 71080, 26735, 24753, 17203, 36698, 55212, 24710, 74006, 76124,\n",
      "        22756, 45164, 13493, 84571, 30563, 53242, 50712, 65328,  8664, 79655,\n",
      "        15298, 13132, 83589,  2032, 55638, 61810, 84221, 55842, 29811, 14492,\n",
      "        31179, 45758, 82179, 64560, 85563, 60739, 51975, 27937, 54351, 69499,\n",
      "        73177, 78108, 43027,  6702, 18250, 69201, 87576, 11972, 76191,  5354,\n",
      "        13734, 37136,  8173, 21653, 49728, 45636, 72722, 44767, 10283,  7933,\n",
      "        48529, 22255, 73844, 87894, 55375,  9380, 38258, 66238, 23509, 46835,\n",
      "        89085, 52057,  6878,  3840, 64182, 61185, 68382, 42383, 81372, 67732,\n",
      "        16739, 28560, 66023, 78114, 21400, 16024, 76552, 74259, 75689, 70934,\n",
      "        56664, 68588, 33090, 70258, 46585, 61978, 22045, 32832, 22371, 79387,\n",
      "        60371, 56578, 53807, 19973, 22589, 57033, 80287,  8471, 37116,  4817,\n",
      "         8857, 50996, 21604, 60785, 68078,  9207, 13318, 25517, 24023, 84313,\n",
      "        83709,  9151, 78155, 34145, 47196, 30094, 87822, 43227, 83807, 83963,\n",
      "        46727, 84364, 70407, 39764, 79509, 84927,  7199, 56897, 76235,  7501,\n",
      "        53923, 21141, 61260,  7386, 21024, 42909, 55332, 27576, 68852, 78510,\n",
      "        83126,  5521, 85661])\n",
      "subgraph_kwargs afterward: {'regenerate': True, 'method': 'random', 'fraction': 0.01, 'numSubgraphs': 2, 'khop_kwargs': {'autoChooseSubGs': True, 'nodeIndices': None, 'numHops': 1, 'max_degree': 50}, 'random_kwargs': {}, 'rwr_kwargs': {'restart_prob': 0.15, 'max_steps': 1000}}\n",
      "Before generate_subgraph: numHops = 1\n",
      "selected_nodes: tensor([87381, 12920, 78109, 83473, 70851, 70246, 68586,  1473, 58389, 38968,\n",
      "         4512, 60661, 61175, 41056,  8751, 22321, 24653, 29388, 20407, 53855,\n",
      "        88966,  3763, 83135, 67826, 80907, 67967, 68338, 67168, 54421, 88912,\n",
      "        19036, 41136, 50880, 37048, 68337, 72935, 30928, 75179, 15494, 13195,\n",
      "        82912, 71194, 18543, 11326, 55508, 15309, 73446, 79411, 44396, 67147,\n",
      "        22095, 56550, 80160, 47617, 24050, 38867, 56110, 43122, 53961,  4153,\n",
      "        78818, 56026, 77834, 33332, 40058, 51155, 67746, 37013,  4290, 29190,\n",
      "         1345, 46500, 33864, 19138, 35045, 45841, 17053, 38926, 40396, 79775,\n",
      "         1620, 64010, 12767, 32662, 51719, 69711, 11974, 81178, 25700, 16839,\n",
      "         6941, 24990, 25340, 54099,   145, 57158, 62136, 72094, 67628, 10044,\n",
      "        75687, 26734, 51492, 79188, 19498, 50273, 61241, 72103, 40781, 14684,\n",
      "        84485, 64262, 86587, 29902, 17394, 84010, 75875,  6531, 38842,  7535,\n",
      "         6320, 36489, 48285, 18128, 63848, 37033, 75968,   335, 64721, 16042,\n",
      "        30409, 13714, 39571, 30498, 13268, 33306, 31774,  4647, 52137,  7785,\n",
      "        45366, 16531, 82274, 60234, 53847, 32938, 30724, 47007, 46295, 61838,\n",
      "        10222, 59658, 83266, 68650, 16098, 10856, 31486,  6093,  3509, 32506,\n",
      "        36879, 78272, 46772, 36790, 80978, 68583, 69649, 22547, 36051, 45358,\n",
      "         3099, 70737, 79149, 57013, 82355, 62343, 57892, 51076, 69187, 18130,\n",
      "        56167, 78995, 46805, 28818, 83366, 73121, 22715, 13831, 42577, 86400,\n",
      "        12366, 22924, 17953, 44423, 82427, 85286, 68109,   292, 66457, 42310,\n",
      "        77972,  5696, 11921, 58125, 35877, 48354, 38788, 78431, 48175, 86870,\n",
      "        16270, 83227, 30653,  3520, 85155, 10898, 78423, 85440, 14909,  1614,\n",
      "        79316, 20050, 28428])\n",
      "subgraph_kwargs afterward: {'regenerate': True, 'method': 'random', 'fraction': 0.01, 'numSubgraphs': 2, 'khop_kwargs': {'autoChooseSubGs': True, 'nodeIndices': None, 'numHops': 1, 'max_degree': 50}, 'random_kwargs': {}, 'rwr_kwargs': {'restart_prob': 0.15, 'max_steps': 1000}}\n",
      "nodeIndices: [52043, 80982, 81279, 70818, 18272, 39812, 71085, 8955, 29682, 64421, 9781, 78657, 55580, 24486, 51615, 29299, 36853, 70046, 69811, 2303, 73512, 33737, 51021, 15574, 62714, 66927, 20956, 86705, 73681, 74292, 19697, 54290, 64002, 47058, 28498, 75353, 71068, 31883, 44830, 27529, 16994, 86528, 28127, 74993, 38129, 5175, 79800, 62001, 29788, 61327, 41870, 25925, 52268, 57187, 12641, 9666, 37265, 69493, 72019, 16774, 1944, 7908, 9449, 3685, 13293, 52316, 22058, 32318, 86067, 66819, 63850, 71080, 26735, 24753, 17203, 36698, 55212, 24710, 74006, 76124, 22756, 45164, 13493, 84571, 30563, 53242, 50712, 65328, 8664, 79655, 15298, 13132, 83589, 2032, 55638, 61810, 84221, 55842, 29811, 14492, 31179, 45758, 82179, 64560, 85563, 60739, 51975, 27937, 54351, 69499, 73177, 78108, 43027, 6702, 18250, 69201, 87576, 11972, 76191, 5354, 13734, 37136, 8173, 21653, 49728, 45636, 72722, 44767, 10283, 7933, 48529, 22255, 73844, 87894, 55375, 9380, 38258, 66238, 23509, 46835, 89085, 52057, 6878, 3840, 64182, 61185, 68382, 42383, 81372, 67732, 16739, 28560, 66023, 78114, 21400, 16024, 76552, 74259, 75689, 70934, 56664, 68588, 33090, 70258, 46585, 61978, 22045, 32832, 22371, 79387, 60371, 56578, 53807, 19973, 22589, 57033, 80287, 8471, 37116, 4817, 8857, 50996, 21604, 60785, 68078, 9207, 13318, 25517, 24023, 84313, 83709, 9151, 78155, 34145, 47196, 30094, 87822, 43227, 83807, 83963, 46727, 84364, 70407, 39764, 79509, 84927, 7199, 56897, 76235, 7501, 53923, 21141, 61260, 7386, 21024, 42909, 55332, 27576, 68852, 78510, 83126, 5521, 85661]\n",
      "nodeIndices: [87381, 12920, 78109, 83473, 70851, 70246, 68586, 1473, 58389, 38968, 4512, 60661, 61175, 41056, 8751, 22321, 24653, 29388, 20407, 53855, 88966, 3763, 83135, 67826, 80907, 67967, 68338, 67168, 54421, 88912, 19036, 41136, 50880, 37048, 68337, 72935, 30928, 75179, 15494, 13195, 82912, 71194, 18543, 11326, 55508, 15309, 73446, 79411, 44396, 67147, 22095, 56550, 80160, 47617, 24050, 38867, 56110, 43122, 53961, 4153, 78818, 56026, 77834, 33332, 40058, 51155, 67746, 37013, 4290, 29190, 1345, 46500, 33864, 19138, 35045, 45841, 17053, 38926, 40396, 79775, 1620, 64010, 12767, 32662, 51719, 69711, 11974, 81178, 25700, 16839, 6941, 24990, 25340, 54099, 145, 57158, 62136, 72094, 67628, 10044, 75687, 26734, 51492, 79188, 19498, 50273, 61241, 72103, 40781, 14684, 84485, 64262, 86587, 29902, 17394, 84010, 75875, 6531, 38842, 7535, 6320, 36489, 48285, 18128, 63848, 37033, 75968, 335, 64721, 16042, 30409, 13714, 39571, 30498, 13268, 33306, 31774, 4647, 52137, 7785, 45366, 16531, 82274, 60234, 53847, 32938, 30724, 47007, 46295, 61838, 10222, 59658, 83266, 68650, 16098, 10856, 31486, 6093, 3509, 32506, 36879, 78272, 46772, 36790, 80978, 68583, 69649, 22547, 36051, 45358, 3099, 70737, 79149, 57013, 82355, 62343, 57892, 51076, 69187, 18130, 56167, 78995, 46805, 28818, 83366, 73121, 22715, 13831, 42577, 86400, 12366, 22924, 17953, 44423, 82427, 85286, 68109, 292, 66457, 42310, 77972, 5696, 11921, 58125, 35877, 48354, 38788, 78431, 48175, 86870, 16270, 83227, 30653, 3520, 85155, 10898, 78423, 85440, 14909, 1614, 79316, 20050, 28428]\n",
      "Sacrificing 100% of subgraph nodes from node classification training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579d3f01e17042abb3cd07568ac485ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len sorted_indices: 500\n",
      "len_watermark: 15\n",
      "most_represented_indices: tensor([ 62, 392, 425,  36, 302, 316,  40, 203, 482, 255, 156, 283, 442, 135,\n",
      "        306])\n",
      "watermarks: [tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  1.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      "watermark_indices: tensor([ 36,  40,  62, 135, 156, 203, 255, 283, 302, 306, 316, 392, 425, 442,\n",
      "        482])\n",
      "Epoch:   0, loss_primary = 1.934, loss_watermark = 2.037, B*W = -0.00186, train acc = 0.240, val acc = 0.246\n",
      "Epoch:   1, loss_primary = 1.931, loss_watermark = 1.847, B*W = 0.00767, train acc = 0.257, val acc = 0.258\n",
      "Epoch:   2, loss_primary = 1.922, loss_watermark = 1.689, B*W = 0.01556, train acc = 0.257, val acc = 0.258\n",
      "Epoch:   3, loss_primary = 1.911, loss_watermark = 1.542, B*W = 0.02292, train acc = 0.257, val acc = 0.258\n",
      "Epoch:   4, loss_primary = 1.899, loss_watermark = 1.396, B*W = 0.03200, train acc = 0.258, val acc = 0.259\n",
      "Epoch:   5, loss_primary = 1.885, loss_watermark = 1.257, B*W = 0.04196, train acc = 0.266, val acc = 0.265\n",
      "Epoch:   6, loss_primary = 1.872, loss_watermark = 1.126, B*W = 0.05080, train acc = 0.288, val acc = 0.283\n",
      "Epoch:   7, loss_primary = 1.858, loss_watermark = 1.022, B*W = 0.05716, train acc = 0.323, val acc = 0.329\n",
      "Epoch:   8, loss_primary = 1.842, loss_watermark = 0.931, B*W = 0.06026, train acc = 0.366, val acc = 0.373\n",
      "Epoch:   9, loss_primary = 1.827, loss_watermark = 0.881, B*W = 0.06136, train acc = 0.395, val acc = 0.409\n",
      "Epoch:  10, loss_primary = 1.810, loss_watermark = 0.872, B*W = 0.06129, train acc = 0.411, val acc = 0.421\n",
      "Epoch:  11, loss_primary = 1.792, loss_watermark = 0.867, B*W = 0.06181, train acc = 0.417, val acc = 0.423\n",
      "Epoch:  12, loss_primary = 1.774, loss_watermark = 0.835, B*W = 0.06528, train acc = 0.420, val acc = 0.424\n",
      "Epoch:  13, loss_primary = 1.755, loss_watermark = 0.798, B*W = 0.06878, train acc = 0.421, val acc = 0.424\n",
      "Epoch:  14, loss_primary = 1.737, loss_watermark = 0.782, B*W = 0.07028, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  15, loss_primary = 1.719, loss_watermark = 0.768, B*W = 0.07013, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  16, loss_primary = 1.701, loss_watermark = 0.762, B*W = 0.06861, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  17, loss_primary = 1.683, loss_watermark = 0.768, B*W = 0.06675, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  18, loss_primary = 1.667, loss_watermark = 0.772, B*W = 0.06613, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  19, loss_primary = 1.652, loss_watermark = 0.763, B*W = 0.06697, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  20, loss_primary = 1.639, loss_watermark = 0.755, B*W = 0.06787, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  21, loss_primary = 1.627, loss_watermark = 0.748, B*W = 0.06839, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  22, loss_primary = 1.616, loss_watermark = 0.744, B*W = 0.06834, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  23, loss_primary = 1.607, loss_watermark = 0.743, B*W = 0.06828, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  24, loss_primary = 1.600, loss_watermark = 0.740, B*W = 0.06831, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  25, loss_primary = 1.593, loss_watermark = 0.736, B*W = 0.06843, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  26, loss_primary = 1.588, loss_watermark = 0.732, B*W = 0.06842, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  27, loss_primary = 1.585, loss_watermark = 0.729, B*W = 0.06846, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  28, loss_primary = 1.583, loss_watermark = 0.721, B*W = 0.06888, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  29, loss_primary = 1.582, loss_watermark = 0.712, B*W = 0.06963, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  30, loss_primary = 1.581, loss_watermark = 0.707, B*W = 0.07032, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  31, loss_primary = 1.582, loss_watermark = 0.701, B*W = 0.07088, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  32, loss_primary = 1.582, loss_watermark = 0.695, B*W = 0.07127, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  33, loss_primary = 1.582, loss_watermark = 0.690, B*W = 0.07148, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  34, loss_primary = 1.582, loss_watermark = 0.685, B*W = 0.07156, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  35, loss_primary = 1.582, loss_watermark = 0.680, B*W = 0.07171, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  36, loss_primary = 1.582, loss_watermark = 0.674, B*W = 0.07197, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  37, loss_primary = 1.582, loss_watermark = 0.669, B*W = 0.07219, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  38, loss_primary = 1.583, loss_watermark = 0.664, B*W = 0.07240, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  39, loss_primary = 1.583, loss_watermark = 0.658, B*W = 0.07291, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  40, loss_primary = 1.583, loss_watermark = 0.653, B*W = 0.07334, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  41, loss_primary = 1.584, loss_watermark = 0.649, B*W = 0.07365, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  42, loss_primary = 1.585, loss_watermark = 0.645, B*W = 0.07405, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  43, loss_primary = 1.584, loss_watermark = 0.641, B*W = 0.07463, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  44, loss_primary = 1.583, loss_watermark = 0.637, B*W = 0.07509, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  45, loss_primary = 1.583, loss_watermark = 0.634, B*W = 0.07532, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  46, loss_primary = 1.583, loss_watermark = 0.632, B*W = 0.07540, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  47, loss_primary = 1.583, loss_watermark = 0.629, B*W = 0.07563, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  48, loss_primary = 1.583, loss_watermark = 0.625, B*W = 0.07584, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  49, loss_primary = 1.583, loss_watermark = 0.622, B*W = 0.07590, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  50, loss_primary = 1.582, loss_watermark = 0.620, B*W = 0.07578, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  51, loss_primary = 1.582, loss_watermark = 0.617, B*W = 0.07575, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  52, loss_primary = 1.582, loss_watermark = 0.616, B*W = 0.07600, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  53, loss_primary = 1.581, loss_watermark = 0.614, B*W = 0.07655, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  54, loss_primary = 1.581, loss_watermark = 0.611, B*W = 0.07696, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  55, loss_primary = 1.581, loss_watermark = 0.609, B*W = 0.07724, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  56, loss_primary = 1.581, loss_watermark = 0.607, B*W = 0.07728, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  57, loss_primary = 1.581, loss_watermark = 0.604, B*W = 0.07723, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  58, loss_primary = 1.581, loss_watermark = 0.602, B*W = 0.07733, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  59, loss_primary = 1.581, loss_watermark = 0.600, B*W = 0.07758, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  60, loss_primary = 1.580, loss_watermark = 0.597, B*W = 0.07762, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  61, loss_primary = 1.580, loss_watermark = 0.595, B*W = 0.07747, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  62, loss_primary = 1.580, loss_watermark = 0.592, B*W = 0.07723, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  63, loss_primary = 1.580, loss_watermark = 0.590, B*W = 0.07755, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  64, loss_primary = 1.580, loss_watermark = 0.587, B*W = 0.07782, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  65, loss_primary = 1.580, loss_watermark = 0.584, B*W = 0.07803, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  66, loss_primary = 1.580, loss_watermark = 0.586, B*W = 0.07798, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  67, loss_primary = 1.580, loss_watermark = 0.580, B*W = 0.07837, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  68, loss_primary = 1.580, loss_watermark = 0.580, B*W = 0.07857, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  69, loss_primary = 1.580, loss_watermark = 0.579, B*W = 0.07880, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  70, loss_primary = 1.580, loss_watermark = 0.578, B*W = 0.07889, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  71, loss_primary = 1.579, loss_watermark = 0.576, B*W = 0.07909, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  72, loss_primary = 1.579, loss_watermark = 0.575, B*W = 0.07910, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  73, loss_primary = 1.579, loss_watermark = 0.572, B*W = 0.07929, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  74, loss_primary = 1.579, loss_watermark = 0.571, B*W = 0.07967, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  75, loss_primary = 1.579, loss_watermark = 0.569, B*W = 0.07975, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  76, loss_primary = 1.578, loss_watermark = 0.569, B*W = 0.07979, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  77, loss_primary = 1.578, loss_watermark = 0.567, B*W = 0.08028, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  78, loss_primary = 1.578, loss_watermark = 0.566, B*W = 0.08042, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  79, loss_primary = 1.578, loss_watermark = 0.564, B*W = 0.08044, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  80, loss_primary = 1.577, loss_watermark = 0.563, B*W = 0.08020, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  81, loss_primary = 1.578, loss_watermark = 0.560, B*W = 0.08009, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  82, loss_primary = 1.578, loss_watermark = 0.558, B*W = 0.08041, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  83, loss_primary = 1.578, loss_watermark = 0.556, B*W = 0.08057, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  84, loss_primary = 1.577, loss_watermark = 0.553, B*W = 0.08052, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  85, loss_primary = 1.578, loss_watermark = 0.551, B*W = 0.08033, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  86, loss_primary = 1.576, loss_watermark = 0.550, B*W = 0.08029, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  87, loss_primary = 1.576, loss_watermark = 0.548, B*W = 0.08068, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  88, loss_primary = 1.575, loss_watermark = 0.550, B*W = 0.08127, train acc = 0.422, val acc = 0.424\n",
      "Epoch:  89, loss_primary = 1.575, loss_watermark = 0.549, B*W = 0.08148, train acc = 0.422, val acc = 0.424\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39msubgraph_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumSubgraphs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m subgraphs.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWatermarking \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mwatermark_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercent_of_features_to_watermark\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% of node features\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m Trainer_ \u001b[38;5;241m=\u001b[39m Trainer(data, dataset_name)\n\u001b[1;32m     18\u001b[0m node_classifier, history, subgraph_dict, \\\n\u001b[0;32m---> 19\u001b[0m all_feature_importances, all_watermark_indices, probas \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug_multiple_subgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n\u001b[1;32m     22\u001b[0m percent_matches, percent_match_mean, percent_match_std, \\\n\u001b[1;32m     23\u001b[0m     primary_acc_curve, watermark_acc_curve, train_acc, val_acc \u001b[38;5;241m=\u001b[39m get_performance_trends(history, subgraph_dict)\n\u001b[1;32m     24\u001b[0m final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/eaaw_graphlime_utils.py:829\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, debug_multiple_subgraphs, save, print_every)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta_similarities\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_similarities_dict\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m replace_history_Nones(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    830\u001b[0m     save_results(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_classifier, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubgraph_dict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meach_subgraph_feature_importances, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meach_subgraph_watermark_indices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobas)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_classifier, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubgraph_dict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meach_subgraph_feature_importances, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meach_subgraph_watermark_indices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobas\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/general_utils.py:250\u001b[0m, in \u001b[0;36msave_results\u001b[0;34m(dataset_name, node_classifier, history, subgraph_dict, all_feature_importances, all_watermark_indices, probas)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m object_name, \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubgraph_dict\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_feature_importances\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_watermark_indices\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobas\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    248\u001b[0m                                [ node_classifier,  history,  subgraph_dict,  all_feature_importances,  all_watermark_indices,  probas]):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(results_folder_name,object_name),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 250\u001b[0m         \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNode classifier, history, subgraph dict, feature importances, watermark indices, and probas saved in:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_folder_name)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "config.optimization_kwargs['separate_forward_passes_per_subgraph']=True\n",
    "# for type_ in ['unimportant','most_represented']:\n",
    "type_ = 'most_represented'\n",
    "config.watermark_kwargs['watermark_type'] = type_\n",
    "if type_ == 'unimportant':\n",
    "    config.optimization_kwargs['coefWmk_kwargs']['coefWmk'] = 50\n",
    "    config.optimization_kwargs['epochs'] = 70\n",
    "else:\n",
    "    config.optimization_kwargs['coefWmk_kwargs']['coefWmk'] = 20\n",
    "    config.optimization_kwargs['epochs'] = 90\n",
    "for perc in [3,10,50]:\n",
    "    config.watermark_kwargs['percent_of_features_to_watermark']=perc\n",
    "    for num_sub in [2,5,10]:\n",
    "        config.subgraph_kwargs['numSubgraphs']=num_sub\n",
    "        title_ = f'Separate Forward Passes -- {type_} feature indices'\n",
    "        title = f'{title_}.\\n{config.subgraph_kwargs['numSubgraphs']} subgraphs.\\nWatermarking {config.watermark_kwargs['percent_of_features_to_watermark']}% of node features'\n",
    "        Trainer_ = Trainer(data, dataset_name)\n",
    "        node_classifier, history, subgraph_dict, \\\n",
    "        all_feature_importances, all_watermark_indices, probas = Trainer_.train(debug_multiple_subgraphs=False, save=True, print_every=1)\n",
    "\n",
    "        primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "        percent_matches, percent_match_mean, percent_match_std, \\\n",
    "            primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "        final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subgraph_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7551, dtype=torch.float64) tensor(0.7802, dtype=torch.float64)\n",
      "tensor(0.8130, dtype=torch.float64) tensor(0.8195, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "Trainer_.node_classifier.eval()\n",
    "Trainer_.optimizer.zero_grad()\n",
    "log_logits          = Trainer_.forward(data.x, data.edge_index, dropout=config.node_classifier_kwargs['dropout'])\n",
    "acc_trn = accuracy(log_logits[Trainer_.train_mask], data.y[Trainer_.train_mask],verbose=False)\n",
    "acc_val = accuracy(log_logits[Trainer_.val_mask],   data.y[Trainer_.val_mask],verbose=False)\n",
    "print(acc_trn, acc_val)\n",
    "Trainer_.node_classifier.eval()\n",
    "Trainer_.optimizer.zero_grad()\n",
    "log_logits          = Trainer_.forward(Trainer_.x, Trainer_.edge_index, dropout=config.node_classifier_kwargs['dropout'])\n",
    "acc_trn = accuracy(log_logits[Trainer_.train_mask], Trainer_.y[Trainer_.train_mask],verbose=False)\n",
    "acc_val = accuracy(log_logits[Trainer_.val_mask],   Trainer_.y[Trainer_.val_mask],verbose=False)\n",
    "print(acc_trn, acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_subgraph(self, p_to_swap, subgraph_node_indices):\n",
    "    num_to_swap = int(p_to_swap*len(subgraph_node_indices))\n",
    "    random_indices = torch.randperm(len(subgraph_node_indices))\n",
    "    subgraph_node_indices = subgraph_node_indices[random_indices[:len(subgraph_node_indices)-num_to_swap]]\n",
    "    filtered_tensor = Trainer_.train_nodes_to_consider[~Trainer_.train_nodes_to_consider.unsqueeze(1).eq(subgraph_node_indices).any(dim=1)]\n",
    "    random_index = torch.randint(0, filtered_tensor.size(0), (num_to_swap,))\n",
    "    random_element = filtered_tensor[random_index]\n",
    "    subgraph_node_indices = torch.concatenate([subgraph_node_indices, random_element])\n",
    "\n",
    "    sub_edge_index, _ = subgraph(subgraph_node_indices, self.data.edge_index, relabel_nodes=True, num_nodes=self.data.num_nodes)\n",
    "    shifted_subgraph = Data(\n",
    "        x          = self.data.x[subgraph_node_indices]          if self.data.x is not None else None,\n",
    "        edge_index = sub_edge_index,\n",
    "        y          = self.data.y[subgraph_node_indices]          if self.data.y is not None else None,\n",
    "        train_mask = self.data.train_mask[subgraph_node_indices] if self.data.train_mask is not None else None,\n",
    "        test_mask  = self.data.test_mask[subgraph_node_indices]  if self.data.test_mask is not None else None,\n",
    "        val_mask   = self.data.val_mask[subgraph_node_indices]   if self.data.val_mask is not None else None)\n",
    "    return shifted_subgraph\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_.percent_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full prediction accuracies:\n",
      "tensor(0.8491, dtype=torch.float64)\n",
      "tensor(0.7925, dtype=torch.float64)\n",
      "tensor(0.7358, dtype=torch.float64)\n",
      "tensor(0.7547, dtype=torch.float64)\n",
      "tensor(0.6604, dtype=torch.float64)\n",
      "tensor(0.7547, dtype=torch.float64)\n",
      "tensor(0.7547, dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'all_watermark_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m this_raw_beta \u001b[38;5;241m=\u001b[39m solve_regression(x_sub, y_sub, regression_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     52\u001b[0m beta  \u001b[38;5;241m=\u001b[39m process_beta(this_raw_beta, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m], omit_indices, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_beta_method\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 53\u001b[0m these_watermark_indices \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_watermark_indices\u001b[49m[i]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#print('watermark:',this_watermark[these_watermark_indices])\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#print('beta:     ',torch.sign(beta[these_watermark_indices]))\u001b[39;00m\n\u001b[1;32m     56\u001b[0m match \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(this_watermark[these_watermark_indices]\u001b[38;5;241m==\u001b[39mtorch\u001b[38;5;241m.\u001b[39msign(beta[these_watermark_indices]))\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(these_watermark_indices)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'all_watermark_indices'"
     ]
    }
   ],
   "source": [
    "Trainer_.node_classifier.eval()\n",
    "Trainer_.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "log_logits   = Trainer_.forward(data.x, data.edge_index, dropout=0)\n",
    "print('Full prediction accuracies:')\n",
    "for i, sig in enumerate(Trainer_.subgraph_dict.keys()):\n",
    "    node_indices = Trainer_.subgraph_dict[sig]['nodeIndices']\n",
    "    these_out = log_logits[node_indices]\n",
    "    these_y = data.y[node_indices]\n",
    "    acc = accuracy(these_out, these_y, verbose=False)\n",
    "    print(acc)\n",
    "for p_to_swap in [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]:\n",
    "    matches = []\n",
    "    sign_betas = []\n",
    "    for i, sig in enumerate(Trainer_.subgraph_dict.keys()):\n",
    "        node_indices = Trainer_.subgraph_dict[sig]['nodeIndices']\n",
    "        num_to_swap = int(p_to_swap*len(node_indices))\n",
    "\n",
    "        # print('node indices start:', node_indices)\n",
    "        random_indices = torch.randperm(len(node_indices))\n",
    "        node_indices = node_indices[random_indices[:len(node_indices)-num_to_swap]]\n",
    "        filtered_tensor = Trainer_.train_nodes_to_consider[~Trainer_.train_nodes_to_consider.unsqueeze(1).eq(node_indices).any(dim=1)]\n",
    "        random_index = torch.randint(0, filtered_tensor.size(0), (num_to_swap,))\n",
    "        random_element = filtered_tensor[random_index]\n",
    "        node_indices = torch.concatenate([node_indices, random_element])\n",
    "        # print('node indices end  :', node_indices)\n",
    "\n",
    "        sub_edge_index, _ = subgraph(node_indices, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "        subgraph_ = Data(\n",
    "            x=data.x[node_indices] if data.x is not None else None,\n",
    "            edge_index=sub_edge_index,\n",
    "            y=data.y[node_indices] if data.y is not None else None,\n",
    "            train_mask=data.train_mask[node_indices] if data.train_mask is not None else None,\n",
    "            test_mask=data.test_mask[node_indices] if data.test_mask is not None else None,\n",
    "            val_mask=data.val_mask[node_indices] if data.val_mask is not None else None)\n",
    "    \n",
    "\n",
    "        x_sub, edge_index_sub = subgraph_.x, subgraph_.edge_index\n",
    "        these_log_logits   = Trainer_.forward(x_sub, edge_index_sub, dropout=config.node_classifier_kwargs['dropout'])\n",
    "        these_probas = these_log_logits.clone().exp()\n",
    "        y_sub = these_probas\n",
    "\n",
    "        watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "        regression_kwargs = config.regression_kwargs\n",
    "        this_watermark = subgraph_dict[sig]['watermark']#[subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "\n",
    "\n",
    "        ''' epoch condtion: epoch==epoch-1'''\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        this_raw_beta = solve_regression(x_sub, y_sub, regression_kwargs['lambda'])\n",
    "        beta  = process_beta(this_raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        these_watermark_indices = Trainer_.all_watermark_indices[i]\n",
    "        #print('watermark:',this_watermark[these_watermark_indices])\n",
    "        #print('beta:     ',torch.sign(beta[these_watermark_indices]))\n",
    "        match = torch.sum(this_watermark[these_watermark_indices]==torch.sign(beta[these_watermark_indices]))/len(these_watermark_indices)\n",
    "        sign_betas.append(torch.sign(beta[these_watermark_indices]))\n",
    "        #match_str = str(np.round(100*match.item(),3)) + '%'\n",
    "        matches.append(100*match.item())\n",
    "    print(f'Replacing {int(100*p_to_swap)}% watermark subgraph indices:')\n",
    "    print('watermark match rates:',np.mean(matches))\n",
    "    bs = torch.vstack(sign_betas)\n",
    "    match_counts= count_matches(bs)\n",
    "    print('count of shared sign(beta) values across subgraphs:',match_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(these_watermark_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = Trainer_.train_nodes_to_consider\n",
    "\n",
    "# Separate list of elements to exclude\n",
    "exclude_tensor = node_indices\n",
    "\n",
    "# Convert exclude_list to a tensor for comparison\n",
    "# exclude_tensor = torch.tensor(exclude_list)\n",
    "\n",
    "# Filter the original tensor to exclude elements in the exclude list\n",
    "filtered_tensor = tensor[~tensor.unsqueeze(1).eq(exclude_tensor).any(dim=1)]\n",
    "random_index = torch.randint(0, filtered_tensor.size(0), (5,))\n",
    "random_element = filtered_tensor[random_index]\n",
    "node_indices = torch.concatenate([node_indices, random_element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
       "        [ 0., 10.,  0.,  ..., 10.,  0., 10.],\n",
       "        ...,\n",
       "        [10.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0., -9.,  0.,  ...,  0., -9.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_.x[torch.tensor(list(set(np.concatenate(Trainer_.all_watermark_indices))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['arch', 'activation', 'nLayers', 'hDim', 'dropout', 'dropout_subgraphs', 'skip_connections', 'heads_1', 'heads_2', 'inDim', 'outDim'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.node_classifier_kwargs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m perturb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     59\u001b[0m perturb_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m     60\u001b[0m node_classifier, history, subgraph_dict, \\\n\u001b[0;32m---> 61\u001b[0m     all_feature_importances, all_watermark_indices, probas \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m(data, dataset_name, debug_multiple_subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,perturb\u001b[38;5;241m=\u001b[39mperturb,perturb_lr\u001b[38;5;241m=\u001b[39mperturb_lr)\n\u001b[1;32m     62\u001b[0m primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n\u001b[1;32m     63\u001b[0m     percent_matches, percent_match_mean, percent_match_std, \\\n\u001b[1;32m     64\u001b[0m         primary_acc_curve, watermark_acc_curve, train_acc, val_acc \u001b[38;5;241m=\u001b[39m get_performance_trends(history, subgraph_dict)\n\u001b[1;32m     65\u001b[0m final_plot(history, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "''' (individualize boolean, multisubgraph method, index selection method) '''\n",
    "selection_kwargss = [#(False, None, 'random'), # random, not individualized\n",
    "                    (False,'average','unimportant'), # average, not individualized\n",
    "                    #(False,'concat','unimportant'), # concat, not inividualized\n",
    "                    # (True,None,'unimportant') # unimportant, individualized\n",
    "                    ]\n",
    "\n",
    "''' (method, regenerate_boolean) '''\n",
    "subgraph_methods = [('random',False)]\n",
    "\n",
    "''' [subset, percentage] '''\n",
    "sacrifice_kwargss = [['train_node_indices',1],['train_node_indices',0.75],['subgraph_node_indices',1],[None,None]]\n",
    "\n",
    "''' (method, L2_lambda (if applicable, else None) '''\n",
    "regularization_kwargs = [(None,None),('L2',0.01),('beta_var',None)]\n",
    "\n",
    "\n",
    "variables = {'augment': [{'separate_trainset_from_subgraphs': True,\n",
    "                          'ignore_subgraphs': True,\n",
    "                          'nodeDrop': False,\n",
    "                          'nodeMixUp': False,\n",
    "                          'nodeFeatMask': False,\n",
    "                          'edgeDrop': False},\n",
    "                          {'separate_trainset_from_subgraphs': True,\n",
    "                          'ignore_subgraphs': True,\n",
    "                          'nodeDrop': True,\n",
    "                          'nodeMixUp': True,\n",
    "                          'nodeFeatMask': True,\n",
    "                          'edgeDrop': True},],\n",
    "             'sacrifice_kwargs':  sacrifice_kwargss,\n",
    "             'beta_selection': selection_kwargss,\n",
    "             'use_PCgrad': [True,False],\n",
    "             'subgraph_method': subgraph_methods,\n",
    "             'reg': regularization_kwargs,\n",
    "             'perc': [3],\n",
    "             'clf_epochs': [10,20],\n",
    "             'coef_wmk': [200,600],\n",
    "             'frac': [0.02],\n",
    "             'num_subgraphs': [2,7],\n",
    "             'balance_beta_weights': [True,False],\n",
    "             'ignore_subgraph_neighbors': [True, False]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# config.optimization_kwargs['clf_only']=False       \n",
    "# watermarking_order = ['augment','use_PCgrad','sacrifice_kwargs','beta_selection','subgraph_method','reg','perc','clf_epochs','frac','coef_wmk','num_subgraphs','balance_beta_weights','ignore_subgraph_neighbors']\n",
    "# print(\"watermarking:\", watermarking_order)\n",
    "# count, [node_classifier, history, subgraph_dict,\\\n",
    "#          all_feature_importances, all_watermark_indices, probas] = dynamic_grid_search(data, dataset_name, debug_multiple_subgraphs, \n",
    "#                                                                                        all_dfs, False, variables, watermarking_order,\n",
    "#                                                                                        count_only=True)\n",
    "# print(count)\n",
    "\n",
    "\n",
    "config.augment_kwargs['separate_trainset_from_subgraphs'] = True\n",
    "config.augment_kwargs['ignore_subgraphs'] = True\n",
    "perturb=False\n",
    "perturb_lr = 1e-3\n",
    "node_classifier, history, subgraph_dict, \\\n",
    "    all_feature_importances, all_watermark_indices, probas = train(data, dataset_name, debug_multiple_subgraphs=False, save=True, print_every=1,perturb=perturb,perturb_lr=perturb_lr)\n",
    "primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "    percent_matches, percent_match_mean, percent_match_std, \\\n",
    "        primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "final_plot(history, '', percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got builtin_function_or_method)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got builtin_function_or_method)"
     ]
    }
   ],
   "source": [
    "data.x[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/training_results/computers/archGCN_elu_nLayers3_hDim256_drop0_skipTrue/_3%UnimportantIndices_average_10ClfEpochs_random_fraction0.01_numSubgraphs10_eps0.1_raw_beta_nodeMixUp40_lr0.002_epochs80_coefWmk200_regressionLambda0.1'\n",
    "probas = pickle.load(open(f'{folder}/probas','rb'))\n",
    "subgraph_dict = pickle.load(open(f'{folder}/subgraph_dict','rb'))\n",
    "all_watermark_indices = pickle.load(open(f'{folder}/all_watermark_indices','rb'))\n",
    "node_classifier = pickle.load(open(f'{folder}/node_classifier','rb'))\n",
    "history = pickle.load(open(f'{folder}/history','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlopt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define your objective function\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nlopt'"
     ]
    }
   ],
   "source": [
    "import nlopt\n",
    "import numpy as np\n",
    "\n",
    "# Define your objective function\n",
    "def objective(x, grad):\n",
    "    if grad.size > 0:\n",
    "        # Compute gradient if needed\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).reshape(original_shape)\n",
    "        x_tensor.requires_grad = True\n",
    "        log_logits_sub = node_classifier(x_tensor, edge_index)\n",
    "        probas_sub = log_logits_sub.clone().exp()\n",
    "\n",
    "        omit_indices, not_omit_indices = get_omit_indices(x_tensor, this_watermark, ignore_zeros_from_subgraphs=False)\n",
    "        raw_beta = solve_regression(x_tensor, probas_sub, regression_kwargs['lambda'])\n",
    "        beta = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta * this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "        balanced_beta_weights = torch.ones_like(B_x_W)\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        loss = torch.mean(torch.clamp(watermark_loss_kwargs['epsilon'] - B_x_W, min=0) * balanced_beta_weights)\n",
    "        loss.backward()\n",
    "\n",
    "        grad[:] = x_tensor.grad.numpy().flatten()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_minimize_lbfgsb options: 2 2\n",
      "_prepare_scalar_function\n",
      "C\n",
      "D\n",
      "F\n",
      "init\n",
      "update_fun: <function ScalarFunction.__init__.<locals>.update_fun at 0x3170d3880>\n",
      "fun_wrapped\n",
      "args: ()\n",
      "FD_METHODS\n",
      "finite_diff_options: {'method': '2-point', 'rel_step': None, 'abs_step': 1e-08, 'bounds': (array([-inf, -inf, -inf, ..., -inf, -inf, -inf]), array([inf, inf, inf, ..., inf, inf, inf]))}\n",
      "Help on function update_grad in module scipy.optimize._differentiable_functions:\n",
      "\n",
      "update_grad()\n",
      "\n",
      "update_grad: None\n",
      "not self.g_updated\n",
      "approx deriv\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m110\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m}\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Use the L-BFGS-B algorithm for optimization\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_watermark_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_sub_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# The optimal x found by the optimizer\u001b[39;00m\n\u001b[1;32m     45\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:308\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 308\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    314\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_optimize.py:390\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 390\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:188\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate_grad:\u001b[39m\u001b[38;5;124m'\u001b[39m,help(update_grad))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad_impl \u001b[38;5;241m=\u001b[39m update_grad\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Hessian Evaluation\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(hess):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:271\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot self.g_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:183\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapprox deriv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    574\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[1;32m    575\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    578\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 456\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:141\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs:\u001b[39m\u001b[38;5;124m'\u001b[39m,args)\n\u001b[0;32m--> 141\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mcompute_watermark_loss\u001b[0;34m(x_sub_flat)\u001b[0m\n\u001b[1;32m     21\u001b[0m log_logits_sub \u001b[38;5;241m=\u001b[39m node_classifier(x_sub, edge_index)\n\u001b[1;32m     22\u001b[0m probas_sub \u001b[38;5;241m=\u001b[39m log_logits_sub\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m---> 25\u001b[0m omit_indices,not_omit_indices \u001b[38;5;241m=\u001b[39m \u001b[43mget_omit_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_watermark\u001b[49m\u001b[43m,\u001b[49m\u001b[43mignore_zeros_from_subgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#indices where watermark is 0\u001b[39;00m\n\u001b[1;32m     26\u001b[0m raw_beta            \u001b[38;5;241m=\u001b[39m solve_regression(x_sub, probas_sub, regression_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m beta                \u001b[38;5;241m=\u001b[39m process_beta(raw_beta, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m], omit_indices, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_beta_method\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/eaaw_graphlime_utils.py:250\u001b[0m, in \u001b[0;36mget_omit_indices\u001b[0;34m(x_sub, watermark, ignore_zeros_from_subgraphs)\u001b[0m\n\u001b[1;32m    248\u001b[0m zero_indices_within_watermark \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(watermark\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    249\u001b[0m omit_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(zero_features_within_subgraph[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m+\u001b[39m zero_indices_within_watermark[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())))\n\u001b[0;32m--> 250\u001b[0m not_omit_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_sub\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43momit_indices\u001b[49m])\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m omit_indices, not_omit_indices\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/_tensor.py:1116\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, element)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1113\u001b[0m     element, (torch\u001b[38;5;241m.\u001b[39mTensor, Number, torch\u001b[38;5;241m.\u001b[39mSymInt, torch\u001b[38;5;241m.\u001b[39mSymFloat, torch\u001b[38;5;241m.\u001b[39mSymBool)\n\u001b[1;32m   1114\u001b[0m ):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1120\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.optimize import minimize\n",
    "results = []\n",
    "for sig in subgraph_dict.keys():\n",
    "    watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "    regression_kwargs = config.regression_kwargs\n",
    "    this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "    x_sub = data_sub.x\n",
    "    edge_index = data_sub.edge_index\n",
    "    original_shape = x_sub.shape\n",
    "    x_sub_flat = x_sub.flatten()\n",
    "    # global i\n",
    "    # i = 0\n",
    "    def compute_watermark_loss(x_sub_flat):\n",
    "        # global i\n",
    "        # i+=1\n",
    "        # print(/i,end='\\r')\n",
    "        # print('hi')\n",
    "        x_sub = torch.tensor(x_sub_flat, dtype=torch.float32).reshape(original_shape)\n",
    "        balanced_beta_weights = torch.ones(x_sub.shape[1])\n",
    "\n",
    "        log_logits_sub = node_classifier(x_sub, edge_index)\n",
    "        probas_sub = log_logits_sub.clone().exp()\n",
    "\n",
    "\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        raw_beta            = solve_regression(x_sub, probas_sub, regression_kwargs['lambda'])\n",
    "        beta                = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta*this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        this_loss_watermark = torch.mean(torch.clamp(watermark_loss_kwargs['epsilon']-B_x_W, min=0)*balanced_beta_weights)\n",
    "        loss_watermark  = this_loss_watermark\n",
    "        # print('ok')\n",
    "        return loss_watermark.item()\n",
    "\n",
    "    # Ensure maxiter is enforced and debugging output\n",
    "    options = {'maxfun': 2, 'iprint':110, 'maxls':2, 'maxiter':2}\n",
    "\n",
    "    # Use the L-BFGS-B algorithm for optimization\n",
    "    result = minimize(compute_watermark_loss, x_sub_flat, \n",
    "                      method='L-BFGS-B', \n",
    "                      options=options)\n",
    "\n",
    "    # The optimal x found by the optimizer\n",
    "    results.append(result)\n",
    "    # print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scipy.optimize' from '/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/__init__.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(torch.wherae(torch.eq(data_copy.x, data.x)==False)[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[24,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.8543e-05,  0.0000e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.x[[24,25],[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([1,12])\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
