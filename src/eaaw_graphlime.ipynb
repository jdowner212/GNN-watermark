{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarFunction\n"
     ]
    }
   ],
   "source": [
    "from eaaw_graphlime_utils import *\n",
    "import gc\n",
    "\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "\n",
    "\n",
    "from   pcgrad.pcgrad import PCGrad # from the following: https://github.com/WeiChengTseng/Pytorch-PCGrad. Renamed to 'pcgrad' and moved to site-packages folder.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transorms used when loading computers: ['CreateMaskTransform()']\n",
      "train_mask: 12376\n",
      "test_mask: 689\n",
      "val_mask: 687\n"
     ]
    }
   ],
   "source": [
    "dataset_name='computers'\n",
    "if dataset_attributes[dataset_name]['single_or_multi_graph']=='single':\n",
    "    dataset = prep_data(dataset_name=dataset_name, location='default', batch_size='default', transform_list='default',\n",
    "                        train_val_test_split=[0.9,0.05,0.05])\n",
    "    graph_to_watermark = data = dataset[0]\n",
    "elif dataset_attributes[dataset_name]['single_or_multi_graph']=='multi':\n",
    "    [train_dataset, val_dataset, test_dataset], [train_loader, val_loader, test_loader] = prep_data(dataset_name=dataset_name, location='default', \n",
    "                                                                                                    batch_size='default', transform_list='default',\n",
    "                                                                                                                            train_val_test_split=[0.9,0.05,0.05])\n",
    "    graph_to_watermark = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization_kwargs, node_classifier_kwargs, watermark_kwargs, subgraph_kwargs, augment_kwargs, watermark_loss_kwargs, regression_kwargs = get_presets(dataset, 'default')\n",
    "get_presets(dataset,dataset_name)\n",
    "\n",
    "compare_unimportant_against_random=False\n",
    "\n",
    "config.node_classifier_kwargs['dropout']=0.1\n",
    "config.node_classifier_kwargs['dropout_subgraphs']=0\n",
    "config.watermark_kwargs['fancy_selection_kwargs']['clf_only_epochs'] = 20\n",
    "config.optimization_kwargs['lr']=0.001\n",
    "config.optimization_kwargs['epochs']=50\n",
    "# config.optimization_kwargs['coefWmk']=300\n",
    "config.optimization_kwargs['perturb_x']=False\n",
    "config.optimization_kwargs['perturb_lr']=1e5\n",
    "\n",
    "config.optimization_kwargs['coefWmk_kwargs']= {\n",
    "                                                'coefWmk':50,\n",
    "                                                'schedule_coef_wmk': False,\n",
    "                                                'min_coefWmk_scheduled': 100,\n",
    "                                                'reach_max_coef_wmk_by_epoch':70,\n",
    "                                                }\n",
    "\n",
    "config.optimization_kwargs['use_pcgrad']=False\n",
    "config.optimization_kwargs['use_sam']=False\n",
    "config.optimization_kwargs['sam_momentum']=0.5\n",
    "config.optimization_kwargs['sam_rho']=1e-5\n",
    "\n",
    "\n",
    "config.optimization_kwargs['use_gradnorm']=False\n",
    "\n",
    "\n",
    "config.augment_kwargs['nodeDrop']['use']=False\n",
    "config.augment_kwargs['nodeMixUp']['use']=True\n",
    "config.augment_kwargs['nodeMixUp']['lambda']=5\n",
    "config.augment_kwargs['nodeFeatMask']['use']=False\n",
    "config.augment_kwargs['edgeDrop']['use']=False\n",
    "config.augment_kwargs['separate_trainset_from_subgraphs'] = True\n",
    "config.augment_kwargs['ignore_subgraphs'] = True\n",
    "config.watermark_kwargs['fancy_selection_kwargs']['percent_of_features_to_watermark']=3\n",
    "config.watermark_kwargs['watermark_type']='fancy'\n",
    "config.subgraph_kwargs['numSubgraphs']=7\n",
    "config.subgraph_kwargs['fraction']=0.03\n",
    "config.subgraph_kwargs['method']='random'\n",
    "config.optimization_kwargs['sacrifice_kwargs']['method']='subgraph_node_indices'\n",
    "config.optimization_kwargs['sacrifice_kwargs']['percentage']=1\n",
    "config.optimization_kwargs['separate_forward_passes_per_subgraph']=False\n",
    "config.optimization_kwargs['clf_only']=False\n",
    "config.watermark_loss_kwargs['epsilon']=1e-1\n",
    "\n",
    "config.regression_kwargs['lambda']=1e-3\n",
    "\n",
    "validate_kwargs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = merge_kwargs_dicts()#config.node_classifier_kwargs, config.optimization_kwargs, config.watermark_kwargs, config.subgraph_kwargs, config.regression_kwargs, config.watermark_loss_kwargs, config.augment_kwargs)\n",
    "merged_dict_keys = list(merged_dict.keys()) + ['Train Acc','Val Acc','Match Rates','Final Betas','watermark']\n",
    "all_dfs = pd.DataFrame(dict(zip(merged_dict_keys,[]*len(merged_dict_keys))))\n",
    "\n",
    "# all_dfs = pickle.load(open('/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/src/results_df.pkl','rb'))\n",
    "# all_results = {}\n",
    "\n",
    "\n",
    "\n",
    "scale_beta_method=None\n",
    "debug_multiple_subgraphs=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer_test = Trainer(data, dataset_name)\n",
    "# Trainer_test.x, Trainer_test.edge_index = data.x, data.edge_index\n",
    "# Trainer_test.test_perturb_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda: 5\n",
      "setup_subgraph_dict\n",
      "nodeIndices: [3941, 10762, 7971, 10573, 12294, 9009, 2051, 7184, 12412, 3009, 8366, 11870, 8078, 4310, 1436, 5810, 5176, 63, 5480, 3341, 3391, 3471, 10484, 13199, 271, 8673, 4218, 7932, 4076, 77, 3494, 12140, 12368, 8868, 7950, 828, 10143, 3512, 4907, 6076, 11597, 7539, 3852, 10297, 2495, 2228, 1059, 5460, 10440, 6053, 13425, 5291, 5392]\n",
      "nodeIndices: [10420, 6468, 7324, 12547, 2648, 2352, 6451, 8998, 12179, 4946, 10235, 9702, 7397, 7543, 13192, 6969, 13118, 2503, 7228, 4760, 4088, 11662, 2083, 4610, 9324, 10388, 6961, 11461, 2921, 7085, 4745, 12816, 8528, 13447, 9648, 7067, 8270, 10815, 12520, 9413, 4964, 2868, 2400, 11572, 204, 12844, 9258, 4512, 12080, 2056, 404, 10371, 10955]\n",
      "nodeIndices: [3846, 12387, 10313, 6170, 9120, 11052, 11328, 3261, 694, 2515, 8044, 9323, 4843, 3838, 5289, 8344, 7747, 6051, 10242, 197, 8988, 359, 13407, 11363, 7318, 9849, 638, 802, 3091, 9023, 11745, 5595, 9287, 1080, 9358, 2420, 11009, 1284, 3787, 12090, 2164, 3720, 6824, 1604, 6672, 13673, 2697, 2029, 6411, 1872, 8306, 2163, 6644]\n",
      "nodeIndices: [7329, 12775, 129, 11529, 10597, 9307, 11585, 5024, 3866, 10031, 8417, 9357, 5995, 6826, 5696, 6308, 6788, 10612, 1909, 3178, 5794, 13624, 1566, 10750, 363, 6719, 11499, 2188, 9263, 10456, 8321, 502, 2413, 2336, 408, 3221, 5283, 5075, 11442, 5764, 3346, 4061, 2657, 13327, 5033, 11177, 13707, 11600, 11919, 11379, 7858, 11824, 3633]\n",
      "nodeIndices: [1302, 8370, 2546, 3219, 8587, 1722, 1577, 9244, 12516, 6485, 513, 7700, 523, 3135, 4080, 4130, 751, 9969, 1670, 6310, 11549, 5793, 8678, 11151, 13476, 13296, 1464, 13532, 10684, 3635, 1188, 10302, 1401, 1600, 11316, 8039, 13256, 10378, 6994, 1549, 1773, 13477, 96, 4481, 10847, 9214, 2072, 5629, 703, 4178, 11992, 3856, 539]\n",
      "nodeIndices: [13409, 3491, 554, 2719, 7409, 4144, 3938, 7308, 10174, 7065, 7216, 13424, 7117, 4580, 12728, 8032, 6812, 4143, 11927, 1075, 3973, 6027, 7671, 516, 659, 11165, 4408, 56, 6100, 925, 10888, 12458, 6284, 12306, 9500, 6548, 4060, 6266, 11105, 2233, 10565, 1370, 3399, 8369, 10055, 9577, 6619, 12142, 11606, 2623, 11385, 1940, 6088]\n",
      "nodeIndices: [10861, 9478, 8240, 12350, 6959, 6518, 3493, 4806, 3310, 13454, 3611, 12525, 5774, 9963, 9160, 13536, 12609, 43, 12146, 6622, 5110, 4191, 6063, 3036, 1331, 7486, 4265, 7464, 4884, 10449, 11396, 10452, 13600, 2826, 279, 6246, 10179, 2470, 12289, 7056, 5859, 11567, 12846, 10187, 13321, 11331, 3760, 2841, 8074, 8312, 3615, 1434, 10863]\n",
      "Sacrificing 100% of subgraph nodes from node classification training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb64461f5a2a4ccfb231f4b4f27a8402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0, loss_primary = 2.503, loss_watermark = n/a, B*W = n/a, train acc = 0.047, val acc = 0.036\n",
      "Epoch:   1, loss_primary = 2.059, loss_watermark = n/a, B*W = n/a, train acc = 0.342, val acc = 0.348\n",
      "Epoch:   2, loss_primary = 1.809, loss_watermark = n/a, B*W = n/a, train acc = 0.538, val acc = 0.568\n",
      "Epoch:   3, loss_primary = 1.565, loss_watermark = n/a, B*W = n/a, train acc = 0.614, val acc = 0.624\n",
      "Epoch:   4, loss_primary = 1.350, loss_watermark = n/a, B*W = n/a, train acc = 0.676, val acc = 0.687\n",
      "Epoch:   5, loss_primary = 1.196, loss_watermark = n/a, B*W = n/a, train acc = 0.698, val acc = 0.741\n",
      "Epoch:   6, loss_primary = 1.052, loss_watermark = n/a, B*W = n/a, train acc = 0.728, val acc = 0.748\n",
      "Epoch:   7, loss_primary = 0.942, loss_watermark = n/a, B*W = n/a, train acc = 0.747, val acc = 0.764\n",
      "Epoch:   8, loss_primary = 0.870, loss_watermark = n/a, B*W = n/a, train acc = 0.763, val acc = 0.767\n",
      "Epoch:   9, loss_primary = 0.800, loss_watermark = n/a, B*W = n/a, train acc = 0.784, val acc = 0.780\n",
      "Epoch:  10, loss_primary = 0.761, loss_watermark = n/a, B*W = n/a, train acc = 0.790, val acc = 0.798\n",
      "Epoch:  11, loss_primary = 0.721, loss_watermark = n/a, B*W = n/a, train acc = 0.800, val acc = 0.821\n",
      "Epoch:  12, loss_primary = 0.679, loss_watermark = n/a, B*W = n/a, train acc = 0.805, val acc = 0.821\n",
      "Epoch:  13, loss_primary = 0.647, loss_watermark = n/a, B*W = n/a, train acc = 0.819, val acc = 0.825\n",
      "Epoch:  14, loss_primary = 0.617, loss_watermark = n/a, B*W = n/a, train acc = 0.826, val acc = 0.831\n",
      "Epoch:  15, loss_primary = 0.589, loss_watermark = n/a, B*W = n/a, train acc = 0.833, val acc = 0.841\n",
      "Epoch:  16, loss_primary = 0.570, loss_watermark = n/a, B*W = n/a, train acc = 0.836, val acc = 0.836\n",
      "Epoch:  17, loss_primary = 0.549, loss_watermark = n/a, B*W = n/a, train acc = 0.843, val acc = 0.852\n",
      "Epoch:  18, loss_primary = 0.529, loss_watermark = n/a, B*W = n/a, train acc = 0.852, val acc = 0.859\n",
      "Epoch:  19, loss_primary = 0.528, loss_watermark = n/a, B*W = n/a, train acc = 0.853, val acc = 0.852\n",
      "apply_fancy_watermark\n",
      "importance: tensor(1.0463e-05)\n",
      "importance: tensor(1.7575e-05)\n",
      "importance: tensor(7.5751e-05)\n",
      "importance: tensor(0.0001)\n",
      "importance: tensor(0.0001)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0002)\n",
      "importance: tensor(0.0003)\n",
      "importance: tensor(0.0003)\n",
      "importance: tensor(0.0004)\n",
      "importance: tensor(0.0005)\n",
      "importance: tensor(0.0006)\n",
      "importance: tensor(0.0006)\n",
      "importance: tensor(0.0006)\n",
      "importance: tensor(0.0006)\n",
      "importance: tensor(0.0007)\n",
      "importance: tensor(0.0007)\n",
      "importance: tensor(0.0007)\n",
      "importance: tensor(0.0008)\n",
      "importance: tensor(0.0008)\n",
      "importance: tensor(0.0008)\n",
      "Using averaged betas from subgraphs to identify bottom 3% of important feature indices for watermarking, uniformly across subgraphs\n",
      "Epoch:  20, loss_primary = 0.513, loss_watermark = 5.098, B*W = 0.00007, train acc = 0.856, val acc = 0.849\n",
      "Epoch:  21, loss_primary = 0.563, loss_watermark = 4.489, B*W = 0.01350, train acc = 0.842, val acc = 0.857\n",
      "Epoch:  22, loss_primary = 0.684, loss_watermark = 3.986, B*W = 0.02368, train acc = 0.821, val acc = 0.814\n",
      "Epoch:  23, loss_primary = 0.724, loss_watermark = 3.513, B*W = 0.03313, train acc = 0.806, val acc = 0.803\n",
      "Epoch:  24, loss_primary = 0.636, loss_watermark = 3.126, B*W = 0.04260, train acc = 0.808, val acc = 0.822\n",
      "Epoch:  25, loss_primary = 0.580, loss_watermark = 2.919, B*W = 0.04766, train acc = 0.822, val acc = 0.825\n",
      "Epoch:  26, loss_primary = 0.565, loss_watermark = 2.662, B*W = 0.05194, train acc = 0.836, val acc = 0.856\n",
      "Epoch:  27, loss_primary = 0.535, loss_watermark = 2.339, B*W = 0.06093, train acc = 0.841, val acc = 0.849\n",
      "Epoch:  28, loss_primary = 0.510, loss_watermark = 2.222, B*W = 0.06477, train acc = 0.848, val acc = 0.869\n",
      "Epoch:  29, loss_primary = 0.519, loss_watermark = 2.037, B*W = 0.06712, train acc = 0.847, val acc = 0.850\n",
      "Epoch:  30, loss_primary = 0.508, loss_watermark = 1.935, B*W = 0.06942, train acc = 0.848, val acc = 0.857\n",
      "Epoch:  31, loss_primary = 0.496, loss_watermark = 1.769, B*W = 0.07528, train acc = 0.853, val acc = 0.854\n",
      "Epoch:  32, loss_primary = 0.519, loss_watermark = 1.734, B*W = 0.07565, train acc = 0.847, val acc = 0.856\n",
      "Epoch:  33, loss_primary = 0.501, loss_watermark = 1.823, B*W = 0.07466, train acc = 0.858, val acc = 0.869\n",
      "Epoch:  34, loss_primary = 0.505, loss_watermark = 1.515, B*W = 0.08020, train acc = 0.853, val acc = 0.865\n",
      "Epoch:  35, loss_primary = 0.517, loss_watermark = 1.521, B*W = 0.08112, train acc = 0.852, val acc = 0.854\n",
      "Epoch:  36, loss_primary = 0.503, loss_watermark = 1.493, B*W = 0.08466, train acc = 0.850, val acc = 0.852\n",
      "Epoch:  37, loss_primary = 0.495, loss_watermark = 1.248, B*W = 0.08960, train acc = 0.857, val acc = 0.865\n",
      "Epoch:  38, loss_primary = 0.510, loss_watermark = 1.361, B*W = 0.08508, train acc = 0.853, val acc = 0.852\n",
      "Epoch:  39, loss_primary = 0.492, loss_watermark = 1.224, B*W = 0.08816, train acc = 0.859, val acc = 0.863\n",
      "Epoch:  40, loss_primary = 0.485, loss_watermark = 1.174, B*W = 0.09148, train acc = 0.859, val acc = 0.868\n",
      "Epoch:  41, loss_primary = 0.494, loss_watermark = 1.080, B*W = 0.09402, train acc = 0.852, val acc = 0.859\n",
      "Epoch:  42, loss_primary = 0.489, loss_watermark = 0.974, B*W = 0.09524, train acc = 0.861, val acc = 0.856\n",
      "Epoch:  43, loss_primary = 0.480, loss_watermark = 0.985, B*W = 0.09406, train acc = 0.861, val acc = 0.862\n",
      "Epoch:  44, loss_primary = 0.493, loss_watermark = 0.890, B*W = 0.09725, train acc = 0.856, val acc = 0.866\n",
      "Epoch:  45, loss_primary = 0.487, loss_watermark = 0.913, B*W = 0.09615, train acc = 0.854, val acc = 0.863\n",
      "Epoch:  46, loss_primary = 0.491, loss_watermark = 0.810, B*W = 0.09921, train acc = 0.857, val acc = 0.853\n",
      "Epoch:  47, loss_primary = 0.470, loss_watermark = 0.781, B*W = 0.09855, train acc = 0.860, val acc = 0.863\n",
      "Epoch:  48, loss_primary = 0.478, loss_watermark = 0.749, B*W = 0.09937, train acc = 0.858, val acc = 0.856\n",
      "Epoch:  49, loss_primary = 0.472, loss_watermark = 0.691, B*W = 0.10183, train acc = 0.862, val acc = 0.863\n",
      "Node classifier, history, subgraph dict, feature importances, watermark indices, and probas saved in:\n",
      "/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/training_results/computers/archGCN_elu_nLayers3_hDim256_drop0.1_skipTrue/_3%UnimportantIndices_average_20ClfEpochs_random_fraction0.03_numSubgraphs7_eps0.1_raw_beta_nodeMixUp5_lr0.001_epochs50_coefWmk50_sacrifice1subNodes_regressionLambda0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for _ in range(3):\n",
    "# for mom in [1e-3, 1e-2, 1e-1, 0.5]:\n",
    "#     for rho in [1e-7,1e-5,1e-3]:\n",
    "# config.optimization_kwargs['sam_momentum'] = mom\n",
    "# config.optimization_kwargs['sam_rho'] = rho\n",
    "# for i in range(10):\n",
    "#     if i<5:\n",
    "#         config.optimization_kwargs['use_sam']=True\n",
    "#         title = f'With SAM - momentum={np.round(config.optimization_kwargs['sam_momentum'],3)}, rho={config.optimization_kwargs['sam_rho']}'\n",
    "#     else:\n",
    "# config.optimization_kwargs['use_sam']=False\n",
    "for _ in range(1):\n",
    "    config.optimization_kwargs['perturb_x']=False\n",
    "    config.optimization_kwargs['separate_forward_passes_per_subgraph']=True\n",
    "    title = f'Separate'\n",
    "    Trainer_ = Trainer(data, dataset_name)\n",
    "    node_classifier, history, subgraph_dict, \\\n",
    "    all_feature_importances, all_watermark_indices, probas = Trainer_.train(debug_multiple_subgraphs=False, save=True, print_every=1)\n",
    "\n",
    "    primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "    percent_matches, percent_match_mean, percent_match_std, \\\n",
    "        primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "    final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "# for _ in range(1):\n",
    "#     config.optimization_kwargs['perturb_x']=False\n",
    "#     config.optimization_kwargs['separate_forward_passes_per_subgraph']=False\n",
    "#     title = f'Not Separate'\n",
    "#     Trainer_ = Trainer(data, dataset_name)\n",
    "#     node_classifier, history, subgraph_dict, \\\n",
    "#     all_feature_importances, all_watermark_indices, probas = Trainer_.train(debug_multiple_subgraphs=False, save=True, print_every=1)\n",
    "\n",
    "#     primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "#     percent_matches, percent_match_mean, percent_match_std, \\\n",
    "#         primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "#     final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "# for _ in range(1):\n",
    "#     config.optimization_kwargs['perturb_x']=False\n",
    "#     title = f'Vanilla'\n",
    "#     Trainer_ = Trainer(data, dataset_name)\n",
    "#     node_classifier, history, subgraph_dict, \\\n",
    "#     all_feature_importances, all_watermark_indices, probas = Trainer_.train(debug_multiple_subgraphs=False, save=True, print_every=1)\n",
    "\n",
    "#     primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "#     percent_matches, percent_match_mean, percent_match_std, \\\n",
    "#         primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "#     final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "# for _ in range(1):\n",
    "#     config.optimization_kwargs['separate_forward_passes_per_subgraph']=True\n",
    "#     title = f'Separate Forward Passes for D_trn and D_wmk'\n",
    "#     Trainer_ = Trainer(data, dataset_name)\n",
    "#     node_classifier, history, subgraph_dict, \\\n",
    "#     all_feature_importances, all_watermark_indices, probas = Trainer_.train(debug_multiple_subgraphs=False, save=True, print_every=1)\n",
    "\n",
    "#     primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "#     percent_matches, percent_match_mean, percent_match_std, \\\n",
    "#         primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "#     final_plot(history, title, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8205, dtype=torch.float64) tensor(0.8311, dtype=torch.float64)\n",
      "tensor(0.8686, dtype=torch.float64) tensor(0.8763, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "Trainer_.node_classifier.eval()\n",
    "Trainer_.optimizer.zero_grad()\n",
    "log_logits          = Trainer_.forward(data.x, data.edge_index, dropout=config.node_classifier_kwargs['dropout'])\n",
    "acc_trn = accuracy(log_logits[Trainer_.train_mask], data.y[Trainer_.train_mask],verbose=False)\n",
    "acc_val = accuracy(log_logits[Trainer_.val_mask],   data.y[Trainer_.val_mask],verbose=False)\n",
    "print(acc_trn, acc_val)\n",
    "Trainer_.node_classifier.eval()\n",
    "Trainer_.optimizer.zero_grad()\n",
    "log_logits          = Trainer_.forward(Trainer_.x, Trainer_.edge_index, dropout=config.node_classifier_kwargs['dropout'])\n",
    "acc_trn = accuracy(log_logits[Trainer_.train_mask], Trainer_.y[Trainer_.train_mask],verbose=False)\n",
    "acc_val = accuracy(log_logits[Trainer_.val_mask],   Trainer_.y[Trainer_.val_mask],verbose=False)\n",
    "print(acc_trn, acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., -4.,  0.,  ...,  0., -4., -4.],\n",
       "        [ 5.,  5.,  0.,  ...,  0.,  5.,  0.],\n",
       "        [ 0.,  0., -4.,  ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 1., -4.,  0.,  ..., -4., -4., -4.],\n",
       "        [ 5.,  0., -4.,  ...,  0., -4.,  0.],\n",
       "        [ 5.,  1.,  0.,  ...,  5.,  1., -4.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = list(Trainer_.subgraph_dict.keys())[0]\n",
    "node_indices = Trainer_.subgraph_dict[sig]['nodeIndices']\n",
    "Trainer_.x#[node_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5135, -2.0671, -2.5162,  ..., -2.5989, -2.0855, -2.4737],\n",
       "        [-2.9204, -2.0398, -3.1309,  ..., -3.5097, -1.9472, -3.0037],\n",
       "        [-2.6028, -2.2278, -2.8592,  ..., -2.9719, -1.3860, -2.3866],\n",
       "        ...,\n",
       "        [-2.9182, -2.6029, -2.9013,  ..., -3.2258, -1.1315, -2.7037],\n",
       "        [-3.1914, -1.7261, -3.4235,  ..., -3.5965, -1.5664, -3.1513],\n",
       "        [-1.9254, -2.5194, -2.0963,  ..., -2.5219, -1.8731, -2.4154]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_.forward(data.x, data.edge_index, dropout=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_subgraph(self, p_to_swap, subgraph_node_indices):\n",
    "    num_to_swap = int(p_to_swap*len(subgraph_node_indices))\n",
    "    random_indices = torch.randperm(len(subgraph_node_indices))\n",
    "    subgraph_node_indices = subgraph_node_indices[random_indices[:len(subgraph_node_indices)-num_to_swap]]\n",
    "    filtered_tensor = Trainer_.train_nodes_to_consider[~Trainer_.train_nodes_to_consider.unsqueeze(1).eq(subgraph_node_indices).any(dim=1)]\n",
    "    random_index = torch.randint(0, filtered_tensor.size(0), (num_to_swap,))\n",
    "    random_element = filtered_tensor[random_index]\n",
    "    subgraph_node_indices = torch.concatenate([subgraph_node_indices, random_element])\n",
    "\n",
    "    sub_edge_index, _ = subgraph(subgraph_node_indices, self.data.edge_index, relabel_nodes=True, num_nodes=self.data.num_nodes)\n",
    "    shifted_subgraph = Data(\n",
    "        x          = self.data.x[subgraph_node_indices]          if self.data.x is not None else None,\n",
    "        edge_index = sub_edge_index,\n",
    "        y          = self.data.y[subgraph_node_indices]          if self.data.y is not None else None,\n",
    "        train_mask = self.data.train_mask[subgraph_node_indices] if self.data.train_mask is not None else None,\n",
    "        test_mask  = self.data.test_mask[subgraph_node_indices]  if self.data.test_mask is not None else None,\n",
    "        val_mask   = self.data.val_mask[subgraph_node_indices]   if self.data.val_mask is not None else None)\n",
    "    return shifted_subgraph\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100.0, 100.0, 100.0, 95.65217391304348, 100.0, 100.0, 100.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_.percent_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full prediction accuracies:\n",
      "tensor(0.8679, dtype=torch.float64)\n",
      "tensor(0.8679, dtype=torch.float64)\n",
      "tensor(0.8113, dtype=torch.float64)\n",
      "tensor(0.8113, dtype=torch.float64)\n",
      "tensor(0.8113, dtype=torch.float64)\n",
      "tensor(0.9434, dtype=torch.float64)\n",
      "tensor(0.8679, dtype=torch.float64)\n",
      "Match rates when replacing 0% watermark subgraph indices:\n",
      "['100.0%', '100.0%', '100.0%', '95.652%', '100.0%', '100.0%', '100.0%']\n",
      "match counts across subgraphs: 22\n",
      "Match rates when replacing 5% watermark subgraph indices:\n",
      "['100.0%', '95.652%', '95.652%', '91.304%', '95.652%', '86.957%', '100.0%']\n",
      "match counts across subgraphs: 16\n",
      "Match rates when replacing 10% watermark subgraph indices:\n",
      "['91.304%', '78.261%', '82.609%', '86.957%', '100.0%', '86.957%', '86.957%']\n",
      "match counts across subgraphs: 8\n",
      "Match rates when replacing 15% watermark subgraph indices:\n",
      "['82.609%', '82.609%', '91.304%', '86.957%', '95.652%', '86.957%', '78.261%']\n",
      "match counts across subgraphs: 8\n",
      "Match rates when replacing 20% watermark subgraph indices:\n",
      "['82.609%', '73.913%', '95.652%', '69.565%', '82.609%', '86.957%', '82.609%']\n",
      "match counts across subgraphs: 5\n",
      "Match rates when replacing 25% watermark subgraph indices:\n",
      "['82.609%', '73.913%', '78.261%', '91.304%', '86.957%', '86.957%', '69.565%']\n",
      "match counts across subgraphs: 5\n",
      "Match rates when replacing 30% watermark subgraph indices:\n",
      "['82.609%', '60.87%', '65.217%', '56.522%', '91.304%', '69.565%', '78.261%']\n",
      "match counts across subgraphs: 2\n",
      "Match rates when replacing 35% watermark subgraph indices:\n",
      "['65.217%', '56.522%', '60.87%', '69.565%', '65.217%', '65.217%', '60.87%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 40% watermark subgraph indices:\n",
      "['69.565%', '56.522%', '78.261%', '69.565%', '60.87%', '69.565%', '69.565%']\n",
      "match counts across subgraphs: 1\n",
      "Match rates when replacing 45% watermark subgraph indices:\n",
      "['60.87%', '43.478%', '65.217%', '60.87%', '69.565%', '73.913%', '60.87%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 50% watermark subgraph indices:\n",
      "['56.522%', '65.217%', '43.478%', '52.174%', '56.522%', '69.565%', '39.13%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 55% watermark subgraph indices:\n",
      "['47.826%', '56.522%', '39.13%', '47.826%', '52.174%', '34.783%', '52.174%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 60% watermark subgraph indices:\n",
      "['43.478%', '60.87%', '43.478%', '69.565%', '60.87%', '39.13%', '60.87%']\n",
      "match counts across subgraphs: 1\n",
      "Match rates when replacing 65% watermark subgraph indices:\n",
      "['56.522%', '56.522%', '56.522%', '34.783%', '65.217%', '52.174%', '47.826%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 70% watermark subgraph indices:\n",
      "['43.478%', '39.13%', '65.217%', '39.13%', '60.87%', '56.522%', '52.174%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 75% watermark subgraph indices:\n",
      "['56.522%', '60.87%', '47.826%', '39.13%', '56.522%', '60.87%', '43.478%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 80% watermark subgraph indices:\n",
      "['56.522%', '30.435%', '56.522%', '34.783%', '39.13%', '52.174%', '39.13%']\n",
      "match counts across subgraphs: 1\n",
      "Match rates when replacing 85% watermark subgraph indices:\n",
      "['65.217%', '39.13%', '39.13%', '43.478%', '60.87%', '78.261%', '52.174%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 90% watermark subgraph indices:\n",
      "['65.217%', '56.522%', '60.87%', '34.783%', '65.217%', '52.174%', '47.826%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 95% watermark subgraph indices:\n",
      "['60.87%', '34.783%', '60.87%', '56.522%', '65.217%', '43.478%', '69.565%']\n",
      "match counts across subgraphs: 0\n",
      "Match rates when replacing 100% watermark subgraph indices:\n",
      "['52.174%', '60.87%', '56.522%', '39.13%', '52.174%', '56.522%', '60.87%']\n",
      "match counts across subgraphs: 1\n"
     ]
    }
   ],
   "source": [
    "Trainer_.node_classifier.eval()\n",
    "Trainer_.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "log_logits   = Trainer_.forward(data.x, data.edge_index, dropout=0)\n",
    "print('Full prediction accuracies:')\n",
    "for i, sig in enumerate(Trainer_.subgraph_dict.keys()):\n",
    "    node_indices = Trainer_.subgraph_dict[sig]['nodeIndices']\n",
    "    these_out = log_logits[node_indices]\n",
    "    these_y = data.y[node_indices]\n",
    "    acc = accuracy(these_out, these_y, verbose=False)\n",
    "    print(acc)\n",
    "for p_to_swap in [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]:\n",
    "    matches = []\n",
    "    sign_betas = []\n",
    "    for i, sig in enumerate(Trainer_.subgraph_dict.keys()):\n",
    "        node_indices = Trainer_.subgraph_dict[sig]['nodeIndices']\n",
    "        num_to_swap = int(p_to_swap*len(node_indices))\n",
    "\n",
    "        # print('node indices start:', node_indices)\n",
    "        random_indices = torch.randperm(len(node_indices))\n",
    "        node_indices = node_indices[random_indices[:len(node_indices)-num_to_swap]]\n",
    "        filtered_tensor = Trainer_.train_nodes_to_consider[~Trainer_.train_nodes_to_consider.unsqueeze(1).eq(node_indices).any(dim=1)]\n",
    "        random_index = torch.randint(0, filtered_tensor.size(0), (num_to_swap,))\n",
    "        random_element = filtered_tensor[random_index]\n",
    "        node_indices = torch.concatenate([node_indices, random_element])\n",
    "        # print('node indices end  :', node_indices)\n",
    "\n",
    "        sub_edge_index, _ = subgraph(node_indices, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "        subgraph_ = Data(\n",
    "            x=data.x[node_indices] if data.x is not None else None,\n",
    "            edge_index=sub_edge_index,\n",
    "            y=data.y[node_indices] if data.y is not None else None,\n",
    "            train_mask=data.train_mask[node_indices] if data.train_mask is not None else None,\n",
    "            test_mask=data.test_mask[node_indices] if data.test_mask is not None else None,\n",
    "            val_mask=data.val_mask[node_indices] if data.val_mask is not None else None)\n",
    "    \n",
    "\n",
    "        x_sub, edge_index_sub = subgraph_.x, subgraph_.edge_index\n",
    "        these_log_logits   = Trainer_.forward(x_sub, edge_index_sub, dropout=config.node_classifier_kwargs['dropout'])\n",
    "        these_probas = these_log_logits.clone().exp()\n",
    "        y_sub = these_probas\n",
    "\n",
    "        watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "        regression_kwargs = config.regression_kwargs\n",
    "        this_watermark = subgraph_dict[sig]['watermark']#[subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "\n",
    "\n",
    "        ''' epoch condtion: epoch==epoch-1'''\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        this_raw_beta = solve_regression(x_sub, y_sub, regression_kwargs['lambda'])\n",
    "        beta  = process_beta(this_raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        these_watermark_indices = Trainer_.all_watermark_indices[i]\n",
    "        #print('watermark:',this_watermark[these_watermark_indices])\n",
    "        #print('beta:     ',torch.sign(beta[these_watermark_indices]))\n",
    "        match = torch.sum(this_watermark[these_watermark_indices]==torch.sign(beta[these_watermark_indices]))/len(these_watermark_indices)\n",
    "        sign_betas.append(torch.sign(beta[these_watermark_indices]))\n",
    "        match_str = str(np.round(100*match.item(),3)) + '%'\n",
    "        matches.append(match_str)\n",
    "    print(f'Match rates when replacing {int(100*p_to_swap)}% watermark subgraph indices:')\n",
    "    print(matches)\n",
    "    bs = torch.vstack(sign_betas)\n",
    "    match_counts= count_matches(bs)\n",
    "    print('match counts across subgraphs:',match_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(these_watermark_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = Trainer_.train_nodes_to_consider\n",
    "\n",
    "# Separate list of elements to exclude\n",
    "exclude_tensor = node_indices\n",
    "\n",
    "# Convert exclude_list to a tensor for comparison\n",
    "# exclude_tensor = torch.tensor(exclude_list)\n",
    "\n",
    "# Filter the original tensor to exclude elements in the exclude list\n",
    "filtered_tensor = tensor[~tensor.unsqueeze(1).eq(exclude_tensor).any(dim=1)]\n",
    "random_index = torch.randint(0, filtered_tensor.size(0), (5,))\n",
    "random_element = filtered_tensor[random_index]\n",
    "node_indices = torch.concatenate([node_indices, random_element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
       "        [ 0., 10.,  0.,  ..., 10.,  0., 10.],\n",
       "        ...,\n",
       "        [10.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0., -9.,  0.,  ...,  0., -9.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trainer_.x[torch.tensor(list(set(np.concatenate(Trainer_.all_watermark_indices))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['arch', 'activation', 'nLayers', 'hDim', 'dropout', 'dropout_subgraphs', 'skip_connections', 'heads_1', 'heads_2', 'inDim', 'outDim'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.node_classifier_kwargs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m perturb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     59\u001b[0m perturb_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m     60\u001b[0m node_classifier, history, subgraph_dict, \\\n\u001b[0;32m---> 61\u001b[0m     all_feature_importances, all_watermark_indices, probas \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m(data, dataset_name, debug_multiple_subgraphs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, print_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,perturb\u001b[38;5;241m=\u001b[39mperturb,perturb_lr\u001b[38;5;241m=\u001b[39mperturb_lr)\n\u001b[1;32m     62\u001b[0m primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n\u001b[1;32m     63\u001b[0m     percent_matches, percent_match_mean, percent_match_std, \\\n\u001b[1;32m     64\u001b[0m         primary_acc_curve, watermark_acc_curve, train_acc, val_acc \u001b[38;5;241m=\u001b[39m get_performance_trends(history, subgraph_dict)\n\u001b[1;32m     65\u001b[0m final_plot(history, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "''' (individualize boolean, multisubgraph method, index selection method) '''\n",
    "selection_kwargss = [#(False, None, 'random'), # random, not individualized\n",
    "                    (False,'average','unimportant'), # average, not individualized\n",
    "                    #(False,'concat','unimportant'), # concat, not inividualized\n",
    "                    # (True,None,'unimportant') # unimportant, individualized\n",
    "                    ]\n",
    "\n",
    "''' (method, regenerate_boolean) '''\n",
    "subgraph_methods = [('random',False)]\n",
    "\n",
    "''' [subset, percentage] '''\n",
    "sacrifice_kwargss = [['train_node_indices',1],['train_node_indices',0.75],['subgraph_node_indices',1],[None,None]]\n",
    "\n",
    "''' (method, L2_lambda (if applicable, else None) '''\n",
    "regularization_kwargs = [(None,None),('L2',0.01),('beta_var',None)]\n",
    "\n",
    "\n",
    "variables = {'augment': [{'separate_trainset_from_subgraphs': True,\n",
    "                          'ignore_subgraphs': True,\n",
    "                          'nodeDrop': False,\n",
    "                          'nodeMixUp': False,\n",
    "                          'nodeFeatMask': False,\n",
    "                          'edgeDrop': False},\n",
    "                          {'separate_trainset_from_subgraphs': True,\n",
    "                          'ignore_subgraphs': True,\n",
    "                          'nodeDrop': True,\n",
    "                          'nodeMixUp': True,\n",
    "                          'nodeFeatMask': True,\n",
    "                          'edgeDrop': True},],\n",
    "             'sacrifice_kwargs':  sacrifice_kwargss,\n",
    "             'beta_selection': selection_kwargss,\n",
    "             'use_PCgrad': [True,False],\n",
    "             'subgraph_method': subgraph_methods,\n",
    "             'reg': regularization_kwargs,\n",
    "             'perc': [3],\n",
    "             'clf_epochs': [10,20],\n",
    "             'coef_wmk': [200,600],\n",
    "             'frac': [0.02],\n",
    "             'num_subgraphs': [2,7],\n",
    "             'balance_beta_weights': [True,False],\n",
    "             'ignore_subgraph_neighbors': [True, False]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# config.optimization_kwargs['clf_only']=False       \n",
    "# watermarking_order = ['augment','use_PCgrad','sacrifice_kwargs','beta_selection','subgraph_method','reg','perc','clf_epochs','frac','coef_wmk','num_subgraphs','balance_beta_weights','ignore_subgraph_neighbors']\n",
    "# print(\"watermarking:\", watermarking_order)\n",
    "# count, [node_classifier, history, subgraph_dict,\\\n",
    "#          all_feature_importances, all_watermark_indices, probas] = dynamic_grid_search(data, dataset_name, debug_multiple_subgraphs, \n",
    "#                                                                                        all_dfs, False, variables, watermarking_order,\n",
    "#                                                                                        count_only=True)\n",
    "# print(count)\n",
    "\n",
    "\n",
    "config.augment_kwargs['separate_trainset_from_subgraphs'] = True\n",
    "config.augment_kwargs['ignore_subgraphs'] = True\n",
    "perturb=False\n",
    "perturb_lr = 1e-3\n",
    "node_classifier, history, subgraph_dict, \\\n",
    "    all_feature_importances, all_watermark_indices, probas = train(data, dataset_name, debug_multiple_subgraphs=False, save=True, print_every=1,perturb=perturb,perturb_lr=perturb_lr)\n",
    "primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "    percent_matches, percent_match_mean, percent_match_std, \\\n",
    "        primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "final_plot(history, '', percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got builtin_function_or_method)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got builtin_function_or_method)"
     ]
    }
   ],
   "source": [
    "data.x[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/training_results/computers/archGCN_elu_nLayers3_hDim256_drop0_skipTrue/_3%UnimportantIndices_average_10ClfEpochs_random_fraction0.01_numSubgraphs10_eps0.1_raw_beta_nodeMixUp40_lr0.002_epochs80_coefWmk200_regressionLambda0.1'\n",
    "probas = pickle.load(open(f'{folder}/probas','rb'))\n",
    "subgraph_dict = pickle.load(open(f'{folder}/subgraph_dict','rb'))\n",
    "all_watermark_indices = pickle.load(open(f'{folder}/all_watermark_indices','rb'))\n",
    "node_classifier = pickle.load(open(f'{folder}/node_classifier','rb'))\n",
    "history = pickle.load(open(f'{folder}/history','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlopt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define your objective function\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nlopt'"
     ]
    }
   ],
   "source": [
    "import nlopt\n",
    "import numpy as np\n",
    "\n",
    "# Define your objective function\n",
    "def objective(x, grad):\n",
    "    if grad.size > 0:\n",
    "        # Compute gradient if needed\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).reshape(original_shape)\n",
    "        x_tensor.requires_grad = True\n",
    "        log_logits_sub = node_classifier(x_tensor, edge_index)\n",
    "        probas_sub = log_logits_sub.clone().exp()\n",
    "\n",
    "        omit_indices, not_omit_indices = get_omit_indices(x_tensor, this_watermark, ignore_zeros_from_subgraphs=False)\n",
    "        raw_beta = solve_regression(x_tensor, probas_sub, regression_kwargs['lambda'])\n",
    "        beta = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta * this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "        balanced_beta_weights = torch.ones_like(B_x_W)\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        loss = torch.mean(torch.clamp(watermark_loss_kwargs['epsilon'] - B_x_W, min=0) * balanced_beta_weights)\n",
    "        loss.backward()\n",
    "\n",
    "        grad[:] = x_tensor.grad.numpy().flatten()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_minimize_lbfgsb options: 2 2\n",
      "_prepare_scalar_function\n",
      "C\n",
      "D\n",
      "F\n",
      "init\n",
      "update_fun: <function ScalarFunction.__init__.<locals>.update_fun at 0x3170d3880>\n",
      "fun_wrapped\n",
      "args: ()\n",
      "FD_METHODS\n",
      "finite_diff_options: {'method': '2-point', 'rel_step': None, 'abs_step': 1e-08, 'bounds': (array([-inf, -inf, -inf, ..., -inf, -inf, -inf]), array([inf, inf, inf, ..., inf, inf, inf]))}\n",
      "Help on function update_grad in module scipy.optimize._differentiable_functions:\n",
      "\n",
      "update_grad()\n",
      "\n",
      "update_grad: None\n",
      "not self.g_updated\n",
      "approx deriv\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m110\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m}\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Use the L-BFGS-B algorithm for optimization\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_watermark_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_sub_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# The optimal x found by the optimizer\u001b[39;00m\n\u001b[1;32m     45\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:308\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 308\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    314\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_optimize.py:390\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 390\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:188\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate_grad:\u001b[39m\u001b[38;5;124m'\u001b[39m,help(update_grad))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad_impl \u001b[38;5;241m=\u001b[39m update_grad\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Hessian Evaluation\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(hess):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:271\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot self.g_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:183\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapprox deriv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    574\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[1;32m    575\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    578\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 456\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:141\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs:\u001b[39m\u001b[38;5;124m'\u001b[39m,args)\n\u001b[0;32m--> 141\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mcompute_watermark_loss\u001b[0;34m(x_sub_flat)\u001b[0m\n\u001b[1;32m     21\u001b[0m log_logits_sub \u001b[38;5;241m=\u001b[39m node_classifier(x_sub, edge_index)\n\u001b[1;32m     22\u001b[0m probas_sub \u001b[38;5;241m=\u001b[39m log_logits_sub\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m---> 25\u001b[0m omit_indices,not_omit_indices \u001b[38;5;241m=\u001b[39m \u001b[43mget_omit_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_watermark\u001b[49m\u001b[43m,\u001b[49m\u001b[43mignore_zeros_from_subgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#indices where watermark is 0\u001b[39;00m\n\u001b[1;32m     26\u001b[0m raw_beta            \u001b[38;5;241m=\u001b[39m solve_regression(x_sub, probas_sub, regression_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m beta                \u001b[38;5;241m=\u001b[39m process_beta(raw_beta, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m], omit_indices, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_beta_method\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/eaaw_graphlime_utils.py:250\u001b[0m, in \u001b[0;36mget_omit_indices\u001b[0;34m(x_sub, watermark, ignore_zeros_from_subgraphs)\u001b[0m\n\u001b[1;32m    248\u001b[0m zero_indices_within_watermark \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(watermark\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    249\u001b[0m omit_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(zero_features_within_subgraph[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m+\u001b[39m zero_indices_within_watermark[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())))\n\u001b[0;32m--> 250\u001b[0m not_omit_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_sub\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43momit_indices\u001b[49m])\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m omit_indices, not_omit_indices\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/_tensor.py:1116\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, element)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1113\u001b[0m     element, (torch\u001b[38;5;241m.\u001b[39mTensor, Number, torch\u001b[38;5;241m.\u001b[39mSymInt, torch\u001b[38;5;241m.\u001b[39mSymFloat, torch\u001b[38;5;241m.\u001b[39mSymBool)\n\u001b[1;32m   1114\u001b[0m ):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1120\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.optimize import minimize\n",
    "results = []\n",
    "for sig in subgraph_dict.keys():\n",
    "    watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "    regression_kwargs = config.regression_kwargs\n",
    "    this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "    x_sub = data_sub.x\n",
    "    edge_index = data_sub.edge_index\n",
    "    original_shape = x_sub.shape\n",
    "    x_sub_flat = x_sub.flatten()\n",
    "    # global i\n",
    "    # i = 0\n",
    "    def compute_watermark_loss(x_sub_flat):\n",
    "        # global i\n",
    "        # i+=1\n",
    "        # print(/i,end='\\r')\n",
    "        # print('hi')\n",
    "        x_sub = torch.tensor(x_sub_flat, dtype=torch.float32).reshape(original_shape)\n",
    "        balanced_beta_weights = torch.ones(x_sub.shape[1])\n",
    "\n",
    "        log_logits_sub = node_classifier(x_sub, edge_index)\n",
    "        probas_sub = log_logits_sub.clone().exp()\n",
    "\n",
    "\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        raw_beta            = solve_regression(x_sub, probas_sub, regression_kwargs['lambda'])\n",
    "        beta                = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta*this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        this_loss_watermark = torch.mean(torch.clamp(watermark_loss_kwargs['epsilon']-B_x_W, min=0)*balanced_beta_weights)\n",
    "        loss_watermark  = this_loss_watermark\n",
    "        # print('ok')\n",
    "        return loss_watermark.item()\n",
    "\n",
    "    # Ensure maxiter is enforced and debugging output\n",
    "    options = {'maxfun': 2, 'iprint':110, 'maxls':2, 'maxiter':2}\n",
    "\n",
    "    # Use the L-BFGS-B algorithm for optimization\n",
    "    result = minimize(compute_watermark_loss, x_sub_flat, \n",
    "                      method='L-BFGS-B', \n",
    "                      options=options)\n",
    "\n",
    "    # The optimal x found by the optimizer\n",
    "    results.append(result)\n",
    "    # print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scipy.optimize' from '/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/__init__.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(torch.wherae(torch.eq(data_copy.x, data.x)==False)[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[24,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.8543e-05,  0.0000e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.x[[24,25],[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([1,12])\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/20: 19.604000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m\n\u001b[1;32m     62\u001b[0m x_copy \u001b[38;5;241m=\u001b[39m x_copy\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#    edge_index_copy, x_copy, _ = augment_data(data, node_aug, edge_aug, train_nodes_to_consider, all_subgraph_indices)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#    x_copy = x_copy.requires_grad_(True)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m log_logits_copy          \u001b[38;5;241m=\u001b[39m \u001b[43mnode_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m probas_copy \u001b[38;5;241m=\u001b[39m log_logits_copy\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m     68\u001b[0m loss_watermark_scaled_copy \u001b[38;5;241m=\u001b[39m compute_watermark_loss(subgraph_dict, probas_copy, beta_weights)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/models.py:201\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    199\u001b[0m intermediate_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnLayers):\n\u001b[0;32m--> 201\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(x)\n\u001b[1;32m    203\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:263\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:547\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 547\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    549\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_beta_weights(subgraph_dict, num_features):\n",
    "    if config.watermark_loss_kwargs['balance_beta_weights'] == True:\n",
    "        beta_weights = get_balanced_beta_weights([subgraph_dict[sig]['subgraph'] for sig in subgraph_dict.keys()])\n",
    "    elif config.watermark_loss_kwargs['balance_beta_weights'] == False:\n",
    "        beta_weights = torch.ones(len(subgraph_dict),num_features)\n",
    "    return beta_weights\n",
    "\n",
    "def process_beta(beta, alpha, omit_indices, scale_beta_method='clip'):\n",
    "    if scale_beta_method=='tanh':\n",
    "        beta = torch.tanh(alpha*beta)\n",
    "    elif scale_beta_method=='tan':\n",
    "        beta = torch.tan(alpha*beta)\n",
    "    elif scale_beta_method=='clip':\n",
    "        beta = torch.clip(beta,min=-1,max=1)\n",
    "    elif scale_beta_method==None:\n",
    "        pass\n",
    "    beta = beta.clone()  # Avoid in-place operation\n",
    "    if omit_indices is not None and len(omit_indices)>0:\n",
    "        beta[omit_indices] = 0 # zero out non-contributing indices\n",
    "    return beta\n",
    "\n",
    "def compute_watermark_loss(subgraph_dict, probas, beta_weights):\n",
    "    watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "    regression_kwargs = config.regression_kwargs\n",
    "    optimization_kwargs = config.optimization_kwargs\n",
    "\n",
    "    loss_watermark = torch.tensor(0.0)\n",
    "    for s, sig in enumerate(subgraph_dict.keys()):\n",
    "        this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "        x_sub, y_sub = data_sub.x, probas[subgraph_node_indices]\n",
    "        ''' epoch condtion: epoch==epoch-1'''\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        raw_beta            = solve_regression(x_sub, y_sub, regression_kwargs['lambda'])\n",
    "        beta                = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta*this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "\n",
    "        balanced_beta_weights = beta_weights[s]\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        loss_watermark = loss_watermark+torch.mean(torch.clamp(watermark_loss_kwargs['epsilon']-B_x_W, min=0)*balanced_beta_weights)\n",
    "\n",
    "\n",
    "    loss_watermark = loss_watermark/len(subgraph_dict)\n",
    "    loss_watermark_scaled = loss_watermark*optimization_kwargs['coefWmk']\n",
    "    return loss_watermark_scaled\n",
    "\n",
    "\n",
    "beta_weights = get_beta_weights(subgraph_dict, data.x.shape[1])\n",
    "all_subgraph_indices = []\n",
    "for sig in subgraph_dict.keys():\n",
    "    nodeIndices = subgraph_dict[sig]['nodeIndices']\n",
    "    all_subgraph_indices+=nodeIndices.tolist()\n",
    "reg_copy=None\n",
    "\n",
    "\n",
    "lr = 0.002\n",
    "optimizer = optim.Adam(node_classifier.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for j in range(100):\n",
    "    x_copy,edge_index_copy = copy.deepcopy(data).x, copy.deepcopy(data).edge_index\n",
    "    x_copy = x_copy.requires_grad_(True)\n",
    "    #with torch.no_grad():\n",
    "    #    edge_index_copy, x_copy, _ = augment_data(data, node_aug, edge_aug, train_nodes_to_consider, all_subgraph_indices)\n",
    "    #    x_copy = x_copy.requires_grad_(True)\n",
    "    log_logits_copy          = node_classifier(x_copy, edge_index_copy)\n",
    "    probas_copy = log_logits_copy.clone().exp()\n",
    "    loss_watermark_scaled_copy = compute_watermark_loss(subgraph_dict, probas_copy, beta_weights)\n",
    "    wmk_loss = loss_watermark_scaled_copy+reg_copy if reg_copy is not None else loss_watermark_scaled_copy\n",
    "    optimizer.zero_grad()\n",
    "    wmk_loss.backward()\n",
    "    print(f'{j}/20: {wmk_loss:3f}',end='\\r')\n",
    "    this_grad = torch.zeros_like(data.x)\n",
    "    this_grad[all_subgraph_indices] = x_copy.grad.clone()[all_subgraph_indices]\n",
    "    this_grad[all_subgraph_indices] = (this_grad[all_subgraph_indices]-this_grad[all_subgraph_indices].mean())/(this_grad[all_subgraph_indices].max()-this_grad[all_subgraph_indices].min())\n",
    "    x_grad_wmk_loss = torch.zeros_like(data.x)\n",
    "    x_grad_wmk_loss[all_subgraph_indices] = this_grad.clone()[all_subgraph_indices]\n",
    "    # print(x_grad_wmk_loss[all_subgraph_indices])\n",
    "    data.x = data.x - perturb_lr*x_grad_wmk_loss\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  7,  11,  23,  34,  37,  41,  43,  69,  73,  86,  97, 105, 109, 110,\n",
      "        134, 144, 154, 159, 168, 177, 185, 186, 195, 212, 217, 219, 225, 227,\n",
      "        238, 250, 259, 277, 279, 284, 298, 328, 355, 358, 363, 365, 367, 376,\n",
      "        382, 386, 427, 430, 439, 442, 443, 457, 489, 503, 506, 507, 515, 520,\n",
      "        542, 544, 554, 559, 582, 597, 617, 649, 664, 687, 695, 698, 704, 708,\n",
      "        715, 729, 732, 747, 755, 758])\n"
     ]
    }
   ],
   "source": [
    "beta_weights = get_beta_weights(subgraph_dict, data.x.shape[1])\n",
    "for s, sig in enumerate(subgraph_dict.keys()):\n",
    "    this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "    x_sub, y_sub = data_sub.x, probas[subgraph_node_indices]\n",
    "    ''' epoch condtion: epoch==epoch-1'''\n",
    "    omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "    print(not_omit_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['11293_4896_5573_3951_10751_63'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_2_env",
   "language": "python",
   "name": "proj_2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
