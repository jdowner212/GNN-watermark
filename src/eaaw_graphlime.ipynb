{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScalarFunction\n"
     ]
    }
   ],
   "source": [
    "from eaaw_graphlime_utils import *\n",
    "import gc\n",
    "\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "\n",
    "\n",
    "from   pcgrad.pcgrad import PCGrad # from the following: https://github.com/WeiChengTseng/Pytorch-PCGrad. Renamed to 'pcgrad' and moved to site-packages folder.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transorms used when loading computers: ['CreateMaskTransform()']\n",
      "train_mask: 12376\n",
      "test_mask: 689\n",
      "val_mask: 687\n"
     ]
    }
   ],
   "source": [
    "dataset_name='computers'\n",
    "if dataset_attributes[dataset_name]['single_or_multi_graph']=='single':\n",
    "    dataset = prep_data(dataset_name=dataset_name, location='default', batch_size='default', transform_list='default',\n",
    "                        train_val_test_split=[0.9,0.05,0.05])\n",
    "    graph_to_watermark = data = dataset[0]\n",
    "elif dataset_attributes[dataset_name]['single_or_multi_graph']=='multi':\n",
    "    [train_dataset, val_dataset, test_dataset], [train_loader, val_loader, test_loader] = prep_data(dataset_name=dataset_name, location='default', \n",
    "                                                                                                    batch_size='default', transform_list='default',\n",
    "                                                                                                                            train_val_test_split=[0.9,0.05,0.05])\n",
    "    graph_to_watermark = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization_kwargs, node_classifier_kwargs, watermark_kwargs, subgraph_kwargs, augment_kwargs, watermark_loss_kwargs, regression_kwargs = get_presets(dataset, 'default')\n",
    "get_presets(dataset,dataset_name)\n",
    "\n",
    "compare_unimportant_against_random=False\n",
    "\n",
    "config.watermark_kwargs['fancy_selection_kwargs']['clf_only_epochs'] = 10\n",
    "config.optimization_kwargs['lr']=0.002\n",
    "config.optimization_kwargs['epochs']=80\n",
    "config.optimization_kwargs['use_pcgrad']=True\n",
    "\n",
    "config.optimization_kwargs['coefWmk']=200\n",
    "config.optimization_kwargs['use_gradnorm']=False\n",
    "\n",
    "config.augment_kwargs['nodeDrop']['use']=False\n",
    "config.augment_kwargs['nodeMixUp']['use']=True\n",
    "config.augment_kwargs['nodeFeatMask']['use']=False\n",
    "config.augment_kwargs['edgeDrop']['use']=False\n",
    "config.augment_kwargs['separate_trainset_from_subgraphs'] = True\n",
    "config.augment_kwargs['ignore_subgraphs'] = True\n",
    "config.watermark_kwargs['fancy_selection_kwargs']['percent_of_features_to_watermark']=3\n",
    "config.watermark_kwargs['watermark_type']='fancy'\n",
    "config.subgraph_kwargs['numSubgraphs']=7\n",
    "config.subgraph_kwargs['fraction']=0.01\n",
    "config.subgraph_kwargs['method']='khop'\n",
    "config.optimization_kwargs['sacrifice_kwargs']['method']='subgraph_node_indices'\n",
    "config.optimization_kwargs['sacrifice_kwargs']['percentage']=1\n",
    "config.optimization_kwargs['separate_forward_passes_per_subgraph']=True\n",
    "config.watermark_loss_kwargs['epsilon']=1e-1\n",
    "\n",
    "validate_kwargs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup_subgraph_dict\n",
      "nodeIndices: [998, 1784, 2215, 2253, 3366, 3875, 4739, 5041, 5713, 6026, 8210, 9010, 10408, 10901, 10941, 12729, 13094]\n",
      "nodeIndices: [227, 421, 1243, 1418, 2611, 3924, 4165, 5250, 6013, 8214, 8417, 8528, 11598, 12337, 12888, 13529, 13708]\n",
      "nodeIndices: [473, 648, 1322, 1549, 3896, 4153, 4862, 6314, 6425, 9446, 9982, 10250, 10611, 11136, 12189, 12758, 12836]\n",
      "nodeIndices: [505, 872, 1524, 2316, 4257, 5060, 5408, 6120, 6544, 7439, 7984, 8266, 8376, 8460, 8841, 12592, 12850]\n",
      "nodeIndices: [400, 830, 1660, 3799, 3892, 4427, 4578, 5510, 6546, 7485, 7748, 9203, 9529, 9574, 9937, 10474, 13304]\n",
      "nodeIndices: [830]\n",
      "nodeIndices: [892, 1093, 1189, 2497, 3915, 4401, 4920, 5927, 6671, 7172, 8280, 8367, 8734, 9717, 10725, 10793, 13573]\n",
      "Sacrificing 100% of subgraph nodes from node classification training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b0994501684a479df1f031ccd61a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'loss_dict' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m perturb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     72\u001b[0m perturb_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m     73\u001b[0m node_classifier, history, subgraph_dict, \\\n\u001b[0;32m---> 74\u001b[0m     all_feature_importances, all_watermark_indices, probas \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_multiple_subgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mperturb_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturb_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n\u001b[1;32m     76\u001b[0m     percent_matches, percent_match_mean, percent_match_std, \\\n\u001b[1;32m     77\u001b[0m         primary_acc_curve, watermark_acc_curve, train_acc, val_acc \u001b[38;5;241m=\u001b[39m get_performance_trends(history, subgraph_dict)\n\u001b[1;32m     78\u001b[0m final_plot(history, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/eaaw_graphlime_utils.py:946\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, dataset_name, debug_multiple_subgraphs, save, print_every, perturb, perturb_lr)\u001b[0m\n\u001b[1;32m    944\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 946\u001b[0m     \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()  \n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wmk_optimization_condition_met:\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/eaaw_graphlime_utils.py:896\u001b[0m, in \u001b[0;36mtrain.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    893\u001b[0m log_logits          \u001b[38;5;241m=\u001b[39m node_classifier(x, edge_index)\n\u001b[1;32m    894\u001b[0m loss_primary        \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(log_logits[train_nodes_to_consider_mask], y[train_nodes_to_consider_mask])\n\u001b[0;32m--> 896\u001b[0m loss_dict, unweighted_total, _ \u001b[38;5;241m=\u001b[39m get_weighted_losses(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mloss_dict\u001b[49m, loss_primary)\n\u001b[1;32m    897\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_primary \u001b[38;5;241m=\u001b[39m loss_primary_weighted \u001b[38;5;241m=\u001b[39m unweighted_total\n\u001b[1;32m    898\u001b[0m backward([loss], optimizer, type_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimary\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch\u001b[38;5;241m=\u001b[39mepoch, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'loss_dict' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "merged_dict = merge_kwargs_dicts()#config.node_classifier_kwargs, config.optimization_kwargs, config.watermark_kwargs, config.subgraph_kwargs, config.regression_kwargs, config.watermark_loss_kwargs, config.augment_kwargs)\n",
    "merged_dict_keys = list(merged_dict.keys()) + ['Train Acc','Val Acc','Match Rates','Final Betas','watermark']\n",
    "all_dfs = pd.DataFrame(dict(zip(merged_dict_keys,[]*len(merged_dict_keys))))\n",
    "\n",
    "# all_dfs = pickle.load(open('/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/src/results_df.pkl','rb'))\n",
    "# all_results = {}\n",
    "\n",
    "\n",
    "\n",
    "scale_beta_method=None\n",
    "debug_multiple_subgraphs=False\n",
    "\n",
    "\n",
    "''' (individualize boolean, multisubgraph method, index selection method) '''\n",
    "selection_kwargss = [#(False, None, 'random'), # random, not individualized\n",
    "                    (False,'average','unimportant'), # average, not individualized\n",
    "                    #(False,'concat','unimportant'), # concat, not inividualized\n",
    "                    # (True,None,'unimportant') # unimportant, individualized\n",
    "                    ]\n",
    "\n",
    "''' (method, regenerate_boolean) '''\n",
    "subgraph_methods = [('random',False)]\n",
    "\n",
    "''' [subset, percentage] '''\n",
    "sacrifice_kwargss = [['train_node_indices',1],['train_node_indices',0.75],['subgraph_node_indices',1],[None,None]]\n",
    "\n",
    "''' (method, L2_lambda (if applicable, else None) '''\n",
    "regularization_kwargs = [(None,None),('L2',0.01),('beta_var',None)]\n",
    "\n",
    "\n",
    "variables = {'augment': [{'separate_trainset_from_subgraphs': True,\n",
    "                          'ignore_subgraphs': True,\n",
    "                          'nodeDrop': False,\n",
    "                          'nodeMixUp': False,\n",
    "                          'nodeFeatMask': False,\n",
    "                          'edgeDrop': False},\n",
    "                          {'separate_trainset_from_subgraphs': True,\n",
    "                          'ignore_subgraphs': True,\n",
    "                          'nodeDrop': True,\n",
    "                          'nodeMixUp': True,\n",
    "                          'nodeFeatMask': True,\n",
    "                          'edgeDrop': True},],\n",
    "             'sacrifice_kwargs':  sacrifice_kwargss,\n",
    "             'beta_selection': selection_kwargss,\n",
    "             'use_PCgrad': [True,False],\n",
    "             'subgraph_method': subgraph_methods,\n",
    "             'reg': regularization_kwargs,\n",
    "             'perc': [3],\n",
    "             'clf_epochs': [10,20],\n",
    "             'coef_wmk': [200,600],\n",
    "             'frac': [0.02],\n",
    "             'num_subgraphs': [2,7],\n",
    "             'balance_beta_weights': [True,False],\n",
    "             'ignore_subgraph_neighbors': [True, False]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# config.optimization_kwargs['clf_only']=False       \n",
    "# watermarking_order = ['augment','use_PCgrad','sacrifice_kwargs','beta_selection','subgraph_method','reg','perc','clf_epochs','frac','coef_wmk','num_subgraphs','balance_beta_weights','ignore_subgraph_neighbors']\n",
    "# print(\"watermarking:\", watermarking_order)\n",
    "# count, [node_classifier, history, subgraph_dict,\\\n",
    "#          all_feature_importances, all_watermark_indices, probas] = dynamic_grid_search(data, dataset_name, debug_multiple_subgraphs, \n",
    "#                                                                                        all_dfs, False, variables, watermarking_order,\n",
    "#                                                                                        count_only=True)\n",
    "# print(count)\n",
    "\n",
    "\n",
    "config.augment_kwargs['separate_trainset_from_subgraphs'] = True\n",
    "config.augment_kwargs['ignore_subgraphs'] = True\n",
    "perturb=False\n",
    "perturb_lr = 1e-3\n",
    "node_classifier, history, subgraph_dict, \\\n",
    "    all_feature_importances, all_watermark_indices, probas = train(data, dataset_name, debug_multiple_subgraphs=False, save=True, print_every=1,perturb=perturb,perturb_lr=perturb_lr)\n",
    "primary_loss_curve, watermark_loss_curve, final_betas, watermarks, \\\n",
    "    percent_matches, percent_match_mean, percent_match_std, \\\n",
    "        primary_acc_curve, watermark_acc_curve, train_acc, val_acc = get_performance_trends(history, subgraph_dict)\n",
    "final_plot(history, '', percent_matches, primary_loss_curve, watermark_loss_curve, train_acc)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 10, 'b': 20, 'c': 30}\n"
     ]
    }
   ],
   "source": [
    "def closure():\n",
    "    # Example local variables\n",
    "    a = 10\n",
    "    b = 20\n",
    "    c = a + b\n",
    "\n",
    "    # This line captures all local variables\n",
    "    intermediate_values.update(locals())\n",
    "\n",
    "    # Return some value (e.g., loss in actual training)\n",
    "    return c\n",
    "\n",
    "# Define a dictionary to store intermediate values\n",
    "intermediate_values = {}\n",
    "\n",
    "# Call the closure function\n",
    "result = closure()\n",
    "\n",
    "# Access the captured intermediate values\n",
    "print(intermediate_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got builtin_function_or_method)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), None and long or byte Variables are valid indices (got builtin_function_or_method)"
     ]
    }
   ],
   "source": [
    "data.x[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/janedowner/Desktop/Desktop/IDEAL/Project_2/training_results/computers/archGCN_elu_nLayers3_hDim256_drop0_skipTrue/_3%UnimportantIndices_average_10ClfEpochs_random_fraction0.01_numSubgraphs10_eps0.1_raw_beta_nodeMixUp40_lr0.002_epochs80_coefWmk200_regressionLambda0.1'\n",
    "probas = pickle.load(open(f'{folder}/probas','rb'))\n",
    "subgraph_dict = pickle.load(open(f'{folder}/subgraph_dict','rb'))\n",
    "all_watermark_indices = pickle.load(open(f'{folder}/all_watermark_indices','rb'))\n",
    "node_classifier = pickle.load(open(f'{folder}/node_classifier','rb'))\n",
    "history = pickle.load(open(f'{folder}/history','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlopt\n",
      "  Downloading nlopt-2.6.2.tar.gz (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages (from nlopt) (1.26.4)\n",
      "Building wheels for collected packages: nlopt\n",
      "  Building wheel for nlopt (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[215 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m cmake version 3.26.4\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m CMake suite maintained and supported by Kitware (kitware.com/cmake).\n",
      "  \u001b[31m   \u001b[0m -- The C compiler identification is AppleClang 15.0.0.15000309\n",
      "  \u001b[31m   \u001b[0m -- The CXX compiler identification is AppleClang 15.0.0.15000309\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Found PythonInterp: /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/bin/python (found version \"3.12\")\n",
      "  \u001b[31m   \u001b[0m -- Found Python includes: /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/include/python3.12\n",
      "  \u001b[31m   \u001b[0m -- Found Python libs: /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib\n",
      "  \u001b[31m   \u001b[0m \u001b[0mCMake Deprecation Warning at extern/nlopt/CMakeLists.txt:15 (cmake_minimum_required):\n",
      "  \u001b[31m   \u001b[0m   Compatibility with CMake < 2.8.12 will be removed from a future version of\n",
      "  \u001b[31m   \u001b[0m   CMake.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  \u001b[31m   \u001b[0m   CMake that the project does not need compatibility with older versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m -- NLopt version 2.6.2\n",
      "  \u001b[31m   \u001b[0m -- Looking for dlfcn.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for dlfcn.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for getopt.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for getopt.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for unistd.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for unistd.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for string.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for string.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for strings.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for strings.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for inttypes.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for inttypes.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for memory.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for memory.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for stdlib.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for stdlib.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for stdint.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for stdint.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for time.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for time.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/types.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/types.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/stat.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/stat.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/time.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for sys/time.h - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for getpid\n",
      "  \u001b[31m   \u001b[0m -- Looking for getpid - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for syscall\n",
      "  \u001b[31m   \u001b[0m -- Looking for syscall - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for isinf\n",
      "  \u001b[31m   \u001b[0m -- Looking for isinf - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for isnan\n",
      "  \u001b[31m   \u001b[0m -- Looking for isnan - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for gettimeofday\n",
      "  \u001b[31m   \u001b[0m -- Looking for gettimeofday - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for qsort_r\n",
      "  \u001b[31m   \u001b[0m -- Looking for qsort_r - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for time\n",
      "  \u001b[31m   \u001b[0m -- Looking for time - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for copysign\n",
      "  \u001b[31m   \u001b[0m -- Looking for copysign - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for stddef.h\n",
      "  \u001b[31m   \u001b[0m -- Looking for stddef.h - found\n",
      "  \u001b[31m   \u001b[0m -- Check size of uint32_t\n",
      "  \u001b[31m   \u001b[0m -- Check size of uint32_t - done\n",
      "  \u001b[31m   \u001b[0m -- Check size of unsigned int\n",
      "  \u001b[31m   \u001b[0m -- Check size of unsigned int - done\n",
      "  \u001b[31m   \u001b[0m -- Check size of unsigned long\n",
      "  \u001b[31m   \u001b[0m -- Check size of unsigned long - done\n",
      "  \u001b[31m   \u001b[0m -- Looking for sqrt in m\n",
      "  \u001b[31m   \u001b[0m -- Looking for sqrt in m - found\n",
      "  \u001b[31m   \u001b[0m -- Looking for fpclassify\n",
      "  \u001b[31m   \u001b[0m -- Looking for fpclassify - TRUE\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAVE_THREAD_LOCAL_STORAGE\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAVE_THREAD_LOCAL_STORAGE - Success\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAVE_THREAD_LOCAL_STORAGE\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAVE_THREAD_LOCAL_STORAGE - Failed\n",
      "  \u001b[31m   \u001b[0m -- Looking for __cplusplus\n",
      "  \u001b[31m   \u001b[0m -- Looking for __cplusplus - found\n",
      "  \u001b[31m   \u001b[0m -- Performing Test SUPPORTS_STDCXX11\n",
      "  \u001b[31m   \u001b[0m -- Performing Test SUPPORTS_STDCXX11 - Success\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAS_FPIC\n",
      "  \u001b[31m   \u001b[0m -- Performing Test HAS_FPIC - Success\n",
      "  \u001b[31m   \u001b[0m -- Found PythonLibs: /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib (found suitable exact version \"3.12.0\")\n",
      "  \u001b[31m   \u001b[0m -- Found NumPy: /opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/numpy/core/include (found version \"1.26.4\")\n",
      "  \u001b[31m   \u001b[0m error: (\"/opt/homebrew/opt/pkg-config/bin/pkg-config\" \"--variable=prefix\" \"guile-3.0\") exited with non-zero error code 127\n",
      "  \u001b[31m   \u001b[0m error: (\"/opt/homebrew/opt/pkg-config/bin/pkg-config\" \"--variable=sitedir\" \"guile-3.0\") exited with non-zero error code 127\n",
      "  \u001b[31m   \u001b[0m error: (\"/opt/homebrew/opt/pkg-config/bin/pkg-config\" \"--variable=extensiondir\" \"guile-3.0\") exited with non-zero error code 127\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find Guile (missing: GUILE_ROOT_DIR GUILE_INCLUDE_DIRS GUILE_LIBRARIES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find SWIG (missing: SWIG_EXECUTABLE SWIG_DIR)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find Octave (missing: OCTAVE_EXECUTABLE OCTAVE_ROOT_DIR OCTAVE_INCLUDE_DIRS OCTAVE_LIBRARIES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find Matlab (missing: Matlab_INCLUDE_DIRS Matlab_MEX_LIBRARY Matlab_MEX_EXTENSION Matlab_ROOT_DIR Matlab_MX_LIBRARY MX_LIBRARY MAIN_PROGRAM) (found version \"NOTFOUND\")\n",
      "  \u001b[31m   \u001b[0m -- Configuring done (10.2s)\n",
      "  \u001b[31m   \u001b[0m -- Generating done (0.0s)\n",
      "  \u001b[31m   \u001b[0m -- Build files have been written to: /private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/build/temp.macosx-11.0-arm64-cpython-312\n",
      "  \u001b[31m   \u001b[0m [  3%] \u001b[34m\u001b[1mGenerating nlopt.hpp\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [  3%] \u001b[34m\u001b[1mGenerating nlopt.f\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[0mCMake Deprecation Warning at /private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/extern/nlopt/cmake/generate-fortran.cmake:1 (cmake_minimum_required):\n",
      "  \u001b[31m   \u001b[0m   Compatibility with CMake < 2.8.12 will be removed from a future version of\n",
      "  \u001b[31m   \u001b[0m   CMake.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  \u001b[31m   \u001b[0m   CMake that the project does not need compatibility with older versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\u001b[0mCMake Deprecation Warning at /private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/extern/nlopt/cmake/generate-cpp.cmake:1 (cmake_minimum_required):\n",
      "  \u001b[31m   \u001b[0m   Compatibility with CMake < 2.8.12 will be removed from a future version of\n",
      "  \u001b[31m   \u001b[0m   CMake.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
      "  \u001b[31m   \u001b[0m   CMake that the project does not need compatibility with older versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[0m\n",
      "  \u001b[31m   \u001b[0m [  3%] Built target generate-fortran\n",
      "  \u001b[31m   \u001b[0m [  3%] Built target generate-cpp\n",
      "  \u001b[31m   \u001b[0m [  7%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/direct/DIRect.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [  7%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/direct/direct_wrap.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [  9%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/direct/DIRserial.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 11%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/direct/DIRsubrout.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 13%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/cdirect/cdirect.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 15%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/cdirect/hybrid.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 17%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/praxis/praxis.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 19%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/luksan/plis.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 21%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/luksan/plip.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 23%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/luksan/pnet.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 25%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/luksan/mssubs.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 27%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/luksan/pssubs.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 29%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/crs/crs.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 31%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/mlsl/mlsl.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 33%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/mma/mma.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 35%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/mma/ccsa_quadratic.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 37%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/cobyla/cobyla.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 39%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/newuoa/newuoa.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 41%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/neldermead/nldrmd.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 43%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/neldermead/sbplx.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 45%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/auglag/auglag.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 47%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/bobyqa/bobyqa.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 49%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/isres/isres.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 50%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/slsqp/slsqp.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 52%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/esch/esch.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 54%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/api/general.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 56%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/api/options.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 58%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/api/optimize.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 60%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/api/deprecated.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 62%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/api/f77api.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 64%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/mt19937ar.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 66%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/sobolseq.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 68%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/timer.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 70%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/stop.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 72%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/redblack.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 74%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/qsort_r.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 76%] \u001b[32mBuilding C object extern/nlopt/CMakeFiles/nlopt.dir/src/util/rescale.c.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 78%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/stogo/global.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 80%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/stogo/linalg.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 82%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/stogo/local.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 84%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/stogo/stogo.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 86%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/stogo/tools.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 88%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/ags/evolvent.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 90%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/ags/solver.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 92%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/ags/local_optimizer.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 94%] \u001b[32mBuilding CXX object extern/nlopt/CMakeFiles/nlopt.dir/src/algs/ags/ags.cc.o\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [ 96%] \u001b[32m\u001b[1mLinking CXX static library libnlopt.a\u001b[0m\n",
      "  \u001b[31m   \u001b[0m [100%] Built target nlopt\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/setup.py\", line 85, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/__init__.py\", line 104, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/wheel/bdist_wheel.py\", line 368, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/setup.py\", line 28, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/hl/yv33n5y94_db5qlr1ht0ckd40000gn/T/pip-install-1qrjo3ly/nlopt_cbdee7a8b00f49a9bb8fdca1b81b471c/setup.py\", line 70, in build_extension\n",
      "  \u001b[31m   \u001b[0m     nlopt_py = next(Path(self.build_temp).rglob(\"nlopt.py\"))\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m StopIteration\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for nlopt\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for nlopt\n",
      "Failed to build nlopt\n",
      "\u001b[31mERROR: Could not build wheels for nlopt, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nlopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlopt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define your objective function\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nlopt'"
     ]
    }
   ],
   "source": [
    "import nlopt\n",
    "import numpy as np\n",
    "\n",
    "# Define your objective function\n",
    "def objective(x, grad):\n",
    "    if grad.size > 0:\n",
    "        # Compute gradient if needed\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).reshape(original_shape)\n",
    "        x_tensor.requires_grad = True\n",
    "        log_logits_sub = node_classifier(x_tensor, edge_index)\n",
    "        probas_sub = log_logits_sub.clone().exp()\n",
    "\n",
    "        omit_indices, not_omit_indices = get_omit_indices(x_tensor, this_watermark, ignore_zeros_from_subgraphs=False)\n",
    "        raw_beta = solve_regression(x_tensor, probas_sub, regression_kwargs['lambda'])\n",
    "        beta = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta * this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "        balanced_beta_weights = torch.ones_like(B_x_W)\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        loss = torch.mean(torch.clamp(watermark_loss_kwargs['epsilon'] - B_x_W, min=0) * balanced_beta_weights)\n",
    "        loss.backward()\n",
    "\n",
    "        grad[:] = x_tensor.grad.numpy().flatten()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_minimize_lbfgsb options: 2 2\n",
      "_prepare_scalar_function\n",
      "C\n",
      "D\n",
      "F\n",
      "init\n",
      "update_fun: <function ScalarFunction.__init__.<locals>.update_fun at 0x3170d3880>\n",
      "fun_wrapped\n",
      "args: ()\n",
      "FD_METHODS\n",
      "finite_diff_options: {'method': '2-point', 'rel_step': None, 'abs_step': 1e-08, 'bounds': (array([-inf, -inf, -inf, ..., -inf, -inf, -inf]), array([inf, inf, inf, ..., inf, inf, inf]))}\n",
      "Help on function update_grad in module scipy.optimize._differentiable_functions:\n",
      "\n",
      "update_grad()\n",
      "\n",
      "update_grad: None\n",
      "not self.g_updated\n",
      "approx deriv\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n",
      "fun_wrapped\n",
      "args: ()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m110\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m}\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Use the L-BFGS-B algorithm for optimization\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_watermark_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_sub_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# The optimal x found by the optimizer\u001b[39;00m\n\u001b[1;32m     45\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:308\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 308\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    314\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_optimize.py:390\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 390\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:188\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate_grad:\u001b[39m\u001b[38;5;124m'\u001b[39m,help(update_grad))\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad_impl \u001b[38;5;241m=\u001b[39m update_grad\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Hessian Evaluation\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(hess):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:271\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot self.g_updated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:183\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapprox deriv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    574\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[1;32m    575\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    578\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 456\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:141\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs:\u001b[39m\u001b[38;5;124m'\u001b[39m,args)\n\u001b[0;32m--> 141\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mcompute_watermark_loss\u001b[0;34m(x_sub_flat)\u001b[0m\n\u001b[1;32m     21\u001b[0m log_logits_sub \u001b[38;5;241m=\u001b[39m node_classifier(x_sub, edge_index)\n\u001b[1;32m     22\u001b[0m probas_sub \u001b[38;5;241m=\u001b[39m log_logits_sub\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m---> 25\u001b[0m omit_indices,not_omit_indices \u001b[38;5;241m=\u001b[39m \u001b[43mget_omit_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_watermark\u001b[49m\u001b[43m,\u001b[49m\u001b[43mignore_zeros_from_subgraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#indices where watermark is 0\u001b[39;00m\n\u001b[1;32m     26\u001b[0m raw_beta            \u001b[38;5;241m=\u001b[39m solve_regression(x_sub, probas_sub, regression_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m beta                \u001b[38;5;241m=\u001b[39m process_beta(raw_beta, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m], omit_indices, watermark_loss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_beta_method\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/eaaw_graphlime_utils.py:250\u001b[0m, in \u001b[0;36mget_omit_indices\u001b[0;34m(x_sub, watermark, ignore_zeros_from_subgraphs)\u001b[0m\n\u001b[1;32m    248\u001b[0m zero_indices_within_watermark \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(watermark\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    249\u001b[0m omit_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(zero_features_within_subgraph[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m+\u001b[39m zero_indices_within_watermark[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())))\n\u001b[0;32m--> 250\u001b[0m not_omit_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_sub\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43momit_indices\u001b[49m])\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m omit_indices, not_omit_indices\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/_tensor.py:1116\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, element)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1113\u001b[0m     element, (torch\u001b[38;5;241m.\u001b[39mTensor, Number, torch\u001b[38;5;241m.\u001b[39mSymInt, torch\u001b[38;5;241m.\u001b[39mSymFloat, torch\u001b[38;5;241m.\u001b[39mSymBool)\n\u001b[1;32m   1114\u001b[0m ):\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1120\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.optimize import minimize\n",
    "results = []\n",
    "for sig in subgraph_dict.keys():\n",
    "    watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "    regression_kwargs = config.regression_kwargs\n",
    "    this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "    x_sub = data_sub.x\n",
    "    edge_index = data_sub.edge_index\n",
    "    original_shape = x_sub.shape\n",
    "    x_sub_flat = x_sub.flatten()\n",
    "    # global i\n",
    "    # i = 0\n",
    "    def compute_watermark_loss(x_sub_flat):\n",
    "        # global i\n",
    "        # i+=1\n",
    "        # print(/i,end='\\r')\n",
    "        # print('hi')\n",
    "        x_sub = torch.tensor(x_sub_flat, dtype=torch.float32).reshape(original_shape)\n",
    "        balanced_beta_weights = torch.ones(x_sub.shape[1])\n",
    "\n",
    "        log_logits_sub = node_classifier(x_sub, edge_index)\n",
    "        probas_sub = log_logits_sub.clone().exp()\n",
    "\n",
    "\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        raw_beta            = solve_regression(x_sub, probas_sub, regression_kwargs['lambda'])\n",
    "        beta                = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta*this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        this_loss_watermark = torch.mean(torch.clamp(watermark_loss_kwargs['epsilon']-B_x_W, min=0)*balanced_beta_weights)\n",
    "        loss_watermark  = this_loss_watermark\n",
    "        # print('ok')\n",
    "        return loss_watermark.item()\n",
    "\n",
    "    # Ensure maxiter is enforced and debugging output\n",
    "    options = {'maxfun': 2, 'iprint':110, 'maxls':2, 'maxiter':2}\n",
    "\n",
    "    # Use the L-BFGS-B algorithm for optimization\n",
    "    result = minimize(compute_watermark_loss, x_sub_flat, \n",
    "                      method='L-BFGS-B', \n",
    "                      options=options)\n",
    "\n",
    "    # The optimal x found by the optimizer\n",
    "    results.append(result)\n",
    "    # print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scipy.optimize' from '/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/scipy/optimize/__init__.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(torch.wherae(torch.eq(data_copy.x, data.x)==False)[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[24,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.8543e-05,  0.0000e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.x[[24,25],[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([1,12])\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/20: 19.604000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m\n\u001b[1;32m     62\u001b[0m x_copy \u001b[38;5;241m=\u001b[39m x_copy\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#    edge_index_copy, x_copy, _ = augment_data(data, node_aug, edge_aug, train_nodes_to_consider, all_subgraph_indices)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#    x_copy = x_copy.requires_grad_(True)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m log_logits_copy          \u001b[38;5;241m=\u001b[39m \u001b[43mnode_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m probas_copy \u001b[38;5;241m=\u001b[39m log_logits_copy\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m     68\u001b[0m loss_watermark_scaled_copy \u001b[38;5;241m=\u001b[39m compute_watermark_loss(subgraph_dict, probas_copy, beta_weights)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Desktop/IDEAL/Project_2/src/models.py:201\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    199\u001b[0m intermediate_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnLayers):\n\u001b[0;32m--> 201\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(x)\n\u001b[1;32m    203\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:263\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/proj_2_env/lib/python3.12/site-packages/torch_geometric/nn/conv/message_passing.py:547\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 547\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    549\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_beta_weights(subgraph_dict, num_features):\n",
    "    if config.watermark_loss_kwargs['balance_beta_weights'] == True:\n",
    "        beta_weights = get_balanced_beta_weights([subgraph_dict[sig]['subgraph'] for sig in subgraph_dict.keys()])\n",
    "    elif config.watermark_loss_kwargs['balance_beta_weights'] == False:\n",
    "        beta_weights = torch.ones(len(subgraph_dict),num_features)\n",
    "    return beta_weights\n",
    "\n",
    "def process_beta(beta, alpha, omit_indices, scale_beta_method='clip'):\n",
    "    if scale_beta_method=='tanh':\n",
    "        beta = torch.tanh(alpha*beta)\n",
    "    elif scale_beta_method=='tan':\n",
    "        beta = torch.tan(alpha*beta)\n",
    "    elif scale_beta_method=='clip':\n",
    "        beta = torch.clip(beta,min=-1,max=1)\n",
    "    elif scale_beta_method==None:\n",
    "        pass\n",
    "    beta = beta.clone()  # Avoid in-place operation\n",
    "    if omit_indices is not None and len(omit_indices)>0:\n",
    "        beta[omit_indices] = 0 # zero out non-contributing indices\n",
    "    return beta\n",
    "\n",
    "def compute_watermark_loss(subgraph_dict, probas, beta_weights):\n",
    "    watermark_loss_kwargs = config.watermark_loss_kwargs\n",
    "    regression_kwargs = config.regression_kwargs\n",
    "    optimization_kwargs = config.optimization_kwargs\n",
    "\n",
    "    loss_watermark = torch.tensor(0.0)\n",
    "    for s, sig in enumerate(subgraph_dict.keys()):\n",
    "        this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "        x_sub, y_sub = data_sub.x, probas[subgraph_node_indices]\n",
    "        ''' epoch condtion: epoch==epoch-1'''\n",
    "        omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "        raw_beta            = solve_regression(x_sub, y_sub, regression_kwargs['lambda'])\n",
    "        beta                = process_beta(raw_beta, watermark_loss_kwargs['alpha'], omit_indices, watermark_loss_kwargs['scale_beta_method'])\n",
    "        B_x_W = (beta*this_watermark).clone()\n",
    "        B_x_W = B_x_W[not_omit_indices]\n",
    "\n",
    "        balanced_beta_weights = beta_weights[s]\n",
    "        balanced_beta_weights = balanced_beta_weights[not_omit_indices]\n",
    "        loss_watermark = loss_watermark+torch.mean(torch.clamp(watermark_loss_kwargs['epsilon']-B_x_W, min=0)*balanced_beta_weights)\n",
    "\n",
    "\n",
    "    loss_watermark = loss_watermark/len(subgraph_dict)\n",
    "    loss_watermark_scaled = loss_watermark*optimization_kwargs['coefWmk']\n",
    "    return loss_watermark_scaled\n",
    "\n",
    "\n",
    "beta_weights = get_beta_weights(subgraph_dict, data.x.shape[1])\n",
    "all_subgraph_indices = []\n",
    "for sig in subgraph_dict.keys():\n",
    "    nodeIndices = subgraph_dict[sig]['nodeIndices']\n",
    "    all_subgraph_indices+=nodeIndices.tolist()\n",
    "reg_copy=None\n",
    "\n",
    "\n",
    "lr = 0.002\n",
    "optimizer = optim.Adam(node_classifier.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for j in range(100):\n",
    "    x_copy,edge_index_copy = copy.deepcopy(data).x, copy.deepcopy(data).edge_index\n",
    "    x_copy = x_copy.requires_grad_(True)\n",
    "    #with torch.no_grad():\n",
    "    #    edge_index_copy, x_copy, _ = augment_data(data, node_aug, edge_aug, train_nodes_to_consider, all_subgraph_indices)\n",
    "    #    x_copy = x_copy.requires_grad_(True)\n",
    "    log_logits_copy          = node_classifier(x_copy, edge_index_copy)\n",
    "    probas_copy = log_logits_copy.clone().exp()\n",
    "    loss_watermark_scaled_copy = compute_watermark_loss(subgraph_dict, probas_copy, beta_weights)\n",
    "    wmk_loss = loss_watermark_scaled_copy+reg_copy if reg_copy is not None else loss_watermark_scaled_copy\n",
    "    optimizer.zero_grad()\n",
    "    wmk_loss.backward()\n",
    "    print(f'{j}/20: {wmk_loss:3f}',end='\\r')\n",
    "    this_grad = torch.zeros_like(data.x)\n",
    "    this_grad[all_subgraph_indices] = x_copy.grad.clone()[all_subgraph_indices]\n",
    "    this_grad[all_subgraph_indices] = (this_grad[all_subgraph_indices]-this_grad[all_subgraph_indices].mean())/(this_grad[all_subgraph_indices].max()-this_grad[all_subgraph_indices].min())\n",
    "    x_grad_wmk_loss = torch.zeros_like(data.x)\n",
    "    x_grad_wmk_loss[all_subgraph_indices] = this_grad.clone()[all_subgraph_indices]\n",
    "    # print(x_grad_wmk_loss[all_subgraph_indices])\n",
    "    data.x = data.x - perturb_lr*x_grad_wmk_loss\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  7,  11,  23,  34,  37,  41,  43,  69,  73,  86,  97, 105, 109, 110,\n",
      "        134, 144, 154, 159, 168, 177, 185, 186, 195, 212, 217, 219, 225, 227,\n",
      "        238, 250, 259, 277, 279, 284, 298, 328, 355, 358, 363, 365, 367, 376,\n",
      "        382, 386, 427, 430, 439, 442, 443, 457, 489, 503, 506, 507, 515, 520,\n",
      "        542, 544, 554, 559, 582, 597, 617, 649, 664, 687, 695, 698, 704, 708,\n",
      "        715, 729, 732, 747, 755, 758])\n"
     ]
    }
   ],
   "source": [
    "beta_weights = get_beta_weights(subgraph_dict, data.x.shape[1])\n",
    "for s, sig in enumerate(subgraph_dict.keys()):\n",
    "    this_watermark, data_sub, subgraph_node_indices = [subgraph_dict[sig][k] for k in ['watermark','subgraph','nodeIndices']]\n",
    "    x_sub, y_sub = data_sub.x, probas[subgraph_node_indices]\n",
    "    ''' epoch condtion: epoch==epoch-1'''\n",
    "    omit_indices,not_omit_indices = get_omit_indices(x_sub, this_watermark,ignore_zeros_from_subgraphs=False) #indices where watermark is 0\n",
    "    print(not_omit_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['11293_4896_5573_3951_10751_63'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_2_env",
   "language": "python",
   "name": "proj_2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
